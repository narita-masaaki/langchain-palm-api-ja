{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ5caKL2Ff2B"
      },
      "source": [
        "# 高度なプロンプト：Chain of Thought and ReAct (Reasoning + Acting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMkREhcA-Rtw"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pecYSnz2i2fk"
      },
      "source": [
        "このノートブックは、**Chain of Thought and ReAct (Reasoning + Acting)に基づいています** [Applied-Ai-Engieering-examples](https://github.com/googlecloudplatform/applied-ai-engineering-サンプル)GitHubリポジトリ。このリポジトリには、Google Cloud Applied AI Engineeringチームが開発したリファレンスガイド、青写真、コードサンプル、および実践的なラボが含まれています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H106E0clf7t"
      },
      "source": [
        "# パート0：はじめに\n",
        "\n",
        "このノートブックのターゲットオーディエンスは、タスク、ワークフロー、プロセス、機能などを繰り返し実行するためのエンジニアリングプロンプトです。安定性とパフォーマンスは、1回限りのニーズを求めるよりも重要です。\n",
        "\n",
        "このノートブックは、2つの強力なLLMプロンプト戦略をカバーしています。\n",
        "\n",
        "React（およびそのバリアント）は、幻覚を最小限に抑えながらLLMの推論を改善するための現在の最先端のプロンプト技術です。\n",
        "\n",
        "このノートブックの4つの部分は次のとおりです。\n",
        "\n",
        "1. 思考のチェーンプロンプト：LLM出力を改善するために推論の言語説明を使用します。\n",
        "\n",
        "1. アクション、検索、ツールの使用：LLMSが外部システムとどのように相互作用するか。\n",
        "\n",
        "1. 反応（推論 +演技）プロンプト：外部システムの相互作用と考えられたチェーンプロンプトの書面による推論の説明を組み合わせます。\n",
        "\n",
        "1. Langchain and React：Langchain React Agentを使用するときに何を期待するか。\n",
        "\n",
        "このノートブックはColabでテストされました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5FUT4VoDhsz"
      },
      "source": [
        "## このノートブックの使用方法\n",
        "\n",
        "* 最初にパート0を実行します。\n",
        "* パート1〜4それぞれパート0のコードに依存しますが、他の以前のパートのコードに依存しません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbz5Q4flkDgo"
      },
      "source": [
        "##前提条件\n",
        "\n",
        "-  LLMS（大規模な言語モデル）の理解：\n",
        "-  LLMとは何か、そしてそれらがどのように機能するか。\n",
        "-LLMSは、次のトークンの繰り返し予測因子として。\n",
        "-LLM予測は、トレーニングデータとの類似性を最大化します。\n",
        "-  LLMプロンプトの経験：\n",
        "- 言語モデルを「プロンプト」することの意味。[推奨リソース](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/introduction-plompt-design)。\n",
        "-  [ゼロショット、ワンショット、少数のショット](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/introduction-plompt-design#include-xamplesの違い)プロンプト、およびパフォーマンスと堅牢性を最大化するために、少数のショットプロンプトが不可欠である理由を理解すること。\n",
        "-Google Cloud Vertex LLMSに基本的な知識。[推奨リソース](https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/api-quickstart)\n",
        "-  Langchainとは何か、それが解決することを目指している問題を知ってください。\n",
        "-  [推奨リソース](https://python.langchain.com/docs/get_started/introduction)および[Tutorials](https://github.com/googlecloudplatform/generative-ai/tree/main/language/orchestration/langchain)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmWgaCsdu6k1"
      },
      "source": [
        "##キー用語\n",
        "\n",
        "一貫性のために、このノートブックは特定の方法で次の用語を使用します。\n",
        "\n",
        "**プロンプト**：テンプレートに挿入される値に関係なく、コールのパフォーマンスと堅牢性を最大化する特定の手法を使用して作成されたテンプレートLLMコール。\n",
        "\n",
        "**LLMコール**：LLMにテキストを送信します。\n",
        "\n",
        "**LLM応答**：LLMによって予測されたテキスト、LLMコールを行うときにLLMから戻ってくるもの。\n",
        "\n",
        "**チェーン/チェーン**コンテキストに応じて：\n",
        "* 紹介されたチェーンのプロンプト、論理的に連続的な推論ステップ。\n",
        "* LLMシステムでは、LLMへの連続呼び出し。各コールは前のコールの応答に依存します。\n",
        "\n",
        "**exemplar**：1つまたは少数のプロンプトの「例」。\n",
        "* 従来のMLの意味での「例」との混乱、つまり「データの一部」（「トレーニングの例」など）を避けるために使用されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-glBTWPl1WD"
      },
      "source": [
        "##参照\n",
        "\n",
        "* 小島、タケシ、他「大規模な言語モデルはゼロショットの推論者です。」神経情報処理システムの進歩35（2022）：22199-22213。[link](https://arxiv.org/abs/2205.11916)（アクセス2023 09 22）\n",
        "* Wang、Xuezhi、et al。「自己整合性は、言語モデルの一連の思考推論を改善します。」arxiv preprint arxiv：2203.11171（2022）。[link](https://arxiv.org/abs/2203.11171)（アクセス2023 09 03）。\n",
        "*ウェイ、ジェイソン他「考えられたチェーンプロンプトは、大規模な言語モデルで推論を引き出します。」神経情報処理システムの進歩35（2022）：24824-24837。[link](https://arxiv.org/abs/2201.11903)（アクセス2023 09 03）。\n",
        "* Yao、Shunyu、et al。「反応：言語モデルでの推論と行動の相乗効果。」Arxiv Preprint arxiv：2210.03629（2022）。[link](https://arxiv.org/abs/2210.03629)（アクセス2023 09 03）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC1b7po9xWM6"
      },
      "source": [
        "## セットアップ - このコードを最初に実行してください！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NZ_4h24m-B8u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "62960a2b-7aa8-4c73-a763-d14c44f2ad5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.316\n",
            "  Downloading langchain-0.0.316-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting google-cloud-aiplatform==1.35.0\n",
            "  Downloading google_cloud_aiplatform-1.35.0-py2.py3-none-any.whl.metadata (27 kB)\n",
            "Collecting prettyprinter==0.18.0\n",
            "  Downloading prettyprinter-0.18.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting wikipedia==1.4.0\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (2.10.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (3.10.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.316)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.316)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.43 (from langchain==0.0.316)\n",
            "  Downloading langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.316) (2.32.3)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.316)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (2.19.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.35.0) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.35.0) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.35.0) (24.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.35.0) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.35.0) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.35.0) (1.12.5)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform==1.35.0) (2.0.6)\n",
            "Requirement already satisfied: Pygments>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from prettyprinter==0.18.0) (2.16.1)\n",
            "Collecting colorful>=0.4.0 (from prettyprinter==0.18.0)\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia==1.4.0) (4.12.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.316) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.316) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.316) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.316) (1.2.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.316)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.316)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.65.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (2.27.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.35.0) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.35.0) (2.7.2)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.35.0) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.35.0) (0.13.1)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.316)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.316) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.316) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.316) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.316) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.316) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.316) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.316) (3.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia==1.4.0) (2.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.35.0) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform==1.35.0) (1.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.316)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.35.0) (0.6.0)\n",
            "Downloading langchain-0.0.316-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_aiplatform-1.35.0-py2.py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prettyprinter-0.18.0-py2.py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=1bf13cee4e160c8bab32308753a269243d24160e52f8ddfe2c2ec24bf96a9ee5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: colorful, tenacity, prettyprinter, mypy-extensions, marshmallow, jsonpointer, wikipedia, typing-inspect, jsonpatch, langsmith, dataclasses-json, langchain, google-cloud-aiplatform\n",
            "\u001b[33m  WARNING: The script langsmith is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts langchain and langchain-server are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed colorful-0.5.6 dataclasses-json-0.6.7 google-cloud-aiplatform-1.35.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.0.316 langsmith-0.0.92 marshmallow-3.22.0 mypy-extensions-1.0.0 prettyprinter-0.18.0 tenacity-8.5.0 typing-inspect-0.9.0 wikipedia-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "a4e4e4471e584f3895e9a67f0876af22"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Tested with these package versions.\n",
        "# Note this notebook uses matplotlib.pyplot. This is in the default Colab\n",
        "#   runtime, but you may need to install it in other notebook environments.\n",
        "!pip install --user langchain==0.0.316 google-cloud-aiplatform==1.35.0 prettyprinter==0.18.0 wikipedia==1.4.0 numexpr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCngWdptsN_Q"
      },
      "source": [
        "**さらに進む前にランタイムを再起動してください**\n",
        "\n",
        "ランタイムが削除されていない限り（再起動しても）、この以前のセルを再実行する必要はありません。\n",
        "\n",
        "ランタイムが再起動した場合、パート0の残りのセルを再実行します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-MjBceCQvcq"
      },
      "source": [
        "Colabを使用している場合は、次のセルでコードを実行します。[Vertex aiisifigine]を使用するには、Googleクラウド[プロジェクト](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects)にアクセスできるアカウントでポップアップをフォローし、認証します。llms](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)。\n",
        "\n",
        "Colab以外の場所でこのノートブックを実行している場合は、環境に適切なGoogleクラウドアクセスがあることを確認してください。それがあなたにとって新しい概念である場合は、[あなたのローカル環境のアプリケーションデフォルトの資格情報](https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev)を調べることを検討してください。より多くの認証オプションについて説明します[こちら](https://cloud.google.com/docs/authentication)。\n",
        "\n",
        "Google Cloudをまったく初めて使用する場合は、[開始](https://cloud.google.com/docs/get-started)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhnxRspMGGiz"
      },
      "outputs": [],
      "source": [
        "# Colab authentication.\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    print('Authenticated')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSeWZt3ZpxeY"
      },
      "source": [
        "Google CloudプロジェクトIDを次のセルに設定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLDEjCVzp7eh"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"YOUR_PROJECT_ID_HERE\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "# Code examples may misbehave if the model is changed.\n",
        "MODEL_NAME = \"text-bison@001\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fTAg64qFY2B"
      },
      "outputs": [],
      "source": [
        "# Set up Vertex PaLM API.\n",
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=PROJECT_ID,\n",
        "              location=LOCATION)\n",
        "parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_output_tokens\": 1024,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "\n",
        "model = TextGenerationModel.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSpDXdhBvhtu"
      },
      "source": [
        "この関数は、ノートブック全体で使用され、完全なLLMコールと応答を表示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esxRVsLAvvr6"
      },
      "outputs": [],
      "source": [
        "def call_llm(model, parameters, llm_call, show_activity = True):\n",
        "  response = model.predict(llm_call, **parameters).text\n",
        "\n",
        "  if show_activity:\n",
        "    BOLD = \"\\033[1m\"\n",
        "    UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
        "    print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
        "    print(f\"{BOLD}The response:{UNFORMAT}\")\n",
        "    print(response)\n",
        "\n",
        "  return response  # Return to `_` if not needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoiMSEJoY9gt"
      },
      "outputs": [],
      "source": [
        "# Wrap code cell output to improve notebook readability.\n",
        "# Source: https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results/61401455#61401455\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css(arg):\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US-jQm1MuGBa"
      },
      "source": [
        "# パート1：考え方のチェーンプロンプト\n",
        "\n",
        "LLMSにとって、チェーンはファッショナブルなアクセサリー以上のものです。\n",
        "\n",
        "<img src = \"https://raw.githubusercontent.com/GoogleCloudPlatform/specialized-training-content/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/images/1-chains.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82YfCjFJVX60"
      },
      "source": [
        "## 概要\n",
        "\n",
        "考え方のプロンプトでは、目的の出力に到達するための推論ステップを示す1つまたは少数のショットの模範を提供します。これは、標準の1または少数のショットプロンプトとは異なり、模範が入力と正しい出力のみを示します。\n",
        "\n",
        "思考の連鎖模範で提供する推論の内訳は、人が問題や仕事を通して考えている自然言語の内部モノローグに似ています。\n",
        "\n",
        "「内部モノローグ」が奇妙な概念である場合、問題を解決したり、タスクを達成したりするために自分の考えを言語化する方法を考えてください。たとえば、あなたは夕食を作っています：\n",
        "\n",
        "「OK OK私はセロリを切り刻みました。今、私は鶏肉を始める必要があります。オーブンはオンですか？オーブンの予熱を始めましょう。待って、どの温度？もう一度レシピをチェックする必要があります... `` `\n",
        "\n",
        "この「内部モノローグ」または「内部スピーチ」は、タスクの次に何が起こるべきかを特定することにより、これまで見たことのない新しい問題に問題解決パターンを適用することを容易にします。\n",
        "\n",
        "テキスト推論の「内部独白」を含む模範を使用してLLMを呼び出すことにより、LLMは同様のテキスト推論を含む応答を生成します。LLMに応答の一部として推論テキストが生成されると、応答が目的の出力で終了する可能性が高くなります。\n",
        "\n",
        "応答の推論ステップ\n",
        "また、LLMが最終出力にどのように到着したかの解釈可能性を提供します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydRfjsuBI5Ip"
      },
      "source": [
        "##思考の基本の連鎖\n",
        "\n",
        "数学の単語の問題は、数学的および論理的に単純なものであるが、推論の複数のステップが必要なため、良いチェーンのデモンストレーションです。\n",
        "\n",
        "この例（思考の連鎖から[紙](https://arxiv.org/pdf/2201.11903.pdf)から）誤った答えに注意してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VJcAD7lYXE0"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"Q: ロジャーはテニスボールを5個持っている。彼はさらに2つのテニスボール缶を買った。\n",
        "それぞれの缶には3個のテニスボールが入っている。ロジャーは今何個のテニスボールを持っていますか？\n",
        "A: 答えは11個。\n",
        "Q：食堂には23個のリンゴがありました。\n",
        "ランチに20個使い、さらに6個買ったとすると、何個のリンゴがあるでしょうか？\n",
        "A:\"\"\"\n",
        "\n",
        "_ = call_llm(model, parameters, question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vmzEro2Z707"
      },
      "source": [
        "一連の思考を含めるために模範を書き直すことは、LLMに、質問を複数の単純な推論のステップに分解する方法を示しています。\n",
        "\n",
        "モデル応答は、同様の思考の連鎖に従い、正解の可能性を高めます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_QojLuvZzLV"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"Q: ロジャーはテニスボールを5個持っている。彼はさらに2つのテニスボール缶を買った。\n",
        "それぞれの缶には3個のテニスボールが入っている。ロジャーは今何個のテニスボールを持っていますか？\n",
        "A: ロジャーは5個のボールから始めました。テニスボール3個入りの缶を2つ\n",
        "は6個です。5 + 6 = 11. 答えは11。\n",
        "Q：食堂には23個のリンゴがありました。\n",
        "ランチに20個使い、さらに6個買ったとすると、何個のリンゴがあるでしょうか？\n",
        "A:\"\"\"\n",
        "\n",
        "_ = call_llm(model, parameters, question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjwgFMOLaem9"
      },
      "source": [
        "思考の連鎖には、各推論ステップからのフォローするステップと中間出力/結論を説明するテキストの両方が含まれています。\n",
        "\n",
        "以下のコードの「質問」変数を変更して、さまざまな質問を試してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd4e62T7aWoG"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"ノムフンドは準備書面を書く。\n",
        "準備書面には3つのセクションがあり、それぞれ4時間かかる。\n",
        "今週は3つの準備書面を書きました。何時間かかりましたか？\"\"\"\n",
        "\n",
        "one_shot_exemplar = \"\"\"Q: ロジャーはテニスボールを5個持っている。\n",
        "彼はさらに2つのテニスボール缶を買った。\n",
        "それぞれの缶には3個のテニスボールが入っている。現在、彼は何個のテニスボールを持っていますか？\n",
        "A: ロジャーは5個のボールから始めました。テニスボール3個入りの缶を2つ\n",
        "は6個です。5 + 6 = 11. 答えは11です。\n",
        "Q: \"\"\"\n",
        "\n",
        "# Prepending the one shot exemplar before the question we want answered.\n",
        "llm_call = f\"{one_shot_exemplar}{question}\\nA:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XUp7beLcQsS"
      },
      "source": [
        "LLM応答は通常、模範の推論スタイルを模倣します。これは、模範の推論のチェーンがタスクに適している場合、最高のパフォーマンスを得ることができます。\n",
        "\n",
        "以下のセルを比較してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPQVYIPucnkF"
      },
      "outputs": [],
      "source": [
        "# Correct answer: 360, 375.\n",
        "question = \"\"\"高効率工場は1日当たり100台を生産する。\n",
        "中効率工場は1日当たり60台を生産する。\n",
        "低効率の工場は1日当たり30個を生産する。\n",
        "メガコープは5つの工場を所有している。3つは高効率、2つは低効率である。\n",
        "明日、低効率工場を中効率工場に改修する。\n",
        "そして、残りの低効率工場では、生産量が半分になるような停電が発生する。\n",
        "今日は何台生産できるだろうか？明日は何台？\"\"\"\n",
        "\n",
        "one_shot_exemplar = \"\"\"Q: ロジャーはテニスボールを5個持っている。\n",
        "彼はさらに2つのテニスボール缶を買った。\n",
        "それぞれの缶には3個のテニスボールが入っている。現在、彼は何個のテニスボールを持っていますか？\n",
        "A: ロジャーは5個のボールから始めました。テニスボール3個入りの缶を2つ\n",
        "は6個です。5 + 6 = 11. 答えは11です。\n",
        "Q: \"\"\"\n",
        "\n",
        "llm_call = f\"{one_shot_exemplar}{question}\\nA:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ6Xo0gwpi35"
      },
      "source": [
        "出力の間違いに注意してください。LLM応答は、明日まだ実行されている3つの高効率工場を考慮していません。\n",
        "\n",
        "このタスクでは、さまざまな測定単位（テニスボール缶対工場出力）への接続と、数日間の数の持ち運びを含む推論ステップを使用して、一連の思考を使用することをお勧めします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThikEZV1cNYM"
      },
      "outputs": [],
      "source": [
        "better_one_shot_exemplar = \"\"\"Q: 大きなテニスボール缶には5個のボールが入っている。\n",
        "小さなテニスボール缶には3個のボールがある。\n",
        "ロジャーは今日、大きい缶を3つ、小さい缶を2つ持っている。\n",
        "明日、彼は賭けに勝ち、小さい缶を1つ大きい缶に変えた。\n",
        "ロジャーが今日持っているボールは何個か？明日は何個？\n",
        "A: 大缶3個は3 * 5 = 15個のテニスボール。\n",
        "小さい缶2個は、2 * 3 = 6個のテニスボール。\n",
        "今日、ロジャーは15 + 6 = 21個のテニスボールを持っている。\n",
        "明日のトレードは、小さなテニスボール缶を1つ失い、大きな缶を1つ得ることを意味する。\n",
        "ロジャーは昨日持っていた缶をまだ持っている。\n",
        "昨日の小さい缶2個 - 1 = 小さい缶1個\n",
        "昨日の大缶3個＋1個＝大缶4個\n",
        "大きい缶4個は、4 * 5 = 20個のテニスボール。\n",
        "小さい缶1個は、1 * 3個のテニスボール。\n",
        "明日、ロジャーは20 + 3 = 23個のテニスボールを持っている。\n",
        "Q: \"\"\"\n",
        "\n",
        "llm_call = f\"{better_one_shot_exemplar}{question}\\nA:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXNKuX_BttIk"
      },
      "source": [
        "##思考ユースケースの連鎖\n",
        "\n",
        "数学の単語の問題はあまり役に立たないかもしれませんが、一連の思考は他のタイプの問題でうまく機能します。\n",
        "\n",
        "思考の連鎖からのいくつかの例[紙](https://arxiv.org/pdf/2201.11903.pdf)は情報を操作し、妥当性を評価し、指示を与え、テキストを変更/理解し、状態を追跡しています。\n",
        "\n",
        "<img src = \"https://raw.githubusercontent.com/GoogleCloudPlatform/specialized-training-content/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/images/2-cot.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX-kn_08m6VW"
      },
      "source": [
        "思考の連鎖によく反応する他のタイプのタスクは次のとおりです。\n",
        "* データの変換と濃縮。\n",
        "* データの解釈。\n",
        "* コード生成。\n",
        "* テキストの品質の評価（LLM応答の品質の評価を含む）。\n",
        "* 合成データの作成。\n",
        "\n",
        "一般的に、いくつかの簡単なステップを「話す」ことによって解決されるあらゆる種類の問題は、思考候補の良いチェーンです。\n",
        "\n",
        "より複雑な思考の使用の使用のために、模範全体であなたの考え方の推論スタイルがより一貫しているほど、LLMはその応答において同じスタイルの推論に従う可能性が高くなります。これは次の2つの例に注意してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRwGi1BUX8IE"
      },
      "source": [
        "####例：テーブルの理解"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFFmFWgIw_Lt"
      },
      "outputs": [],
      "source": [
        "# The correct answer is Post-War British Literature.\n",
        "question = \"\"\"\n",
        "| Book Name | Edition | ISBN | Publisher | Aug 1 Amazon Avg New Price | Aug 1 Amazon Avg Used Price | Aug 1 Abebooks Avg New Price | Aug 1 Abebooks Avg Used Price | Sep 1 Amazon Avg New Price | Sep 1 Amazon Avg Used Price | Sep 1 Abebooks Avg New Price | Sep 1 Abebooks Avg Used Price |\n",
        "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
        "| Physics for Computer Scientists | 10th | 978-1-118-56906-1 | Pearson Education | $149.99 | $79.99 | $142.94 | $66.94 | $129.99 | $59.99 | $139.94 | $56.94 |\n",
        "| Fundamentals of Calculus | 8th | 978-0-470-45831-0 | John Wiley & Sons | $139.99 | $99.99 | $137.94 | $87.94 | $129.99 | $79.99 | $129.94 | $76.94 |\n",
        "| Post-War British Literature | 2nd | 978-0-300-08897-2 | Oxford University Press | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
        "| Modern Religions: An Overview | 3rd | 978-0-19-992545-3 | Oxford University Press | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
        "| The Norton Introduction to Literature | 11th | 978-0-393-45078-1 | W. W. Norton & Company | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
        "| The Norton Anthology of American Literature | 9th | 978-0-393-93750-8 | W. W. Norton & Company | $179.99 | $139.99 | $174.94 | $127.94 | $169.99 | $124.99 | $174.94 | $121.94 |\n",
        "| The Norton Anthology of World Literature | 8th | 978-0-393-92855-6 | W. W. Norton & Company | $179.99 | $139.99 | $174.94 | $127.94 | $169.99 | $124.99 | $174.94 | $121.94 |\n",
        "| The Elements of Style | 5th | 978-0-205-11265-3 | Longman | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
        "\n",
        "8月から9月にかけてAmazonで最も古本価格が下落したOxfordの本は？\n",
        "\"\"\"\n",
        "\n",
        "context = \"\"\"表に関する質問に答える。\n",
        "すべての質問は、表の中の事実によって裏付けられていなければならない。\n",
        "すべての推論はステップ・バイ・ステップで行うこと。\n",
        "理由を説明する。\n",
        "複数の行を見るときは、それぞれの行の理由を1つずつ説明すること。\n",
        "\"\"\"\n",
        "\n",
        "llm_call = f\"{context}\\n{question}\\nAnswer:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_bpOTJcXviZ"
      },
      "source": [
        "次に、いくつかの模範を追加します。\n",
        "\n",
        "模範は質問とは異なるソーステーブルを使用しているが、考え方のチェーンの推論はまだ機能していることに注意してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGUOqCKO_SIW"
      },
      "outputs": [],
      "source": [
        "few_shot_exemplar = \"\"\"\n",
        "Table:\n",
        "| Item Name | SKU | Vendor | Aug 1 Inventory | Sep 1 Inventory | Sale Count |\n",
        "|---|---|---|---|---|---|\n",
        "| iPhone 13 Pro Max | MGL83LL/A | Apple | 100 | 80 | 17 |\n",
        "| iPhone 13 Pro | MLL03LL/A | Apple | 50 | 40 | 9 |\n",
        "| iPhone 13 | MLKG3LL/A | Apple | 25 | 20 | 4 |\n",
        "| Samsung Galaxy S22 Ultra | SM-S908U | Samsung | 100 | 80 | 19 |\n",
        "| Samsung Galaxy S22 Plus | SM-S906U | Samsung | 50 | 40 | 10 |\n",
        "| Samsung Galaxy S22 | SM-S901U | Samsung | 25 | 20 | 5 |\n",
        "| Google Pixel 6 Pro | GA01314-US | Google | 100 | 80 | 20 |\n",
        "\n",
        "Question:\n",
        "8月に最も売れたiPhoneは？\n",
        "答え 1つ1つの商品を見て、それがiPhoneかどうかを判断する必要があります。\n",
        "iPhoneのアイテムのみを考慮する。\n",
        "iPhoneのアイテムとは、iPhone 13 Pro Max、iPhone 13 Pro、iPhone 13です。\n",
        "それぞれのiPhoneがいくら売れたかを1つずつ見て、どの販売カウントが一番高いかを確認する必要がある。\n",
        "iPhone 13 Pro Maxの販売台数は17台。\n",
        "iPhone 13 Proの販売台数は9台。\n",
        "iPhone 13の販売台数は4台。\n",
        "17、9、4のうち最も大きい数は17。\n",
        "Answer:iPhone 13 Pro Max。\n",
        "\n",
        "Table:\n",
        "| Item Name | SKU | Vendor | Aug 1 Inventory | Sep 1 Inventory | Sale Count |\n",
        "|---|---|---|---|---|---|\n",
        "| iPhone 13 Pro Max | MGL83LL/A | Apple | 100 | 80 | 17 |\n",
        "| iPhone 13 Pro | MLL03LL/A | Apple | 50 | 40 | 9 |\n",
        "| iPhone 13 | MLKG3LL/A | Apple | 25 | 20 | 4 |\n",
        "| Samsung Galaxy S22 Ultra | SM-S908U | Samsung | 100 | 80 | 19 |\n",
        "| Samsung Galaxy S22 Plus | SM-S906U | Samsung | 50 | 40 | 10 |\n",
        "| Samsung Galaxy S22 | SM-S901U | Samsung | 25 | 20 | 5 |\n",
        "| Google Pixel 6 Pro | GA01314-US | Google | 100 | 80 | 20 |\n",
        "\n",
        "Question:\n",
        "9月1日に最も多くの台数が不明となったサムスンの携帯電話は？\n",
        "回答 一台ずつ見て、サムスンの商品かどうかを判断する必要があります。\n",
        "サムスンのアイテムはアイテム名を見なければなりません。\n",
        "サムスンの品目のみを考慮する。\n",
        "サムスンのアイテムは、S22 Ultra、S22 Plus、S22です。\n",
        "それぞれのサムスン製品について、9月1日と8月1日の在庫差を見て、何台売れたはずかを確認します。\n",
        "そして、その数字と実際の販売台数を比較する必要がある。\n",
        "販売数フィールドと在庫差の差が最も大きい機種が、最も未カウントの機種です。\n",
        "Samsung Galaxy S22 Ultraは8月1日に100台在庫があり、9月1日に80台在庫がありました。100から80を引くと20です（100 - 80 = 20）。販売数は19。20から19を引くと1（20 - 19 = 1）。1台が未入荷。\n",
        "サムスンGalaxy S22 Plusの在庫は8月1日50台、9月1日40台。50から40を引くと10（50 - 40 = 10）。販売数は10。販売数は在庫差と一致し、未計上の台数はない。\n",
        "サムスンGalaxy S22の在庫は8月1日25個、9月1日20個。25から20を引くと5（25 - 20 = 5）。販売台数は5台、20台マイナス19台は1台。\n",
        "S22ウルトラのみ、未計上のものがあった。\n",
        "Answer:Samsung Galaxy S22 Ultra。\n",
        "\n",
        "Table:\n",
        "| Item Name | SKU | Vendor | Aug 1 Inventory | Sep 1 Inventory | Sale Count |\n",
        "|---|---|---|---|---|---|\n",
        "| iPhone 13 Pro Max | MGL83LL/A | Apple | 100 | 80 | 17 |\n",
        "| iPhone 13 Pro | MLL03LL/A | Apple | 50 | 40 | 9 |\n",
        "| iPhone 13 | MLKG3LL/A | Apple | 25 | 20 | 4 |\n",
        "| Samsung Galaxy S22 Ultra | SM-S908U | Samsung | 100 | 80 | 19 |\n",
        "| Samsung Galaxy S22 Plus | SM-S906U | Samsung | 50 | 40 | 10 |\n",
        "| Samsung Galaxy S22 | SM-S901U | Samsung | 25 | 20 | 5 |\n",
        "| Google Pixel 6 Pro | GA01314-US | Google | 100 | 80 | 20 |\n",
        "\n",
        "Question:\n",
        "総売上高が最も多かったベンダーは？\n",
        "答えてください： ベンダーを一つずつ調べる必要があります。\n",
        "商品名フィールドからベンダーを推測する必要があります。\n",
        "テーブルには3つのユニークなベンダーがあります： アップル、サムスン、グーグルです。\n",
        "各ベンダーについて、各アイテムの販売数を1つずつ見つけ、販売数を合計する必要があります。\n",
        "アップルの商品は、iPhone 13 Pro Maxが17件、iPhone 13 Proが9件、iPhone 13が4件です。\n",
        "17 + 9 + 4 = 30. 30台のアップル製携帯電話が売れたことになる。\n",
        "サムスンの商品は、サムスン・ギャラクシーS22ウルトラで19台、サムスン・ギャラクシーS22プラスで10台、サムスン・ギャラクシーS22で5台。\n",
        "19 + 10 + 5 = 34. 34台のサムスン製携帯電話が売れた。\n",
        "グーグル製はグーグルPixel 6 Proで20台。20台のグーグル・スマホが売れた。\n",
        "アップル30台、サムスン34台、グーグル20台。34が最大の数字で、これはサムスンの販売台数である。\n",
        "Answer:サムスン\n",
        "\n",
        "\n",
        "Table:\n",
        "| Item Name | SKU | Vendor | Aug 1 Inventory | Sep 1 Inventory | Sale Count |\n",
        "|---|---|---|---|---|---|\n",
        "| iPhone 13 Pro Max | MGL83LL/A | Apple | 100 | 80 | 17 |\n",
        "| iPhone 13 Pro | MLL03LL/A | Apple | 50 | 40 | 9 |\n",
        "| iPhone 13 | MLKG3LL/A | Apple | 25 | 20 | 4 |\n",
        "| Samsung Galaxy S22 Ultra | SM-S908U | Samsung | 100 | 80 | 19 |\n",
        "| Samsung Galaxy S22 Plus | SM-S906U | Samsung | 50 | 40 | 10 |\n",
        "| Samsung Galaxy S22 | SM-S901U | Samsung | 25 | 20 | 5 |\n",
        "| Google Pixel 6 Pro | GA01314-US | Google | 100 | 80 | 20 |\n",
        "\n",
        "Question:\n",
        "最も売れた商品は？\n",
        "答えてください： ひとつひとつ見ていく必要がありますね。\n",
        "iPhone 13 Pro Maxは17台売れました。\n",
        "iPhone 13 Proの販売台数は9台でした。\n",
        "iPhone13は4台。\n",
        "サムスン・ギャラクシーS22ウルトラの販売台数は19台。\n",
        "サムスン・ギャラクシーS22 Plusは10台。\n",
        "サムスン・ギャラクシーS22は5台。\n",
        "グーグルPixel 6 Proの販売台数は20台。\n",
        "販売台数は17台、9台、3台、19台、10台、5台、20台。\n",
        "20が最大の販売台数で、これはGoogle Pixel 6 Proの販売台数である。\n",
        "Answer:Google Pixel 6 Pro。\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Prepending the few shot exemplars before the question we want answered.\n",
        "llm_call = f\"{context}\\n{few_shot_exemplar}{question}\\nAnswer:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf0vyGCAZndK"
      },
      "source": [
        "さらに2つの質問（読みやすさのモデルの呼び出しを抑制します）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm_GnH8yZb9-"
      },
      "outputs": [],
      "source": [
        "# The correct answer is $6.15.\n",
        "question = \"\"\"\n",
        "Table:\n",
        "| Book Name | Edition | ISBN | Publisher | Aug 1 Amazon Avg New Price | Aug 1 Amazon Avg Used Price | Aug 1 Abebooks Avg New Price | Aug 1 Abebooks Avg Used Price | Sep 1 Amazon Avg New Price | Sep 1 Amazon Avg Used Price | Sep 1 Abebooks Avg New Price | Sep 1 Abebooks Avg Used Price |\n",
        "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
        "| Physics for Computer Scientists | 10th | 978-1-118-56906-1 | Pearson Education | $149.99 | $79.99 | $142.94 | $66.94 | $129.99 | $59.99 | $139.94 | $56.94 |\n",
        "| Fundamentals of Calculus | 8th | 978-0-470-45831-0 | John Wiley & Sons | $139.99 | $99.99 | $137.94 | $87.94 | $129.99 | $79.99 | $129.94 | $76.94 |\n",
        "| Post-War British Literature | 2nd | 978-0-300-08897-2 | Oxford University Press | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
        "| Modern Religions: An Overview | 3rd | 978-0-19-992545-3 | Oxford University Press | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
        "| The Norton Introduction to Literature | 11th | 978-0-393-45078-1 | W. W. Norton & Company | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
        "| The Norton Anthology of World Literature | 8th | 978-0-393-92855-6 | W. W. Norton & Company | $179.99 | $139.99 | $174.94 | $127.94 | $169.99 | $124.99 | $174.94 | $121.94 |\n",
        "| The Elements of Style | 5th | 978-0-205-11265-3 | Longman | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
        "\n",
        "Question:\n",
        "もし8月にアマゾンではなく阿部書房で『エレメンツ・オブ・スタイル』の新刊を3冊購入したら、いくらの節約になるだろうか？\n",
        "\"\"\"\n",
        "\n",
        "llm_call = f\"{context}\\n{few_shot_exemplar}{question}\\nAnswer:\"\n",
        "print(call_llm(model, parameters, llm_call, show_activity=False))\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# The correct answer is Physics for Computer Scientists.\n",
        "question = \"\"\"\n",
        "Table:\n",
        "| Book Name | Edition | ISBN | Publisher | Aug 1 Amazon Avg New Price | Aug 1 Amazon Avg Used Price | Aug 1 Abebooks Avg New Price | Aug 1 Abebooks Avg Used Price | Sep 1 Amazon Avg New Price | Sep 1 Amazon Avg Used Price | Sep 1 Abebooks Avg New Price | Sep 1 Abebooks Avg Used Price |\n",
        "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
        "| Physics for Computer Scientists | 10th | 978-1-118-56906-1 | Pearson Education | $149.99 | $79.99 | $142.94 | $66.94 | $129.99 | $59.99 | $139.94 | $56.94 |\n",
        "| Fundamentals of Calculus | 8th | 978-0-470-45831-0 | John Wiley & Sons | $139.99 | $99.99 | $137.94 | $87.94 | $129.99 | $79.99 | $129.94 | $76.94 |\n",
        "| Post-War British Literature | 2nd | 978-0-300-08897-2 | Oxford University Press | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
        "| Modern Religions: An Overview | 3rd | 978-0-19-992545-3 | Oxford University Press | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
        "| The Norton Introduction to Literature | 11th | 978-0-393-45078-1 | W. W. Norton & Company | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
        "| The Norton Anthology of World Literature | 8th | 978-0-393-92855-6 | W. W. Norton & Company | $179.99 | $139.99 | $174.94 | $127.94 | $169.99 | $124.99 | $174.94 | $121.94 |\n",
        "| The Elements of Style | 5th | 978-0-205-11265-3 | Longman | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
        "\n",
        "質問です： アマゾンの新品と中古の価格差が最も大きい本は？\n",
        "\"\"\"\n",
        "\n",
        "llm_call = f\"{context}\\n{few_shot_exemplar}{question}\\nAnswer:\"\n",
        "print(call_llm(model, parameters, llm_call, show_activity=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jk98xwBpSnl"
      },
      "source": [
        "データ理解のユースケースの場合、データスキーマが事前にデータスキーマを知っている場合は、そのスキーマと一致する必要があります。\n",
        "\n",
        "一般に、模範的なデータ構造がデータ構造の構造であるほど、LLMが正しく応答する可能性が高くなります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWB4WcfdaLNi"
      },
      "source": [
        "#### 例：タグ付けデータと構造化されたデータ出力\n",
        "\n",
        "LLMワークフローの2つの一般的なニーズは、説明からタグまたはカテゴリを生成し、構造化されたデータを出力することです。\n",
        "\n",
        "この例は両方を行います。タグ付けのパフォーマンスは、特定のタグが最適な理由を通じて、チェーンオブテアの模範とともに改善されます（タグが選択された理由の解釈可能性を提供します）。\n",
        "\n",
        "さらに、JSONのような一般的なデータ形式であっても、構造化されたデータ出力がどのように見えるかを示すと、パフォーマンスが向上します。\n",
        "\n",
        "[データソース](https://data.amerigeoss.org/dataset/gsa-json-adc1d)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xOLcvQdXWfd"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"あるデータ・ソースの JSON エントリが与えられた場合、以下のフィールドを持つ JSON を出力し、その理由を説明しなさい：\n",
        "pii： 真偽値: データセットに個人識別情報が含まれている。\n",
        "age： データセットが最後に更新されてから何年経っているか。\n",
        "keywords: キーワード： このデータセットをインデックス化するための新しいキーワード。\n",
        "最後のテキスト出力はJSONでなければならない。\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "question = \"\"\"\n",
        "{\n",
        "    \"@type\" : \"dcat:Dataset\",\n",
        "    \"description\" : \"<p>The MDS 3.0 Frequency Report summarizes information for active residents currently in nursing homes. The source of these counts is the residents MDS assessment record. The MDS assessment information for each active nursing home resident is consolidated to create a profile of the most recent standard information for the resident.</p>\\n\",\n",
        "    \"title\" : \"MDS 3.0 Frequency Report\",\n",
        "    \"accessLevel\" : \"public\",\n",
        "    \"identifier\" : \"465\",\n",
        "    \"license\" : \"http://opendefinition.org/licenses/odc-odbl/\",\n",
        "    \"modified\" : \"2016-04-05\",\n",
        "    \"temporal\" : \"2012-01-01T00:00:00-05:00/2015-12-31T00:00:00-05:00\",\n",
        "    \"contactPoint\" : {\n",
        "      \"@type\" : \"vcard:Contact\",\n",
        "      \"fn\" : \"Health Data Initiative\",\n",
        "      \"hasEmail\" : \"mailto:HealthData@hhs.gov\"\n",
        "    },\n",
        "    \"bureauCode\" : [ \"009:38\" ],\n",
        "    \"keyword\" : [ \"Activities of Daily Living (ADL)\" ],\n",
        "    \"language\" : [ \"en\" ],\n",
        "    \"programCode\" : [ \"009:000\" ],\n",
        "    \"publisher\" : {\n",
        "      \"@type\" : \"org:Organization\",\n",
        "      \"name\" : \"Centers for Medicare & Medicaid Services\",\n",
        "      \"subOrganizationOf\" : {\n",
        "        \"@type\" : \"org:Organization\",\n",
        "        \"name\" : \"Department of Health & Human Services\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "llm_call = f\"{context}\\nJSON:{question}\\nAnswer:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0W-zY4uewRs"
      },
      "source": [
        "JSON形式は正しいですが、年齢は間違っており、キーワードは予測されていません。1つの模範を追加すると、正しい応答が得られます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUn2EeXQe6pu"
      },
      "outputs": [],
      "source": [
        "one_shot_exemplar = \"\"\"\n",
        "JSON:\n",
        "{\n",
        "\n",
        "    \"@type\" : \"dcat:Dataset\",\n",
        "    \"description\" : \"The primary purpose of this system of records is to properly pay medical insurance benefits to or on behalf of entitled beneficiaries.\",\n",
        "    \"title\" : \"Medicare Multi-Carrier Claims System\",\n",
        "    \"accessLevel\" : \"restricted public\",\n",
        "    \"dataQuality\" : true,\n",
        "    \"identifier\" : \"b6ffafab-1cfd-42dd-b8cb-7a554efaefa7\",\n",
        "    \"landingPage\" : \"http://www.cms.gov/Research-Statistics-Data-and-Systems/Computer-Data-and-Systems/Privacy/Systems-of-Records-Items/09-70-0501-MCS.html\",\n",
        "    \"license\" : \"http://www.usa.gov/publicdomain/label/1.0/\",\n",
        "    \"modified\" : \"2014-09-30\",\n",
        "    \"rights\" : \"Contains personally identifiable information and is subject to the Privacy Act of 1974, as amended at 5 United States Code (U.S.C.) 552a.  Requests should be directed to the appropriate System Manager, identified in the System of Records notice.\",\n",
        "    \"primaryITInvestmentUII\" : \"009-000004256, 009-000004254\",\n",
        "    \"systemOfRecords\" : \"09-70-0501\",\n",
        "\n",
        "    \"contactPoint\" : {\n",
        "      \"@type\" : \"vcard:Contact\",\n",
        "      \"fn\" : \"Health Data Initiative\",\n",
        "      \"hasEmail\" : \"mailto:Healthdata@hhs.gov\"\n",
        "    },\n",
        "    \"bureauCode\" : [ \"009:38\" ],\n",
        "    \"keyword\" : [ \"medicare\", \"part b\", \"claims\" ],\n",
        "    \"programCode\" : [ \"009:078\" ],\n",
        "    \"theme\" : [ \"Medicare\" ],\n",
        "    \"publisher\" : {\n",
        "      \"@type\" : \"org:Organization\",\n",
        "      \"name\" : \"Centers for Medicare & Medicaid Services\",\n",
        "      \"subOrganizationOf\" : {\n",
        "        \"@type\" : \"org:Organization\",\n",
        "        \"name\" : \"Department of Health & Human Services\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "Question: rights'タグに「個人を特定できる情報を含む」とあるので、piiはTrueです。\n",
        "modified' タグは '2014-09-30' です。現在の年は2023年で、2023年から2014年を引くと9なので、年齢は9である。\n",
        "キーワードを決定するために、データセットを記述するすべてのフィールドを調べる。\n",
        "そして、そのフィールドの最も顕著で特徴的な点を取り上げて、それをキーワードにする。\n",
        "すべてのフィールドを見ると、データセットを説明するフィールドは「description」と「title」である。\n",
        "タイトル」フィールドは「Medicare Multi-Carrier Claims System」である。\n",
        "title 「フィールドから得られる良いキーワードは、」medicare 「と 」claims \"である。\n",
        "description 「フィールドは、」「この記録システムの主な目的は、権利を有する受益者に、または受益者に代わ って、医療保険給付金を適切に支払うことである」\"である。\n",
        "description 「フィールドからの良いキーワードは 」medical insurance benefits \"である。\n",
        "両フィールドから提案されるキーワードは、「medicare」、「claims」、「medical insurance benefits 」である。\n",
        "次に 「keyword 」フィールドを検査し、提案されたキーワードがすでに含まれていないことを確認する。\n",
        "keyword 「フィールドには、」medicare「、」part b「、」claims \"というキーワードが含まれている。\n",
        "提案されたキーワードから、「medicare 」はすでに 「keyword 」フィールドに入っているので、出力されるべきではない。\n",
        "残るは「クレーム」と「医療保険給付」である。\n",
        "\n",
        "Output JSON:\n",
        "{\n",
        "  \"pii\" : true,\n",
        "  \"age\" : 9,\n",
        "  \"keywords\" : [\"claims\", \"medical insurance benefits\"]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Prepending the one shot exemplar before the question we want answered.\n",
        "llm_call = f\"{context}{one_shot_exemplar}\\nJSON:{question}\\nAnswer:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbtSBsrpjg56"
      },
      "source": [
        "出力は正しいですが、キーワードのオーバーラップの理由がより明確になる可能性があり、これにより、プロンプトがより堅牢になります。これを改善するために考えてから、1つのソリューションの次のセルをご覧ください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIGy06bNkdNf"
      },
      "outputs": [],
      "source": [
        "few_shot_exemplar = \"\"\"\n",
        "JSON:\n",
        "{\n",
        "\n",
        "    \"@type\" : \"dcat:Dataset\",\n",
        "    \"description\" : \"The primary purpose of this system of records is to properly pay medical insurance benefits to or on behalf of entitled beneficiaries.\",\n",
        "    \"title\" : \"Medicare Multi-Carrier Claims System\",\n",
        "    \"accessLevel\" : \"restricted public\",\n",
        "    \"dataQuality\" : true,\n",
        "    \"identifier\" : \"b6ffafab-1cfd-42dd-b8cb-7a554efaefa7\",\n",
        "    \"landingPage\" : \"http://www.cms.gov/Research-Statistics-Data-and-Systems/Computer-Data-and-Systems/Privacy/Systems-of-Records-Items/09-70-0501-MCS.html\",\n",
        "    \"license\" : \"http://www.usa.gov/publicdomain/label/1.0/\",\n",
        "    \"modified\" : \"2014-09-30\",\n",
        "    \"rights\" : \"Contains personally identifiable information and is subject to the Privacy Act of 1974, as amended at 5 United States Code (U.S.C.) 552a.  Requests should be directed to the appropriate System Manager, identified in the System of Records notice.\",\n",
        "    \"primaryITInvestmentUII\" : \"009-000004256, 009-000004254\",\n",
        "    \"systemOfRecords\" : \"09-70-0501\",\n",
        "\n",
        "    \"contactPoint\" : {\n",
        "      \"@type\" : \"vcard:Contact\",\n",
        "      \"fn\" : \"Health Data Initiative\",\n",
        "      \"hasEmail\" : \"mailto:Healthdata@hhs.gov\"\n",
        "    },\n",
        "    \"bureauCode\" : [ \"009:38\" ],\n",
        "    \"keyword\" : [ \"medicare\", \"part b\", \"claims\" ],\n",
        "    \"programCode\" : [ \"009:078\" ],\n",
        "    \"theme\" : [ \"Medicare\" ],\n",
        "    \"publisher\" : {\n",
        "      \"@type\" : \"org:Organization\",\n",
        "      \"name\" : \"Centers for Medicare & Medicaid Services\",\n",
        "      \"subOrganizationOf\" : {\n",
        "        \"@type\" : \"org:Organization\",\n",
        "        \"name\" : \"Department of Health & Human Services\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "Answer: rights 「フィールドに 」Contains personally identifiable information \"とあるので、piiは本当です。\n",
        "modified 「フィールドは 」2014-09-30 \"です。現在の年は2023年で、2023年から2014年を引くと9なので、年齢は9です。\n",
        "キーワードを決定するために、データセットを記述するすべてのフィールドを調べます。\n",
        "そして、そのフィールドの最も顕著で特徴的な点を取り上げて、それをキーワードにする。\n",
        "すべてのフィールドを見ると、データセットを説明するフィールドは「description」と「title」である。\n",
        "タイトル」フィールドは「Medicare Multi-Carrier Claims System」である。\n",
        "title 「フィールドから得られるよいキーワードは、」medicare 「と 」claims \"である。\n",
        "description 「フィールドは、」The primary purpose of this system of records is to properly pay medical insurance benefits to or behalf of entitled beneficiaries \"である。\n",
        "description 「フィールドからのよいキーワードは 」medical insurance benefits \"である。\n",
        "両フィールドから提案されるキーワードは、「medicare」、「claims」、「medical insurance benefits 」である。\n",
        "次に 「keyword 」フィールドを検査し、提案されたキーワードがすでに含まれていないことを確認する。\n",
        "keyword 「フィールドには、」medicare「、」part b「、」claims \"というキーワードが含まれている。\n",
        "提案されたキーワードから、「medicare 」はすでに 「keyword 」フィールドに入っているので、出力されるべきではない。\n",
        "残るは「クレーム」と「医療保険給付金」である。\n",
        "\n",
        "\n",
        "Output JSON:\n",
        "{\n",
        "  \"pii\" : true,\n",
        "  \"age\" : 9,\n",
        "  \"keywords\" : [\"claims\", \"medical insurance benefits\"]\n",
        "}\n",
        "\n",
        "\n",
        "JSON:\n",
        "{\n",
        "  \"@type\": \"dcat:Dataset\",\n",
        "  \"title\": \"Data.gov Top 10 Visiting Countries - Archival\",\n",
        "  \"description\": \"This dataset provides top 10 visiting countries by month in Data.gov up to July 2013.\",\n",
        "  \"modified\": \"2016-01-20\",\n",
        "  \"accessLevel\": \"public\",\n",
        "  \"identifier\": \"GSA-32491\",\n",
        "  \"dataQuality\": true,\n",
        "  \"describedBy\": \"http://www.data.gov/metric\",\n",
        "  \"describedByType\": \"text/csv\",\n",
        "  \"issued\": \"2013-05-13\",\n",
        "  \"license\": \"https://creativecommons.org/publicdomain/zero/1.0/\",\n",
        "  \"spatial\": \"United States\",\n",
        "  \"publisher\": {\n",
        "      \"@type\": \"org:Organization\",\n",
        "      \"name\": \"General Services Administration\"\n",
        "  },\n",
        "  \"accrualPeriodicity\": \"R/P1M\",\n",
        "  \"isPartOf\": \"GSA-2015-09-14-01\",\n",
        "  \"contactPoint\": {\n",
        "      \"@type\": \"vcard:Contact\",\n",
        "      \"fn\": \"Hyon Joo Kim\",\n",
        "      \"hasEmail\": \"mailto:hyon.kim@gsa.gov\"\n",
        "  },\n",
        "  \"distribution\": [{\n",
        "          \"@type\": \"dcat:Distribution\",\n",
        "          \"mediaType\": \"text/csv\",\n",
        "          \"format\": \"text/csv\",\n",
        "          \"title\": \"Data.gov_Top_10_Visiting_Countries.csv\",\n",
        "          \"downloadURL\": \"https://inventory.data.gov/dataset/b0d40da1-a505-476a-a49b-cfc50ea6d9da/resource/0a1a3fb8-a813-4470-b50c-51b7856203be/download/userssharedsdfdata.govtop10visitingcountries.csv\"\n",
        "      }\n",
        "  ],\n",
        "  \"keyword\": [\"Countries\", \"Interactive\"],\n",
        "  \"bureauCode\": [\"023:00\"],\n",
        "  \"programCode\": [\"023:019\"],\n",
        "  \"language\": [\"us-EN\"],\n",
        "  \"theme\": [\"Countries\", \"Top 10\"]\n",
        "  }\n",
        "\n",
        "Answer: accessLevel 「フィールドには 」public \"とあるので、piiはFalseである。\n",
        "modified 「フィールドは 」2016-01-20 \"である。現在の年は2023年で、2023から16を引くと7なので、年齢は8である。\n",
        "キーワードを決定するために、データセットを記述するすべてのフィールドを調べる。\n",
        "そして、そのフィールドの最も顕著で特徴的な点を取り上げて、それをキーワードにする。\n",
        "すべてのフィールドを見ると、データセットを説明するフィールドは「description」と「title」である。\n",
        "title 「フィールドは、」Data.gov Top 10 Visiting Countries - Archival \"である。\n",
        "title 「フィールドから得られる良いキーワードは、」data.gov「、」top 10 \"である。\n",
        "description 「フィールドは 」This dataset provides top 10 visiting countries by month in Data.gov up to July 2013 \"である。\n",
        "description 「フィールドからの良いキーワードは、」top 10 「と 」visiting countries \"である。\n",
        "両方のフィールドから提案された良いキーワードは、「data.gov」、「top 10」、「visiting countries 」である。\n",
        "次に 「keyword 」フィールドを検査し、提案されたキーワードがすでに含まれていないことを確認する。\n",
        "keyword 「フィールドには、」Countries 「と 」Interactive \"というキーワードが含まれています。\n",
        "提案されたキーワードはどれも 「keyword 」フィールドに含まれていない。\n",
        "「data.gov「、」top 10「、」visiting countries \"はすべて新しいキーワードとして認められる。\n",
        "\n",
        "Output JSON:\n",
        "{\n",
        "  \"pii\" : false,\n",
        "  \"age\" : 9,\n",
        "  \"keywords\" : [\"data.gov\", \"top 10\", \"visiting countries\"]\n",
        "}\n",
        "\"\"\"\n",
        "llm_call = f\"{context}{few_shot_exemplar}\\nJSON:{question}\\nAnswer:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKPOkKfax7wB"
      },
      "source": [
        "## zero-shotの思考チェーン（「段階的に考えましょう」）\n",
        "\n",
        "ゼロショットチェーンの考え方は、LLMコールの最後に「トリガー文」を追加するときです。たとえば、「段階的に考えましょう」、「深呼吸をすることから始めます」、または「解決策：」。これは、迅速かつ簡単なパフォーマンスを向上させる方法であり、さまざまなタスクに柔軟に対応できます（一方、少数の思考の連鎖には、質問に似ている必要があります）。\n",
        "\n",
        "ただし、ゼロショットの思考チェーンは、ほぼすべての状況で数ショットのパフォーマンスを低下させます。さらに、ゼロショットの思考チェーンでは、LLMを2回呼び出す必要があります - 応答を生成するために、そして再び応答から答えを抽出する必要があります（応答構造を示す模範がないため）。最後に、ゼロショットのチェーンオブテアは、質問に答えるのではなく、質問を再定義する傾向があります。\n",
        "\n",
        "一般的に、少数のショットチェーンの模範を書くときのインスピレーションを除いて、堅牢なプロンプトをエンジニアリングする場合は、ゼロショットチェーンの考え方は推奨されません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXosOkcbuaTf"
      },
      "source": [
        "##思考の連鎖の利点\n",
        "\n",
        "1. 最小限の努力のための簡単なLLM品質の向上。\n",
        "1. 問題を解決するための手順を口頭で「話す」ことによって解決できるタスクに適用できます。\n",
        "1. 解釈可能性。これにより、デバッグが支援され、エンドユーザーの解釈が必要なユースケースが可能になります。\n",
        "1. 既製のLLMSで動作し、追加のLLMトレーニングやチューニングは必要ありません。\n",
        "1. 異なるLLM間の堅牢性。考えられたチェーンプロンプトからの最終的な出力は、ドリフトを減らします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqPu2gaXexr3"
      },
      "source": [
        "##思考の短所\n",
        "\n",
        "1. 長いLLMコールと応答によるコストの増加。\n",
        "1. 推論時間が遅い。\n",
        "1. 幻覚はまだ可能です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYrjss2N2qnf"
      },
      "source": [
        "##思考のチェーンベストプラクティス\n",
        "\n",
        "これらの推奨事項は、現在の理解を反映しており、LLMはすべて急速に変化しています。これのいくつかは、特定のコーナーケースとLLMアーキテクチャでは間違っている可能性があります。\n",
        "\n",
        "これらのベストプラクティスの例外を見つけた場合は、GitHubの問題を提出することを検討してください。\n",
        "\n",
        "###重要なベストプラクティス\n",
        "\n",
        "思考の連鎖から良いパフォーマンスを得るには、これらのベストプラクティスに従う必要があります。\n",
        "\n",
        "1. 小さなLLMを**使用しない**でください。\n",
        "  * 理想的には、少なくとも15BパラメーターのLLMを使用します。\n",
        "  * 蒸留や改良されたLLMアーキテクチャのような技術が、最終的にこのアドバイスを変えることを期待したい。\n",
        "1. 思考の連鎖の前に答えを書くのではなく、思考の連鎖の後に答えを書く。\n",
        "1. [温度](https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/api-quickstart#try_text_text_prompts)を0に設定する。\n",
        "1. ワンショットやゼロショットだけでなく、数ショットの思考連鎖を使うこと。\n",
        "1. ステップ・バイ・ステップで推論を話すときに言うことをすべて盛り込んだ模範解答を書くこと。\n",
        "  * 思考の連鎖には自然言語による推論が必要だ。\n",
        "  * 自然言語による推論の代わりに数式を使わない。自然言語を補足するために方程式を加えるのは構わない。\n",
        "1. 思考の連鎖が幻覚を止めると決めつけてはいけない。\n",
        "  * 思考の連鎖はLLMの推論能力を向上させるが、LLMが事実をでっち上げることを止めるわけではない。\n",
        "\n",
        "###追加のベストプラクティス\n",
        "\n",
        "思考の連鎖から最大限に活用するためのより多くのヒント。\n",
        "\n",
        "1. Few-Shotプロンプト作成のDo's and Don'ts\n",
        "Don't 過度にFew-Shot事例の順序にこだわる必要はありません。パフォーマンスに変化はないでしょう。\n",
        "\n",
        "  * 分類タスクは例外です。同じクラスの事例を連続して複数提示しないようにしましょう。\n",
        "\n",
        "1. Do Chain-of-Thoughtプロンプトが失敗する箇所を分析し、よくある失敗に対処するためのFew-Shot事例を作成しましょう。\n",
        "\n",
        "1. Don't 最初から6つ以上のFew-Shot事例を作成する必要はありません。タスクによっては、それ以上の事例が役立つ場合もありますが、多くの場合は不要です。\n",
        "\n",
        "1. Do 複数のプロンプトエンジニアにそれぞれ最適なプロンプトを作成してもらいましょう。\n",
        "\n",
        "  * 例えば、3つのタスクがあり、3人のプロンプトエンジニアがいる場合、各エンジニアが1つのタスクに集中するよりも、全員が3つのタスクのプロンプトを作成する方が良い結果が得られるでしょう。\n",
        "\n",
        "1. Don't タスクに必要な推論ステップが1つか2つの場合、Chain-of-Thoughtで結果が改善するとは期待しないでください。\n",
        "\n",
        "1. Don't 事例とタスクの推論ステップ数を厳密に一致させることに気を使いすぎる必要はありません。\n",
        "\n",
        "  * 推論のスタイルや構造を一致させる方が重要です。\n",
        "  * ステップ数を一致させることができればパフォーマンス上の利点がありますが、できなくてもChain-of-Thoughtは依然としてパフォーマンス向上に貢献します。\n",
        "\n",
        "1. Do LLMをチューニングする際にChain-of-Thoughtを追加しましょう。\n",
        "\n",
        "  * LLMに質問と回答からChain-of-Thoughtによる推論を生成させ、その推論をチューニングデータの回答に追加することができます。\n",
        "  * プロンプトとチューニングは二者択一ではありません。チューニングデータの入力が適切に設計されたプロンプトを含む場合、最適なチューニングモデルパフォーマンスが得られます。\n",
        "\n",
        "1. Do データ分布に一致する事例を含めましょう。\n",
        "\n",
        "  * 例えば、データがクラスA 80%、クラスB 20%で、5つのFew-Shot事例を作成する場合、4つの事例はクラスA、1つの事例はクラスBにするべきです。\n",
        "  * 分類タスクでは事例の順序が重要になることもありますが、クラス分布を一致させることで順序に対する頑健性が高まります。\n",
        "  * 連続して同じクラスの事例を複数提示しないように注意しましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY8sKdk9fN3Z"
      },
      "source": [
        "##自己整合性\n",
        "\n",
        "自己整合性は、一連の思考プロンプトのパフォーマンスを改善するための手法です。同じLLMコールを複数回呼び出して、最も一般的な答えを出します。\n",
        "\n",
        "これは、温度= 0で思考の連鎖を使用するためのルールを「破る」ことを意味します。\n",
        "\n",
        "自己整合性の背後にある直観は次のとおりです。\n",
        "1.同一のLLM呼び出しに対する複数の応答は、応答のさまざまな推論パスを意味します。\n",
        "1.誤った推論パスは、異なる誤った回答につながります。\n",
        "1.正しい推論パスは同じ正解につながります。\n",
        "1.いくつかの正解と多くの誤った答えしか得られないかもしれませんが、正解は一意の誤った答えよりも一般的です。\n",
        "\n",
        "自己整合性を試してみましょう。まず、温度0でこの次のLLM呼び出しを実行して、誤った応答を生成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYKVZ8iHhf1d"
      },
      "outputs": [],
      "source": [
        "# The answer is 1300 + 100 (maintenance) + 75 (upgrade) = 1475.\n",
        "question = \"\"\"工場の基本生産性は1日100ユニットである。\n",
        "すべての工場が基本生産性を有しているわけではない。\n",
        "工場がアップグレードされているときは、ベースラインの生産性の 25％である。\n",
        "工場がメンテナンスを受けているときは、ベースラインの 50％である。\n",
        "工場が労働活動中の場合、何も生産しない。\n",
        "メガコープには合計 19 の工場がある。\n",
        "3 工場がアップグレード中。\n",
        "2 工場がメンテナンス中。\n",
        "1 工場が労働活動中である。\n",
        "メガコーポは1日に何ユニットを生産するか？\"\"\"\n",
        "\n",
        "context = \"\"\"完全な計算と推論を示す質問に答える。\n",
        "例題のパターンに従いましょう。\n",
        "\"\"\"\n",
        "\n",
        "one_shot_exemplar = \"\"\"Q: 通常のテニスボールは5個入ります。\n",
        "大きなテニスボール缶は、通常のテニスボール缶の200％のボールを入れることができます。\n",
        "小さなテニスボール缶には、通常のテニスボール缶の40%のボールが入ります。\n",
        "コレクタブルテニスボール缶には、テニスボールは入りません。\n",
        "ロジャーは10個のテニスボール缶を持っている。\n",
        "3つの缶は大きい缶である。\n",
        "4缶は小さい缶。\n",
        "1缶はコレクション缶である。\n",
        "ロジャーは何個のテニスボールを持っていますか？\n",
        "A: 通常のテニスボール缶の数を求める必要がある。\n",
        "ロジャーは、10(合計)-3(大きい)-4(小さい)-1(収集可能)=2個のレギュラー缶を持っています。\n",
        "大きなテニスボール缶には、5個の200％＝10個のテニスボールが入ります。\n",
        "小さなテニスボール缶には、5の40％＝2個のテニスボールが入ります。\n",
        "次に、それぞれの缶の種類から何個のボールが出るかを数える。\n",
        "大きい缶3個は、3 * 10 = 30個のテニスボール。\n",
        "小さい缶4個は、2 * 4 = 8個のテニスボール。\n",
        "レギュラー缶2個は、2 * 5 = 10テニスボール\n",
        "コレクタブル缶1個はテニスボール0個\n",
        "答えを出すには、それぞれの缶の種類のボールの数を足します。\n",
        "ロジャーは30個（大）＋8個（小）＋10個（レギュラー）＋0個（コレクタブル）＝48個のボールを持っている。\n",
        "答えは48個です。\n",
        "\n",
        "Q: \"\"\"\n",
        "\n",
        "llm_call = f\"{context}\\n{one_shot_exemplar}{question}\\nA:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfyjnV8Clxia"
      },
      "source": [
        "次に、「温度」を.7に上げ、高い「TOP_P」と「TOP_K」値を使用して異なる応答を生成します。\n",
        "\n",
        "次のセルを数回実行し、答えがどのように変化するかに注意してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqr8DxNylcC1"
      },
      "outputs": [],
      "source": [
        "sc_parameters = {\n",
        "    \"temperature\": .7,\n",
        "    \"max_output_tokens\": 512,\n",
        "    \"top_p\": 1,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "\n",
        "_ = call_llm(model, sc_parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrbTQUGymnUr"
      },
      "source": [
        "上記のコードを再実行すると、さまざまな推論と回答が表示されます。\n",
        "\n",
        "次に、多くの応答をループして生成し、回答を抽出し、回答を最も一般的で最も一般的ではないように出力します。\n",
        "\n",
        "これには数分かかります。実行中は、さまざまな推論と回答に注意してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L1KRC6Hm5Ir"
      },
      "outputs": [],
      "source": [
        "from collections import Counter  # Easy counting of most common responses.\n",
        "sc_runs = 40\n",
        "responses = [None] * sc_runs\n",
        "answers = [None] * sc_runs\n",
        "\n",
        "for i in range(0, sc_runs):\n",
        "  print(f\"Response {i}...\")\n",
        "  responses[i] = call_llm(model,\n",
        "                          sc_parameters,\n",
        "                          llm_call,\n",
        "                          # Turn off printing LLM calls/responses.\n",
        "                          show_activity=False)\n",
        "  # If the response doesn't contain 'The answer is', the split fails.\n",
        "  # The split also fails if the answer contains a decimal or comma.\n",
        "  try:\n",
        "    answers[i] = responses[i].split(\"The answer is\")[1].split(\".\")[0].strip()\n",
        "  except Exception as e:\n",
        "    answers[i] = \"NA\"\n",
        "  print(responses[i])\n",
        "print(\"最も一般的なものから最も一般的でないものまで、答えとカウント：\")\n",
        "print(Counter(answers).most_common())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxZ2S8hd9f33"
      },
      "source": [
        "上記のセルからの最後の出力は、異なる回答のカウントです。正解（1475）は、最も一般的な答えとして戻ってくるはずです。\n",
        "\n",
        "LLMの呼び出しが多いほど、最も一般的な答えは正しい答えです。\n",
        "\n",
        "また、結果をプロットして、回答の分布を視覚化することもできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfJiXg_qWB0A"
      },
      "outputs": [],
      "source": [
        "# Thanks to Hans-Christian Fuchs for this.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(Counter(answers).keys(), Counter(answers).values())\n",
        "ax.tick_params(axis='x', rotation=55)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyMEmx1J_osN"
      },
      "source": [
        "###自己整合性の利点\n",
        "\n",
        "1. 低エフォルトのパフォーマンスブースト。\n",
        "1. 思考の模範を助けます。\n",
        "1. 異なるLLMにわたる迅速な堅牢性の増加。\n",
        "1. 回答分布に基づいて、擬似「信頼」推定値を提供します。\n",
        "1. 単一の正解なしで問題に対して「平均」回答を使用する機会。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsaFThs-_pyG"
      },
      "source": [
        "###自己整合性の欠点\n",
        "\n",
        "1. コストの増加。\n",
        "1. 推論時間の遅いおよび/またはスループットの削減。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov281oL--eRh"
      },
      "source": [
        "### 自己整合のベストプラクティス\n",
        "\n",
        "1. Do temperature=.7、top_k=40、top_p=1、そして10個のレスポンスを初期設定として使用しましょう。\n",
        "\n",
        "  * ユースケースによって異なる値が必要になる場合があるため、そこから実験を行いましょう。\n",
        "  * 本番環境で使用する最適な値を見つけるために、ハイパーパラメータ探索を実施しましょう。\n",
        "  * LLMパラメータよりもレスポンス数を探索する方がはるかに価値がある可能性が高いことに注意してください。また、LLMパラメータを試す場合でも、それらを大幅に減らすことは通常価値がありません。\n",
        "\n",
        "1. Do 初期のプロンプトエンジニアリングの試みが失敗した場合、早めに自己整合性を試しましょう。\n",
        "\n",
        "  * 自己整合性は、連鎖思考プロンプトのエンジニアリングを続けるよりも、パフォーマンスを向上させる可能性が高いです。\n",
        "\n",
        "1. Don't コストとレイテンシの影響を無視しないでください。\n",
        "\n",
        "1. Do 実行時間を短縮するためにLLM呼び出しを並列化しましょう。\n",
        "\n",
        "  * 自己整合性ユースケースに必要なLLMスループットとレイテンシの評価を後回しにしないでください。\n",
        "\n",
        "1. Do レスポンス分布を創造的な方法で使用しましょう。例えば：\n",
        "\n",
        "  * Xパーセント未満の回答しか一致しない場合、その質問にフラグを立てて人間のレビューに回しましょう。\n",
        "  * 複数の要約を生成し、テキスト類似性指標を使用して、どの生成された要約が最も「平均的」かを特定しましょう。\n",
        "\n",
        "1. Do 自己整合性をFew-Shot事例の作成やプロンプトのデバッグに活用しましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhA8gbnohLo7"
      },
      "source": [
        "# パート2：アクション、検索、ツールの使用\n",
        "\n",
        "LLMは、カラスのように、ツールを使用するのに熟達しています。\n",
        "\n",
        "<img src = \"https://raw.githubusercontent.com/GoogleCloudPlatform/specialized-training-content/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/images/3-crow.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD2iMx2wwBmN"
      },
      "source": [
        "##幻覚、接地、ツール/アクション/検索/ぼろきれ\n",
        "<a name=\"rag\"> </a>\n",
        "\n",
        "LLMは信頼できる事実源ではありません。LLM応答に正しい事実が含まれている場合、LLMのパラメーターが実際にエンコードするものの緊急効果です。単語間の確率的関係です。\n",
        "\n",
        "事実の正確性が重要な場合、これらの確率的関係に依存することは危険です。\n",
        "\n",
        "また、LLMSは、最新情報について迅速または安価に再訓練することもできません。また、再訓練が可能性がある場合でも、壊滅的な忘却は、トレーニングデータセットが増加するにつれて、古い情報の新しいエラーにつながる可能性があります。\n",
        "\n",
        "LLM応答が事実上正しくない場合、それはしばしば「幻覚」と呼ばれますが、より正確には[妄想](https://en.wikipedia.org/wiki/delusion)です。\n",
        "\n",
        "幻覚は非専門家によって見逃される可能性があります。LLM応答は、生成されたテキストが文法的に正確で、よく形成され、トーンに自信がある場合でも、事実上正しくありません。\n",
        "\n",
        "このLLM呼び出しが出力するものを参照してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD5WUUvDuHuD"
      },
      "outputs": [],
      "source": [
        "question = \"ドイツの首相は誰ですか？\"\n",
        "_ = call_llm(model, parameters, question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM5MjaAjm77V"
      },
      "source": [
        "現在のモデルは正しく反応する可能性がありますが、2023年8月、メルケル首相が辞任してからほぼ2年後、これが応答でした。\n",
        "<img src = \"https://raw.githubusercontent.com/GoogleCloudPlatform/specialized-training-content/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/images/6-hallucinate.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAENyzS3o2YO"
      },
      "source": [
        "幻覚を管理する最良の方法は、LLMを正確で最新の外部データソースに接続することです。\n",
        "\n",
        "「接地」とは、外部情報を使用して幻覚を管理することです。「グラウンド」する1つの方法は、挿入された情報に基づいて応答を基にするための手順とともに、外部情報をLLMコールに挿入することです。\n",
        "\n",
        "「検索拡張生成」または「ラグ」は、LLMが外部知識を使用していると言う一般的な方法です。それは異なることを意味する可能性があります：\n",
        "1. 外部検索システムは、ユーザークエリを入力として取得し、情報を出力し、LLMコールのユーザークエリと組み合わせます。（たとえば、クエリの埋め込みをドキュメントの埋め込みと比較し、LLMコールに最も近いドキュメントを挿入します）。[コードサンプル](https://github.com/googlecloudplatform/generative-ai/blob/main/language/use-cases/document-qa/question_answering_documents_langchain_matching_engine.ipynb)。\n",
        "\n",
        "1. ユーザーのクエリに基づいて外部情報システムへの検索コールを策定する手順でLLMを呼び出し、ユーザーのクエリと取得情報を組み合わせてLLMに別の呼び出しを行います。\n",
        "\n",
        "3. 結合したオーダーメイドレトリバーとジェネレーターディープラーニングモデルを一緒に訓練/調整しました（元の[RAG Paper](https://arxiv.org/pdf/2005.11401.pdf)の焦点）。\n",
        "\n",
        "このノートブックは# 2に焦点を当てており、言語「ツール」/「ツール使用」を使用して、LLMを指示するように外部システムを使用するように説明し、あいまいな用語のぼろを避けます。パート3の後半では、「アクション」と「演技」を使用して、Reactが議論される方法と一致します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVj0W1lihch4"
      },
      "source": [
        "## LLMツールの使用方法\n",
        "\n",
        "LLMツールの使用の基本パターンは次のとおりです。\n",
        "1. 以下を説明する最初のLLMコールを作成します。\n",
        "* I：完了したいタスク。\n",
        "* II：外部システム。\n",
        "* III：外部システムへの呼び出しを策定する方法。\n",
        "2. LLMによって生成された応答を使用して外部システムを呼び出します。\n",
        "3.外部システムからの応答を含む2番目のLLMコールを作成し、LLMが外部システムからの応答を使用して元のタスクを完了するように指示します。\n",
        "\n",
        "私たちのLLMシステムが、上記の首相の例のような事実ベースの質問に答えることになっている場合：\n",
        "1. 最初のLLMコールは、LLMを指示して、知識ベースの検索クエリを生成します。\n",
        "2. LLMの応答は、知識ベースを照会するために使用され、クエリの結果がキャプチャされます。\n",
        "3. 2番目のLLMコールには、ナレッジベースクエリ、元の質問、およびLLMがナレッジベースクエリの結果を使用して質問に答えるための指示の結果が含まれます。\n",
        "\n",
        "LLMのツールは、データベース、Web検索、ドキュメント検索システムなど、多くのものになる可能性があります。LLMシステムの一部は、LLMを外部情報ソースと統合するコードです。\n",
        "\n",
        "このノートブックでは、ウィキペディアを外部情報ソースとして使用し、基本的なLLMシステムを構築して、事実ベースの質問に答えます。私たちのLLMシステムは次のとおりです。\n",
        "1. LLMを呼び出して、ウィキペディア検索クエリを生成します。\n",
        "1. ウィキペディアAPIを呼び出して、クエリの結果を取得します。\n",
        "1. Wikipedia API応答と元の質問を使用して、LLMをもう一度呼び出します。\n",
        "\n",
        "このノートブックの範囲を超えて、LLMSは、複数のツールを説明するInstandinosで呼び出すことができます。LLMはどちらもツールを選択し、ツールへの呼び出しを策定します。また、LLMツールは読み取り専用である必要はありません。ツールを使用して外部システムと対話できます（ただし、倫理と公平性への影響を考慮してください。Tは、 *幻覚は概要を尋ねたいと思っていますが、自動化された紙のグレーディングに影響を与えるように、誰かの人生に影響を与える可能性があるという決定を下すと、壊滅的です。誰かの人生）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91FoLDpruqF4"
      },
      "source": [
        "##サンプルツール\n",
        "\n",
        "以下の関数はクエリを取り、クエリのトップウィキペディア記事マッチを返し、記事の最初の `return_chars`文字を取得します。\n",
        "\n",
        "このツールは教育目的であり、やや制限されています。リストやサイドバーにアクセスすることはできず、提案をうまく処理せず、ウィキペディアの記事内での検索をサポートせず、常に結果を返すとは限りません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cLj2TiCt0cn"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "def wiki_tool(query, return_chars = 1000):\n",
        "  try:\n",
        "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
        "  # If no exact match, take Wikipedia's auto-suggestion.\n",
        "  except wikipedia.exceptions.PageError as e:\n",
        "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
        "  snippet = page[0:return_chars]\n",
        "  return snippet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRZ6v1z0uWAd"
      },
      "source": [
        "ツールを試してください："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4o-3Td9uZ-U"
      },
      "outputs": [],
      "source": [
        "wiki_tool(\"ドイツ首相\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7gGYXbxs8b7"
      },
      "source": [
        "##チェーンLLMはツールの使用を求めています\n",
        "\n",
        "基本的な2段階のツール使用LLMチェーンには、ここで段階的に分類されているいくつかのピースが含まれています。\n",
        "\n",
        "この例でモデルを（2023年10月の時点で）、あいまいなミュージシャンについての質問に電話すると、誤った答えが幻覚を起こします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHK1aJ_oXtJZ"
      },
      "outputs": [],
      "source": [
        "question = \"アルバム「Somebody in the Snow」をリリースしたミュージシャンは？\"\n",
        "_ = call_llm(model, parameters, question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nqc0vhU5H1X"
      },
      "source": [
        "###ステップ1：ツールを使用するためのLLMの指示を提供する\n",
        "\n",
        "LLMに、タスクとツールの使用方法の両方の指示を提供する必要があります。\n",
        "\n",
        "LLM呼び出しのこの「命令」部分は、「コンテキスト」または「条件」（「コンディショニング」、「コンディショニングプロンプト」）のバリエーションと呼ばれることがあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhWpoRFGA21n"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"ウィキペディアの検索を使って質問に答えなさい。\n",
        "各質問の後に、ウィキペディア検索の後に'<STOP>'を書く。\n",
        "ウィキペディア検索は、最も関連性の高いコンテンツを検索するために使用されます。\n",
        "ウィキペディアの記事の一部が次のLLMコールに送られます。\n",
        "ウィキペディアの記事のテキストを使って質問に答えてください。\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqWY6f3EBDyO"
      },
      "source": [
        "###ステップ2：模範を提供します\n",
        "\n",
        "LLMには、ツールを使用してタスクを完了する方法を示す模範が必要です。\n",
        "\n",
        "この例には、1ショットの模範的なものしかありません。\n",
        "\n",
        "この模範のウィキペディアの記事のテキストは、2023年8月に「wiki_tool（ \"chancellor ofドイツ\"）を実行しています。\n",
        "\n",
        "注：将来の再試行の後、LLMは外部ツールなしでこの質問に正しく答えます。しかし、このワンショットの模範は、ウィキペディア検索のパターン、応答、および応答に基づいた回答を示すため、まだ機能します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Haoj8nWSA_fy"
      },
      "outputs": [],
      "source": [
        "exemplar = \"\"\"質問 ドイツの首相は誰ですか？\n",
        "ウィキペディアで検索： ドイツ首相<STOP\n",
        "ウィキペディアの記事 ドイツ首相（ドイツれんぽうしゅしょう）、通称ドイツ首相は、ドイツ連邦共和国の首相であり、ドイツ連邦政府の首班である。首相は連邦内閣の最高責任者であり、行政府の長である。現在の首相はSPDのオラフ・ショルツで、アンゲラ・メルケルの後任として2021年12月に選出された。現在の首相はSPDのオラフ・ショルツで、アンゲラ・メルケルの後任として2021年12月に選出された。SPDが同盟90/緑の党、FDPと連立協定を結んだ後に選出された。この称号は、ドイツ語圏ヨーロッパのいくつかの州で使われたこともある。近代的な大法官職が確立されたのは、神聖ローマ帝国時代からである。\n",
        "答えは？オラフ・ショルツ\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deMaQ9ddDQhc"
      },
      "source": [
        "###ステップ3：LLMチェーンで最初の呼び出しを行う\n",
        "\n",
        "私たちのコンテキストと模範を質問と組み合わせて、Wikipediaの検索クエリを回答として要求するLLMに電話をかけます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC4l5oHtD9OO"
      },
      "outputs": [],
      "source": [
        "step_one_call = f\"\"\"{context}\n",
        "\n",
        "{exemplar}\n",
        "\n",
        "Question: {question}\n",
        "Wikipedia Search:\"\"\"\n",
        "step_one_response = call_llm(model, parameters, step_one_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDUi5JB8GCL9"
      },
      "source": [
        "###ステップ4：LLMの応答を使用してツールをクエリします\n",
        "\n",
        "注LLM応答には、Wikipedia検索クエリ以上のものが含まれています。\n",
        "\n",
        "LLMSは、LLMコールのトークンと以前に予測されたトークンのトークンに基づいて、次のトークンを何度も繰り返し予測することで機能します。これは、LLMが過剰なテキストを生成することを意味します。ウィキペディア検索クエリの後に停止することはわかりません。\n",
        "\n",
        "ウィキペディアの検索クエリを超えたすべてのものはごみです。余分なテキストは「<stop> `signifierを使用して破棄されますが、これはラインブレークでも実行できます。\n",
        "\n",
        "生産システムでは、このようなLLMコールを行うときに応答サイズを制限することにより、コストを制御することが重要です。\n",
        "\n",
        "次の関数は、最初のチェーンステップからLLM応答を取得し、ウィキペディアクエリを返します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2cqh5R4HTHV"
      },
      "outputs": [],
      "source": [
        "def get_wiki_query (llm_response, stop_text = \"<STOP>\"):\n",
        "  # Assumes the query is in the first line.\n",
        "  first_line = llm_response.splitlines()[0]\n",
        "  query = first_line.split(stop_text)[0]\n",
        "  return query.strip() # Remove leading and trailing whitespace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sv6ox89JYPe"
      },
      "source": [
        "以前のLLMコールからの応答でこの関数を使用してクエリを抽出し、「wiki_tool」を使用してウィキペディアを検索します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d5CKJRyJW5C"
      },
      "outputs": [],
      "source": [
        "wiki_query = get_wiki_query(step_one_response)\n",
        "print(f\"Tool Query: {wiki_query}\")\n",
        "wiki_text = wiki_tool(wiki_query)\n",
        "print(f\"Wikipedia Snippet: {wiki_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAmH5sQddF9Q"
      },
      "source": [
        "###ステップ5：ツール応答を使用して、LLMチェーンで2回目の呼び出しを行う\n",
        "\n",
        "次に、ツールから出力を取得し、2番目のLLMコールを作成して質問に答えます。\n",
        "\n",
        "LLMツールの使用は、一般に、以前の呼び出しと応答の履歴を維持しています。チェーン内の2番目の呼び出しを作成するには：\n",
        "1. チェーン内の最初のLLMコールから始めます。\n",
        "1. 以前に生成されたウィキペディアクエリを追加します。\n",
        "1. ウィキペディアの検索結果を追加します。\n",
        "\n",
        "これが私たちの最初の呼び出しがどのように見えるかを思い出させます："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJKx_TKAdmRz"
      },
      "outputs": [],
      "source": [
        "print(step_one_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCCGiIZuChoA"
      },
      "source": [
        "この最初のLLMコールは、最初のLLM応答からのクエリと、ウィキペディアツールからの出力と、模範と一致する構造と組み合わされます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRsLkHfRd3hY"
      },
      "outputs": [],
      "source": [
        "step_two_call = f\"\"\"{step_one_call} {wiki_query}\n",
        "Wikipedia Article: {wiki_text}\n",
        "Answer: \"\"\"\n",
        "step_two_response = call_llm(model, parameters, step_two_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU-UI3mnKPLq"
      },
      "source": [
        "##すべてのステップをまとめる\n",
        "\n",
        "下のこのコードスニペットは、上記のすべての手順、従属パッケージ、および従属関数を2段階のツール使用LLMチェーンを管理する単一の関数に収集します。\n",
        "\n",
        "適切なパッケージをインストールして認証されたと仮定して、このコードを独自のプロジェクトにコピーして貼り付けることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o__JbR9LKiNX"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "\n",
        "def call_llm(model, parameters, llm_call, show_activity = True):\n",
        "  # Wraps an LLM call to Vertex, optionally displaying the call and response.\n",
        "  response = model.predict(llm_call, **parameters).text\n",
        "\n",
        "  if show_activity:\n",
        "    BOLD = \"\\033[1m\"\n",
        "    UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
        "    print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
        "    print(f\"{BOLD}The response:{UNFORMAT}\")\n",
        "    print(response)\n",
        "\n",
        "  return response  # Return to `_` if not needed.\n",
        "\n",
        "\n",
        "def wiki_tool(query, return_chars = 1000):\n",
        "  try:\n",
        "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
        "  # If no exact match, take Wikipedia's suggestion.\n",
        "  except wikipedia.exceptions.PageError as e:\n",
        "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
        "  snippet = page[0:return_chars]\n",
        "  return snippet\n",
        "\n",
        "\n",
        "def get_wiki_query (llm_response, stop_text = \"<STOP>\"):\n",
        "  # Extract the wikipedia query from the LLM response.\n",
        "  # Assumes the query is in the first line.\n",
        "  first_line = llm_response.splitlines()[0]\n",
        "  query = first_line.split(stop_text)[0]\n",
        "  return query.strip() # Remove leading and trailing whitespace\n",
        "\n",
        "\n",
        "def wiki_tool_chain(model,\n",
        "                    parameters,\n",
        "                    context,\n",
        "                    exemplar,\n",
        "                    question,\n",
        "                    show_activity=False):\n",
        "  # Answer a query using wikipedia by calling an LLM.\n",
        "  step_one_call = (\n",
        "      f\"{context}\\n\\n{exemplar}\\n\\nQuestion: {question}\\nWikipedia Search:\"\n",
        "  )\n",
        "  if show_activity:\n",
        "    print(\"\\033[1mMaking the first LLM call...\\033[0m\\x1B[0m\")\n",
        "  step_one_response = call_llm(model, parameters, step_one_call, show_activity)\n",
        "  wiki_query = get_wiki_query(step_one_response)\n",
        "  wiki_text = wiki_tool(wiki_query)\n",
        "\n",
        "  step_two_call = (\n",
        "      f\"{step_one_call} {wiki_query}\\nWikipedia Article: {wiki_text}\\nAnswer: \"\n",
        "  )\n",
        "  if show_activity:\n",
        "    print(\"\\033[1mMaking the second LLM call...\\033[0m\\x1B[0m\")\n",
        "  step_two_response = call_llm(model, parameters, step_two_call, show_activity)\n",
        "\n",
        "  return step_two_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l9ChpYxWlS3"
      },
      "source": [
        "上記のコードを使用する例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChHBEqg7MQCZ"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "# Outside this notebook, set PROJECT_ID, LOCATION, and MODEL_NAME.\n",
        "# When running in the notebook, these are set in part 0.\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "# These settings control how deterministic the LLM response is.\n",
        "parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "context = \"\"\"ウィキペディアの検索を使って質問に答えなさい。\n",
        "各質問の後に、ウィキペディア検索の後に'<STOP>'を書く。\n",
        "ウィキペディア検索は、最も関連性の高いコンテンツを検索するために使用されます。\n",
        "ウィキペディアの記事の一部が次のLLMの呼び出しに送られます。\n",
        "ウィキペディア記事のテキストを使って質問に答えてください。\"\"\"\n",
        "\n",
        "exemplar = \"\"\"質問:ドイツの首相は誰ですか?\n",
        "ウィキペディア検索: ドイツ首相<STOP>\n",
        "ウィキペディアの記事: ドイツ首相、正式にはドイツ連邦共和国連邦首相は、ドイツ連邦政府の長であり、戦時中のドイツ軍の最高司令官です。首相は連邦内閣の最高責任者であり、行政府の長です。首相は連邦大統領の提案に基づき、議論を経ずに連邦議会によって選出される（ドイツ憲法第63条）。現在の首相はアンゲラ・メルケル首相の後任として2021年12月に選出されたSPDのオラフ・ショルツ氏である。同氏は、SPD が同盟 90/緑の党および FDP と連立協定を結んだ後に選出されました。\\n\\n\\n== 首相の職の歴史 ==\\n首相の職には長い歴史があり、その起源は聖公会にまで遡ります。ローマ帝国、当時ドイツの大宰相の職は通常マインツ大司教が務めていた。このタイトルは、ドイツ語圏のヨーロッパのいくつかの州で時々使用されました。近代的な首相官邸は、\n",
        "答え：オラフ・ショルツ\"\"\"\n",
        "\n",
        "answer = wiki_tool_chain(model,\n",
        "                         parameters,\n",
        "                         context,\n",
        "                         exemplar,\n",
        "                         \"アルバム「Somebody in the Snow」をリリースしたミュージシャンは？\",\n",
        "                         show_activity = False)\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oukbFAyxoNPR"
      },
      "source": [
        "`show_activity = true`を使用して、LLMコールの内訳を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU3h3GkcbgUn"
      },
      "outputs": [],
      "source": [
        "wiki_tool_chain(model,\n",
        "                parameters,\n",
        "                context,\n",
        "                exemplar,\n",
        "                \"アルバム「Somebody in the Snow」をリリースしたミュージシャンは？\",\n",
        "                show_activity = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjKjBgbMXWyZ"
      },
      "source": [
        "「質問」を変更して実験してみてください。`show_activity = true`を保持して、LLMチェーンの2つのステップを確認します。\n",
        "\n",
        "これは多くの質問ではうまくいきません。上記のように、私たちのツールはあまり良くなく、いくつかの質問で完全に失敗します。\n",
        "\n",
        "ツールの使用ベストプラクティスは[パート3で詳細](https://github.com/GoogleCloudPlatform/specialized-training-content/blob/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/#react-tools)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH4Z5cqnmkpM"
      },
      "source": [
        "# パート3：反応（推論 +演技）プロンプト\n",
        "\n",
        "React（推論 +アクション）は、外部システムと対話することにより、複雑なタスクを介して、思考とツールの使用のチェーンとツールの使用を組み合わせて合計します。\n",
        "\n",
        "Reactスタイルのプロンプトは、現在（2023年秋）ほとんどのプロンプト駆動型LLMタスクの最先端です。LLMまたはLLMベースのチャットボットまたはシステムが外部システムと対話するプラグインまたは拡張機能を使用すると、Reactスタイルのシステムを使用しています。一般に、最新の知識を反映するLLMシステムは、Hood-The-Hoodの下で反応スタイルの機能を使用して目に見えません。\n",
        "\n",
        "外部システムと対話しようとするLLM：\n",
        "\n",
        "<img src = \"https://raw.githubusercontent.com/GoogleCloudPlatform/specialized-training-content/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/images/4-robot.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mepNl0JxmsTF"
      },
      "source": [
        "##反応の基本\n",
        "\n",
        "反応チェーンには通常、3つのインターリーブパーツがあります。\n",
        "-  **思考**：思考の連鎖のように、これらは最終出力に向けて進歩するため、LLMによって生成されるウェイポイント、計画、推論などです。\n",
        "-  **アクション**：LLMが生成したコマンド、呼び出し、または外部システムにアクセスする手順。外部システムは、情報を提供するツールである可能性がありますが、より一般的なものである可能性があります（つまり、アクションは外部システムの状態を観察または変更します）。\n",
        "-  **観測**：外部システムからの応答、フィードバック、結果など。LLMコールに挿入されて次の思考を生成します。\n",
        "\n",
        "これらの3つのステップは、LLMがタスクを完了するまで繰り返されます。\n",
        "\n",
        "考え方のプロンプトと同様に、この繰り返されるサイクルは「内部の独白」または「内部スピーチ」を形成しますが、行動する決定を重要な追加と、単なる推論を超えて行動からフィードバックします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cZ-EbgBm5Zz"
      },
      "source": [
        "###反応チェーンはどのように見えますか\n",
        "\n",
        "ReactチェーンでLLMコールを分解する前に、完全なReactチェーンがどのように見えるかを確認するのに役立ちます。\n",
        "\n",
        "このチェーンでのアクションはウィキペディアの検索であり、観察はウィキペディアの記事からのスニペットです。\n",
        "\n",
        "LLMへの元の呼び出しは次のとおりです。\n",
        "「質問：誰が最初に生まれたのか、ロナルド・リーガンまたはジェラルド・フォード？」（今のところ、指示、模範などを無視して）。\n",
        "\n",
        "完成したReactチェーンはこのように見えます。完全な観察結果を読むために右にスクロールします。\n",
        "\n",
        "```\n",
        "質問：最初に生まれたのは誰ですか、ロナルドレーガンまたはジェラルドフォード？\n",
        "考え1：ロナルド・レーガンを調べて、彼がいつ生まれたのかを見る必要があります。\n",
        "アクション1：ロナルドレーガン<Stop>\n",
        "観察1：ロナルド・ウィルソン・レーガン（1911年2月6日 -  2004年6月5日）は、1981年から1989年まで米国の第40代大統領を務めたアメリカの政治家および俳優でした。そして、最初の離婚した大統領。レーガンはイリノイ州タンピコで生まれ、イリノイ州ディクソンで育ちました。彼はユーレカ大学で教育を受け、そこで経済学と社会学を学びました。卒業後、レーガンはカリフォルニアに移り、そこでラジオスポーツアナウンサーになりました。彼は後に演技に引っ越し、50以上の映画に出演しました。レーガンは、1947年から1952年までスクリーン俳優ギルドの社長を務めました。\n",
        "考え2：ロナルド・レーガンは1911年に生まれました。ジェラルド・フォードを調べて、彼がいつ生まれたのかを見る必要があります。\n",
        "アクション2：ジェラルドフォード<Stop>\n",
        "観察2：ジェラルド・ルドルフ・フォード・ジュニア（jerr-əld;生まれたレスリー・リンチ・キング・ジュニア、1913年7月14日 -  2006年12月26日）は、1974年から1977年に米国第38代大統領を務めたアメリカの政治家でした。以前は、1965年から1973年まで米国下院で共和党の指導者を務め、スピロアグニューの辞任の後、リチャードニクソン大統領によって40番目の副大統領に任命されました。フォードは、ニクソンが1974年に辞任したときに大統領職に成功しましたが、1976年に完全な任期に選挙で敗北しました。フォードは、大統領または副大統領の選挙を勝ち取らずに米国大統領になった唯一の人物です。フォードはネブラスカ州オマハで生まれ、ミシガン州グランドラピッズで育ちました。彼はミシガン大学に通い、そこで学校のフットボールチームでプレーしてから最終的にイェールロースクールに通いました。その後、彼は1942年から1946年まで米国海軍保護区に勤務しました。フォードは1949年にミシガン州5の米国代表として政治的キャリアを始めました。\n",
        "考え3：ジェラルドフォードは1913年に生まれました。1911年は1913年以前です。回答[ロナルドレーガン]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlDmYHuInLVb"
      },
      "source": [
        "###反応チェーンを分解します\n",
        "\n",
        "上記の例の反応チェーンは、3つのLLM呼び出しから構築されています。\n",
        "\n",
        "注このセクションの回答は、パート2ツールでのディスカッションで追加のテキストがどのように削除されたかと同様に、余分な予測されたテキストを削除されています。\n",
        "\n",
        "**電話1：**\n",
        "```\n",
        "質問：最初に生まれたのは誰ですか、ロナルドレーガンまたはジェラルドフォード？\n",
        "考え1：\n",
        "```\n",
        "**応答1：**\n",
        "```\n",
        "ロナルド・レーガンを調べて、彼がいつ生まれたのかを見る必要があります。\n",
        "アクション1：ロナルドレーガン<Stop>\n",
        "```\n",
        "\n",
        "最初のLLMコールは次のとおりです。\n",
        "1.以前のLLMコールプラス\n",
        "1.前のコールプラスへのLLM応答\n",
        "1.ウィキペディアルックアップ結果プラス\n",
        "1.「思考#：」\n",
        "\n",
        "**コール2：**\n",
        "\n",
        "コール2は、連結コール1 +応答1 +ウィキペディアルックアップの結果（観察中） +「思考2：」によって作成されます。\n",
        "```\n",
        "質問：最初に生まれたのは誰ですか、ロナルドレーガンまたはジェラルドフォード？\n",
        "考え1：ロナルド・レーガンを調べて、彼がいつ生まれたのかを見る必要があります。\n",
        "アクション1：ロナルドレーガン<Stop>\n",
        "観察1：ロナルド・ウィルソン・レーガン（1911年2月6日 -  2004年6月5日）は、1981年から1989年まで米国の第40代大統領を務めたアメリカの政治家および俳優でした。そして、最初の離婚した大統領。レーガンはイリノイ州タンピコで生まれ、イリノイ州ディクソンで育ちました。彼はユーレカ大学で教育を受け、そこで経済学と社会学を学びました。卒業後、レーガンはカリフォルニアに移り、そこでラジオスポーツアナウンサーになりました。彼は後に演技に引っ越し、50以上の映画に出演しました。レーガンは、1947年から1952年までスクリーン俳優ギルドの社長を務めました。\n",
        "考え2：\n",
        "```\n",
        "\n",
        "**応答2：**\n",
        "```\n",
        "ロナルド・レーガンは1911年に生まれました。ジェラルド・フォードを調べて、彼がいつ生まれたのかを見る必要があります。\n",
        "アクション2：ジェラルドフォード<Stop>\n",
        "```\n",
        "\n",
        "**電話3：**\n",
        "\n",
        "コール2と同じように、Wikipedia Lookup + \"Thought 3：\"の結果、コール2 +応答2 +を連結してコール3を作成します。\n",
        "\n",
        "```\n",
        "質問：最初に生まれたのは誰ですか、ロナルドレーガンまたはジェラルドフォード？\n",
        "考え1：ロナルド・レーガンを調べて、彼がいつ生まれたのかを見る必要があります。\n",
        "アクション1：ロナルドレーガン<Stop>\n",
        "観察1：ロナルド・ウィルソン・レーガン（1911年2月6日 -  2004年6月5日）は、1981年から1989年まで米国の第40代大統領を務めたアメリカの政治家および俳優でした。そして、最初の離婚した大統領。レーガンはイリノイ州タンピコで生まれ、イリノイ州ディクソンで育ちました。彼はユーレカ大学で教育を受け、そこで経済学と社会学を学びました。卒業後、レーガンはカリフォルニアに移り、そこでラジオスポーツアナウンサーになりました。彼は後に演技に引っ越し、50以上の映画に出演しました。レーガンは、1947年から1952年までスクリーン俳優ギルドの社長を務めました。\n",
        "考え2：ロナルド・レーガンは1911年に生まれました。ジェラルド・フォードを調べて、彼がいつ生まれたのかを見る必要があります。\n",
        "アクション2：ジェラルドフォード<Stop>\n",
        "観察2：ジェラルド・ルドルフ・フォード・ジュニア（jerr-əld;生まれたレスリー・リンチ・キング・ジュニア、1913年7月14日 -  2006年12月26日）は、1974年から1977年に米国第38代大統領を務めたアメリカの政治家でした。以前は、1965年から1973年まで米国下院で共和党の指導者を務め、スピロアグニューの辞任の後、リチャードニクソン大統領によって40番目の副大統領に任命されました。フォードは、ニクソンが1974年に辞任したときに大統領職に成功しましたが、1976年に完全な任期に選挙で敗北しました。フォードは、大統領または副大統領の選挙を勝ち取らずに米国大統領になった唯一の人物です。\n",
        "フォードはネブラスカ州オマハで生まれ、ミシガン州グランドラピッズで育ちました。彼はミシガン大学に通い、そこで学校のサッカーチームでプレーしてから最終的にイェールロースクールに通いました。その後、彼は1942年から1946年まで米国海軍保護区に勤務しました。フォードは1949年にミシガン州5の米国代表として政治的キャリアを始めました。\n",
        "考え3：\n",
        "```\n",
        "\n",
        "最後に、LLMは答えを返します。\n",
        "\n",
        "**応答3：**\n",
        "```\n",
        "ジェラルドフォードは1913年に生まれました。1911年は1913年以前です。回答[ロナルドレーガン]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX060zm2p4A5"
      },
      "source": [
        "##反応チェーンを手動で実行します\n",
        "\n",
        "このセクションでは、Reactチェーンを段階的に実行します。\n",
        "\n",
        "次のコードセルには、いくつかのことが必要です。\n",
        "1. LLMが反応する方法を理解するための指示（コンテキスト）。\n",
        "2.少なくとも1つの模範。\n",
        "3. LLMのアクションを実行するツール。\n",
        "4. LLM呼び出しを行うパームAPIモデルオブジェクト。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk1oTh8HuXoB"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"思考、行動、観察で質問に答える。\n",
        "\n",
        "次に取るべき行動を考える。そして行動を起こす。\n",
        "すべてのアクションはウィキペディアのルックアップです。\n",
        "ウィキペディアのアクションはベストマッチの記事の先頭を返します。\n",
        "ウィキペディアのルックアップをするときは、<STOP>でルックアップを終了します。\n",
        "ウィキペディアアクションの後、観察があります。\n",
        "オブザベーションはウィキペディアのルックアップアクションから学んだことに基づいています。\n",
        "観察が終わったら、再び思考でループを始めます。\n",
        "\n",
        "必要に応じて、思考、行動、観察を繰り返します。\n",
        "質問の答えがわかるまで、必要に応じて繰り返す。\n",
        "答えがわかったと思ったら、その答えを次のような形式で返します：\n",
        "「Answer[答えはここに角括弧で囲まれています]」という形式で、思考の一部として答えを返します。\n",
        "答え」は必ず大文字にしてください。\n",
        "\n",
        "質問に答えるためには、観察結果の情報のみを使用してください。\"\"\"\n",
        "\n",
        "exemplar = \"\"\"Example:\n",
        "質問 ロナルド・レーガンとジェラルド・フォード、どちらが先に生まれた？\n",
        "思考1：ロナルド・レーガンがいつ生まれたのか調べてみよう。\n",
        "行動1：ロナルド・レーガン<STOP\n",
        "観察1：ロナルド・ウィルソン・レーガン（Ronald Wilson Reagan、1911年2月6日 - 2004年6月5日）はアメリカの政治家、俳優で、1981年から1989年まで第40代大統領を務めた。保守派で、西海岸出身者初の大統領であり、初の離婚歴のある大統領でもある。イリノイ州タンピコで生まれ、同州ディクソンで育つ。ユーリカ・カレッジで経済学と社会学を学んだ。卒業後はカリフォルニアに移り、ラジオのスポーツアナウンサーとなる。その後、俳優業に転身し、50本以上の映画に出演。レーガンは1947年から1952年まで映画俳優組合の会長を務めた。\n",
        "思考2：ロナルド・レーガンは1911年生まれ。ジェラルド・フォードがいつ生まれたか調べてみよう。\n",
        "アクション2：ジェラルド・フォード<STOP\n",
        "観察2：ジェラルド・ルドルフ・フォード・ジュニア（JERR-əld; 本名レスリー・リンチ・キング・ジュニア、1913年7月14日 - 2006年12月26日）は、1974年から1977年まで第38代アメリカ合衆国大統領を務めたアメリカの政治家。それ以前は1965年から1973年まで下院共和党党首を務め、スピロ・アグニューの辞任後、リチャード・ニクソン大統領によって第40代副大統領に任命された。1974年にニクソンが辞任したため、フォードが大統領職を引き継いだが、1976年の任期満了に伴う選挙では落選した。フォードは、大統領選挙にも副大統領選挙にも当選せずにアメリカ大統領になった唯一の人物である。\n",
        "ネブラスカ州オマハで生まれ、ミシガン州グランドラピッズで育った。ミシガン大学に入学し、同校のフットボール・チームでプレーした後、エール大学ロースクールに進学。その後、1942年から1946年まで米海軍予備役として勤務。フォードは1949年、ミシガン州下院議員として政治家としてのキャリアをスタートさせた。\n",
        "思考3：ジェラルド・フォードは1913年生まれ。1911年は1913年より前。答え【ロナルド・レーガン\"\"\"\n",
        "\n",
        "# Code for calling Wikipedia.\n",
        "import wikipedia\n",
        "def wiki_tool(query, return_chars = 1000):\n",
        "  try:\n",
        "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
        "  # If no exact match, take Wikipedia's suggestion.\n",
        "  except wikipedia.exceptions.PageError as e:\n",
        "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
        "  snippet = page[0:return_chars]\n",
        "  return snippet\n",
        "\n",
        "# Initialized PaLM API model.\n",
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "# These settings control how deterministic the LLM response is.\n",
        "parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(MODEL_NAME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5wOlmfAv53K"
      },
      "source": [
        "最初のLLMコールは、コンテキスト、模範、質問、および最初の思考のラベルです。\n",
        "\n",
        "各ラインの開始時のアクション/思考/観測ラベルは、チェーンを反応するために重要であり、LLM応答がインターリーブ反応ステップの「スクリプト」に固執する可能性を高めます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afRXzBhlwBw6"
      },
      "outputs": [],
      "source": [
        "question = \"イプセンの『人形の家』を上演した劇場の開場年は？\"\n",
        "llm_call_1 = f\"{context}\\n\\n{exemplar}\\n\\nQuestion: {question}\\nThought 1:\"\n",
        "print(llm_call_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFm05Ymwwjwc"
      },
      "outputs": [],
      "source": [
        "response_1 = call_llm(model, parameters, llm_call_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVPLsqolw16U"
      },
      "source": [
        "応答の1行目と2行目は良いです。このモデルは、合理的な思考と適切なアクションを生成しました。\n",
        "\n",
        "しかし、上記のツールを使用するのと同じように、LLMはごみのテキストを生成し続けます。LLMSは次のトークンを繰り返し予測し、ReactスタイルのLLMコールでは、次のトークンがRLMの残りのReactチェーンの予測であることを忘れないでください。\n",
        "\n",
        "ツール使用セクションと同じように、追加のテキストが破棄されます。最初の2つの応答線のみが保持されます：「Thought 1」と「アクション1」。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbgFW4Ehy6gh"
      },
      "outputs": [],
      "source": [
        "# Only take the first two lines of the response.\n",
        "# Splitlines returns a list with an item for each line.\n",
        "response_1 = response_1.splitlines()[0:2]\n",
        "# Turn response 1 into text from the list so we can concatenate to llm call 1.\n",
        "response_1 = (\"\\n\").join(response_1)\n",
        "print(response_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZxIR6nmuCLl"
      },
      "source": [
        "次に、LLMの「アクション1」応答でウィキペディアツールをクエリします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU7ExxFq0odj"
      },
      "outputs": [],
      "source": [
        "# Look up the LLM's action in Wikipedia.\n",
        "wiki_text_1 = wiki_tool(\"人形の家\")\n",
        "print(wiki_text_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAWEAjWw3MyU"
      },
      "source": [
        "次に、wikipediaツール出力を「観測1」として追加して、次のLLMコールを作成し、「観測2」の考えを追加します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn4C9X7H0vT4"
      },
      "outputs": [],
      "source": [
        "# Construct the next LLM call.\n",
        "llm_call_2 = f\"{llm_call_1} {response_1}\\nObservation 1: {wiki_text_1}\\nThought 2:\"\n",
        "print(llm_call_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNS0yZre1Obb"
      },
      "outputs": [],
      "source": [
        "response_2 = call_llm(model, parameters, llm_call_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dJZdWI91Xut"
      },
      "source": [
        "Reactチェーンの3回目のLLMコールの場合、2回目の呼び出しと同じ手順に従います。\n",
        "1. 応答の最初の2行を取ります。\n",
        "2. ウィキペディアのアクションを調べます。\n",
        "3. 応答、ウィキペディア出力、および以前のLLMコールからLLMコールを組み立てます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yioUsUmI1mdf"
      },
      "outputs": [],
      "source": [
        "# Only take the first two lines of the response.\n",
        "# Splitlines returns a list with an item for each line.\n",
        "response_2 = response_2.splitlines()[0:2]\n",
        "# Turn response 1 into text from the list so we can concatenate to llm call 1.\n",
        "response_2 = (\"\\n\").join(response_2)\n",
        "print(response_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plRMm1DS1mdf"
      },
      "outputs": [],
      "source": [
        "# Look up the LLM's action in Wikipedia.\n",
        "wiki_text_2 = wiki_tool(\"デンマーク、コペンハーゲンの王立劇場\")\n",
        "print(wiki_text_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEqsooGh1mdf"
      },
      "outputs": [],
      "source": [
        "# Construct the next LLM call.\n",
        "llm_call_3 = f\"{llm_call_2} {response_2}\\nObservation 2: {wiki_text_2}\\nThought 3:\"\n",
        "print(llm_call_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gWWK89A1mdf"
      },
      "outputs": [],
      "source": [
        "response_3 = call_llm(model, parameters, llm_call_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsVNvZ5F2HjV"
      },
      "source": [
        "そして、私たちには答えがあります！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfmXEYdg2aMb"
      },
      "source": [
        "## Running Reactチェーンのための完全なPythonコードスニペット\n",
        "\n",
        "アプリケーションでReactを使用するには、以前に手動で実行された手順を自動化する必要があります。\n",
        "\n",
        "以下の指導的コードスニペットは、Reactチェーンを実行します。LLMへのフォーマットされたReactコールを作成し、アクションを抽出し、アクションを実行し、LLMが回答で応答したかどうかを検出します。\n",
        "\n",
        "それは**非常に**あなたが以下のコードを歩いて、コメントを読んで、Reactチェーンがどのように自動化されているかをよりよく理解することをお勧めします。\n",
        "\n",
        "これは生産対応のコードではありません：\n",
        "1. スニペットは、この特定の最小限のReactの例にハードコードされています。Reactチェーンは異なるように見えます（これについては後で詳しく説明します）、Reactチェーンで構築された有用なアプリケーションにはカスタマイズされたツールが必要です。\n",
        "2. スニペットは脆い、特にベアボーンウィキペディアツール。\n",
        "3. LLMは、以前のアクションを再評価し、無限にループを反応させる可能性があります。このスニペットは、「MAX_STEPS」LLMコールの後に停止し、生産Reactコードはループをキャッチして回復しようとするはずです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVc3xRoWw1HM"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "\n",
        "def call_llm(model, parameters, llm_call, show_activity = True):\n",
        "  # Wraps an LLM call to Vertex, optionally displaying the call and response.\n",
        "  response = model.predict(llm_call, **parameters).text\n",
        "\n",
        "  if show_activity:\n",
        "    BOLD = \"\\033[1m\"\n",
        "    UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
        "    print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
        "    print(f\"{BOLD}The response:{UNFORMAT}\")\n",
        "    print(response)\n",
        "  return response  # Return to `_` if not needed.\n",
        "\n",
        "\n",
        "def wiki_tool(query, return_chars = 1000):\n",
        "  try:\n",
        "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
        "  # If no exact match, take Wikipedia's suggestion.\n",
        "  except wikipedia.exceptions.PageError as e:\n",
        "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
        "  snippet = page[0:return_chars]\n",
        "  return snippet\n",
        "\n",
        "\n",
        "def wiki_react_chain(model,\n",
        "                     parameters,\n",
        "                     context,\n",
        "                     exemplar,\n",
        "                     question,\n",
        "                     max_steps=7,\n",
        "                     show_activity=False):\n",
        "  # Call an LLM in a ReACT-style Thought -> Action -> Observation loop.\n",
        "  # Call the LLM max_steps times or to an answer in the pattern Answer[ans].\n",
        "\n",
        "  # Construct the first LLM call, teeing up the first thought.\n",
        "  next_llm_call = f\"{context}\\n\\n{exemplar}\\n\\nQuestion: {question}\\nThought 1:\"\n",
        "\n",
        "  step = 1\n",
        "  while step <= max_steps:\n",
        "\n",
        "    if show_activity:\n",
        "      print(f\"\\033[1mReAct chain step {step}:\\033[0m\\x1B[0m\")\n",
        "    llm_response = call_llm(model, parameters, next_llm_call, show_activity)\n",
        "\n",
        "    # Check for an answer. Look only at the first line of the response, since\n",
        "    #   the LLM will continue predicting beyond the next thought.\n",
        "    # This is brittle, it assumes no line breaks in the thought.\n",
        "    response_first_line = llm_response.splitlines()[0]\n",
        "    first_line_answer_split = response_first_line.split(\"Answer[\")\n",
        "    if len(first_line_answer_split) > 1:  # If there's a split on \"Answer[\".\n",
        "      # Return the answer, removing the \"]\" that comes after the answer.\n",
        "      return first_line_answer_split[1].split(\"]\")[0]\n",
        "\n",
        "    # If no answer, assume following response line is action.\n",
        "    response_second_line = llm_response.splitlines()[1]\n",
        "    \"\"\"\n",
        "      アクションの終わりを示すハードコードされた「<STOP>」文字に注意。\n",
        "      LLMの最初の行が思考で、2行目が行動だと仮定すれば、これは厳密には必要ない。\n",
        "      レスポンスの最初の行が思考で、2行目がアクションである。\n",
        "      それ以降の行はゴミである。しかし、LLMに明示的に次のようなシグナルを送るように指示する。\n",
        "      LLMに応答構造を明示的に指示することで、より構造的に一貫した\n",
        "      また、ReActが失敗する可能性のある1つの方法を発見しやすくなる。\n",
        "    \"\"\"\n",
        "    # Extract the wiki query from the action line of the response.\n",
        "    wiki_query = response_second_line.split(\":\")[1].split(\"<STOP>\")[0]\n",
        "    # Remove leading/trailing whitespace.\n",
        "    wiki_query = wiki_query.strip()\n",
        "    if show_activity:\n",
        "      print(f\"\\033[1mQuerying wikipedia for: {wiki_query}.\\033[0m\\x1B[0m\")\n",
        "    wiki_text = wiki_tool(wiki_query)\n",
        "\n",
        "    # Assemble the next LLM call.\n",
        "    # Only use the lines of the LLM response with the first thought and action.\n",
        "    usable_response = f\"{response_first_line}\\n{response_second_line}\"\n",
        "    # Assemble the wiki response into the observation line.\n",
        "    obs = f\"Observation {step}: {wiki_text}\"\n",
        "    step += 1\n",
        "    # Previous llm call + the first action and thought in the response +\n",
        "    # the result of the wikipedia lookup = llm call for next ReAct step.\n",
        "    # Note that next_llm_call was the last call we made, but we reassign it to\n",
        "    #   the same variable name so the loop works.\n",
        "    next_llm_call = f\"{next_llm_call} {usable_response}\\n{obs}\\nThought {step}:\"\n",
        "\n",
        "  # If max_steps exceeded and the loop exits.\n",
        "  # Would be better to raise an exception.\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoGC1n_K7r3t"
      },
      "source": [
        "上記の反応チェーンコードスニペットを使用した例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-6qK3n7-6uh"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "# Outside this notebook, set PROJECT_ID, LOCATION, and MODEL_NAME.\n",
        "# When running in the notebook, these are set in part 0.\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "# These settings control how deterministic the LLM response is.\n",
        "parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "context = \"\"\"思考、行動、観察で質問に答える。\n",
        "\n",
        "次に取るべき行動を考える。そして行動を起こす。\n",
        "すべてのアクションはwikipediaのルックアップです。\n",
        "ウィキペディアのアクションはベストマッチする記事の先頭を返します。\n",
        "ウィキペディアを検索するアクションを起こすときは、<STOP>で検索を終了します。\n",
        "wikipediaアクションの後、あなたは観察を行います。\n",
        "観察はwikipedia lookupアクションから学んだことに基づいています。\n",
        "観察が終わったら、再び思考でループを始めます。\n",
        "\n",
        "考え、行動し、観察することを必要に応じて繰り返す。\n",
        "質問の答えがわかるまで、必要に応じて繰り返す。\n",
        "答えがわかったと思ったら、その答えを次のような形式で返します：\n",
        "「答え[答えはここに角括弧で囲む]」という形式で答えを返す。\n",
        "を思考の一部として返します。答え」は必ず大文字で。\n",
        "\n",
        "観察に含まれる情報は、質問に答えるためにのみ使用してください。\"\"\"\n",
        "\n",
        "exemplar = \"\"\"Example:\n",
        "質問 ロナルド・レーガンとジェラルド・フォード、どちらが先に生まれた？\n",
        "思考1：ロナルド・レーガンがいつ生まれたのか調べてみよう。\n",
        "行動1：ロナルド・レーガン<STOP\n",
        "観察1：ロナルド・ウィルソン・レーガン（Ronald Wilson Reagan、1911年2月6日 - 2004年6月5日）はアメリカの政治家、俳優で、1981年から1989年まで第40代大統領を務めた。保守派で、西海岸出身者初の大統領であり、初の離婚歴のある大統領でもある。イリノイ州タンピコで生まれ、同州ディクソンで育つ。ユーリカ・カレッジで経済学と社会学を学んだ。卒業後はカリフォルニアに移り、ラジオのスポーツアナウンサーとなる。その後、俳優業に転身し、50本以上の映画に出演。レーガンは1947年から1952年まで映画俳優組合の会長を務めた。\n",
        "思考2：ロナルド・レーガンは1911年生まれ。ジェラルド・フォードはいつ生まれたのだろう。\n",
        "アクション2：ジェラルド・フォード<STOP\n",
        "観察2：ジェラルド・ルドルフ・フォード・ジュニア（JERR-əld; 本名レスリー・リンチ・キング・ジュニア、1913年7月14日 - 2006年12月26日）は、1974年から1977年まで第38代アメリカ合衆国大統領を務めたアメリカの政治家。それ以前は1965年から1973年まで下院共和党党首を務め、スピロ・アグニューの辞任後、リチャード・ニクソン大統領によって第40代副大統領に任命された。1974年にニクソンが辞任したため、フォードが大統領職を引き継いだが、1976年の任期満了に伴う選挙では落選した。フォードは、大統領選挙にも副大統領選挙にも当選せずにアメリカ大統領になった唯一の人物である。\n",
        "ネブラスカ州オマハで生まれ、ミシガン州グランドラピッズで育った。ミシガン大学に入学し、同校のフットボール・チームでプレーした後、エール大学ロースクールに進学。その後、1942年から1946年まで米海軍予備役として勤務。フォードは1949年、ミシガン州下院議員として政治家としてのキャリアをスタートさせた。\n",
        "思考3：ジェラルド・フォードは1913年生まれ。1911年は1913年より前。答え【ロナルド・レーガン\"\"\"\n",
        "\n",
        "question = \"フォードT型を設計した3人のエンジニアのうち、最も若いエンジニアが生まれた都市は？\"\n",
        "\n",
        "answer = wiki_react_chain(model,\n",
        "                          parameters,\n",
        "                          context,\n",
        "                          exemplar,\n",
        "                          question,\n",
        "                          show_activity = True)\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7L113wd8gdy"
      },
      "source": [
        "上記の「質問」を変更して実験します。あなたは素晴らしい結果を得ることができないかもしれません。これは、脆いウィキペディアツールによるものかもしれませんが、反応のエラーも表示される場合があります。\n",
        "\n",
        "コンテキストまたは模範を変更することにより、反応障害のパフォーマンスをどのように改善できるかを考えてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joeGGFHunvFW"
      },
      "source": [
        "##その他の反応のユースケース\n",
        "\n",
        "Reactパターンは、質問に答えるだけではありません。\n",
        "\n",
        "異なるコンテキストと模範で、上記のReactコードスニペットは事実チェックに適合しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qD06UTNDoVIm"
      },
      "outputs": [],
      "source": [
        "question = \"日本のGDPはBRICSのGDPよりも高い。\"\n",
        "\n",
        "context = \"\"\"あなたは、主張が真か偽かを検証している。\n",
        "思考、行動、観察を用いて主張を検証する。\n",
        "主張を支持する観察があるか、反証する観察があるかを判断する。\n",
        "\n",
        "その主張を検証するために、次に取るべき行動を考える。そして行動を起こす。\n",
        "すべてのアクションはウィキペディアのルックアップです。\n",
        "ウィキペディアのアクションはベストマッチの記事の先頭を返します。\n",
        "ウィキペディアの検索アクションを行うときは、<STOP>で検索を終了します。\n",
        "wikipediaアクションの後、あなたは観察を行います。\n",
        "観察は、ウィキペディアの検索アクションから学んだことに基づく。\n",
        "観察が終わったら、再び思考でループを始める。\n",
        "\n",
        "考え、行動し、観察することを必要に応じて繰り返す。\n",
        "主張について結論が出るまで、必要に応じて繰り返す。\n",
        "観察によって主張が否定された場合は、答えを「Answer[REFUTES]」として返します。\n",
        "主張を支持する観察があった場合は、「Answer[SUPPORTS]」として答えを返してください。\n",
        "\n",
        "観察に含まれる情報は、質問に答えるためにのみ使用してください。\"\"\"\n",
        "\n",
        "exemplar = \"\"\"Example:\n",
        "主張 ロナルド・レーガンはジェラルド・フォードより先に生まれた。\n",
        "思考1：ロナルド・レーガンがいつ生まれたのか調べてみよう。\n",
        "行動1：ロナルド・レーガン<STOP\n",
        "観察1：ロナルド・ウィルソン・レーガン（Ronald Wilson Reagan、1911年2月6日 - 2004年6月5日）はアメリカの政治家、俳優で、1981年から1989年まで第40代大統領を務めた。保守派で、西海岸出身者初の大統領であり、初の離婚歴のある大統領でもある。イリノイ州タンピコで生まれ、同州ディクソンで育つ。ユーリカ・カレッジで経済学と社会学を学んだ。卒業後はカリフォルニアに移り、ラジオのスポーツアナウンサーとなる。その後、俳優業に転身し、50本以上の映画に出演。レーガンは1947年から1952年まで映画俳優組合の会長を務めた。\n",
        "思考2：ロナルド・レーガンは1911年生まれ。ジェラルド・フォードはいつ生まれたのだろう。\n",
        "アクション 2: ジェラルド フォード<STOP>\n",
        "観察 2: ジェラルド ルドルフ フォード ジュニア (JERR-əld、本名: レスリー リンチ キング ジュニア、1913 年 7 月 14 日 - 2006 年 12 月 26 日) は、1974 年から 1977 年まで第 38 代アメリカ合衆国大統領を務めたアメリカの政治家です。1965 年から 1973 年まで、共和党の米国下院議員を務め、スピロ アグニューの辞任後、リチャード ニクソン大統領によって第 40 代副大統領に任命されました。フォードは、1974 年にニクソンが辞任したときに大統領職を引き継ぎましたが、1976 年に任期満了の選挙で敗れました。フォードは、大統領または副大統領の選挙に勝たずに米国大統領になった唯一の人物です。\n",
        "フォードはネブラスカ州オマハで生まれ、ミシガン州グランドラピッズで育った。ミシガン大学に入学し、同校のフットボールチームでプレーした後、最終的にイェール大学ロースクールに進学した。その後、1942年から1946年まで米国海軍予備役に勤務した。フォードは1949年にミシガン州5区の米国下院議員として政治活動を開始した。\n",
        "考え3: ジェラルド・フォードは1913年に生まれた。ロナルド・レーガンは1911年に生まれた。1911年は1913年より前である。ロナルド・レーガンはジェラルド・フォードより前に生まれた。回答[支持]\"\"\"\n",
        "\n",
        "answer = wiki_react_chain(model,\n",
        "                          parameters,\n",
        "                          context,\n",
        "                          exemplar,\n",
        "                          question,\n",
        "                          show_activity = True)\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6d_bgAVJgLa"
      },
      "source": [
        "ウィキペディアツールの制限は、このプロンプトのユーティリティを制限し、ニュートラルな「十分な情報」の回答に対するサポートの欠如も同様です。\n",
        "\n",
        "ただし、このユースケースにどのように簡単に適応したかを考えてください。反応パターンは、次のことでも良い結果を示しています。\n",
        "* テキストベースの仮想世界とのナビゲートと対話。\n",
        "* Webをサーフィンします。\n",
        "* 購入手順を使用して、eコマーストランザクションを作成します。\n",
        "* ジャーナル記事の文献検索の実施。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHexpQOYLb9E"
      },
      "source": [
        "<a name=\"react-tools\"> </a>\n",
        "##ツールの使用ベストプラクティス\n",
        "\n",
        "上記のプロンプトを実験した場合、おそらく失敗が発生した可能性があります。多くの場合、これはウィキペディアツールが限られているためです。\n",
        "\n",
        "いくつかのベストプラクティスに従うことで、この教育の例よりも堅牢で効果的なツールを構築するのに役立ちます。\n",
        "\n",
        "1. Do プロンプト内でツールとその使用方法を明確に記述しましょう。\n",
        "  * 理想的なツール使用を示すfew-shot事例を含めましょう。\n",
        "  * 例えば、「doc search」とだけ記述されたツールは、「自然言語クエリを使用して社内文書を検索します。レスポンスは、クエリとの関連性が高い順に並べられた文書名のリストです。」のように記述された同じツールよりもパフォーマンスが低くなります。\n",
        "2. Do ツールの範囲と複雑さを慎重に検討しましょう。\n",
        "  * ツールのAPIがLLMにとって十分にシンプルかどうかを検討しましょう。\n",
        "  * 多くの場合、1つの複雑なツールよりも複数のシンプルなツールの方が効果的です。開発者にとっては単一のAPIであっても、LLMツールとしては複数のツールに分けた方が良い場合があります。\n",
        "  * 例えば、ユースケースでデータベースにアクセスするためにSQLを実行する必要がある場合、LLMを使用してSQLクエリを最初から生成するのではなく、いくつかの個別のSQLテンプレートを個々のツールとして検討しましょう。\n",
        "3. Do ツール出力を構造的および文体的に一貫させましょう。\n",
        "  * ツール出力のバリエーションが少ないほど、LLMがその出力を効果的に使用する可能性が高くなります。\n",
        "4. Do ツール出力を短く、関連性の高いものにしましょう。\n",
        "  * 冗長なツール出力は、LLMの入力長制限に負担をかける可能性があります。\n",
        "  * ReAct論文のWikipediaエージェント実装は素晴らしい例です。Wikipedia記事内を検索し、記事全体ではなく、見つかった用語の周辺のテキストスニペットのみを返します。\n",
        "5. Do エラーを適切に処理しましょう。\n",
        "  * 例外をキャッチし、有用なエラーメッセージを提供しましょう。\n",
        "  * タイムアウトやレート制限などのツール側の問題を管理しましょう。\n",
        "  * few-shot事例でエラー処理を示しましょう。\n",
        "  * ツールが失敗し、次のLLM呼び出しで有用なエラーを提供した場合、LLMは自己修正する可能性があります。\n",
        "6. Do ツール使用プロンプトをチューニングしましょう。\n",
        "  * 様々なツール使用を含むパラメータ効率の良いチューニングセット（わずか10個の例でも）は、パフォーマンスを大幅に向上させることができます。\n",
        "7. Do LLMを呼び出してツールアクションを生成する際の出力長を制限しましょう。\n",
        "  * LLMはツールアクションを超えてテキストを生成し続けます。\n",
        "8. Don't セキュリティを忘れないでください。多くのツール使用パターンはセキュリティリスクを生み出します。\n",
        "  * LLMのツールを介してアクセス可能なものはすべて、敵対的な入力で実験するエンドユーザーに見られる可能性があると想定しましょう。\n",
        "  * LLMのツール呼び出しが悪意のあるものであることはないと想定しないでください。例えば、SQLインジェクションはLLMツールを介して可能です。\n",
        "\n",
        "このノートブックのツールは、これらのベストプラクティスの多くに従っていません。\n",
        "\n",
        "1. Wikipediaの記事は構造が予測できません。\n",
        "1. Wikipediaの記事は数千語になることもありますが、このツールは記事の関連部分に焦点を当てることをサポートしていません。\n",
        "1. プロンプトではWikipediaとは何か、またはその使用方法が説明されていません（ただし、LLMはトレーニングデータからWikipediaが何かを「知って」います）。\n",
        "1. エラーメッセージがなく、エラー処理は最小限です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZyRtBuT_eSQ"
      },
      "source": [
        "## 利点を反応します\n",
        "\n",
        "1. 幻覚が少ない。\n",
        "  * 信頼できる情報源とLLMの「メモリ」に依存する接地。\n",
        "1. 再訓練なしでLLMの知識を更新/拡張します。\n",
        "1. 既製のLLMSで動作し、追加のLLMトレーニングやチューニングは必要ありません。\n",
        "1. さまざまなユースケースをサポートします。\n",
        "1. 複数のツールで動作します。\n",
        "1. ツールを改善することでシステム全体のパフォーマンスを改善することは、プロンプトまたはLLM自体を改善するよりも簡単です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mgJ8MSQKIkt"
      },
      "source": [
        "## React Disdvantages\n",
        "\n",
        "1. 複数のLLM呼び出しにより、遅い（高レイテンシー）および高価です。\n",
        "1. 外部ツールは、維持およびセキュリティの懸念を維持し、より多くのシステムコンポーネントを意味します。\n",
        "1. 反応ループとその他の非回答シナリオが一般的です。\n",
        "  * Vs.幻覚がより一般的であると思考の連鎖。\n",
        "  * 専門的または最新の情報を必要としないユースケースの場合、思考の連鎖は反応する可能性があります。\n",
        "1. 反応推論（Think-> Act）は柔軟性が低く、純粋な一連の思考の連鎖のより柔軟な推論とパフォーマンスが低下する可能性があります。\n",
        "1. 外部情報が必要な場合、RAGよりも複雑な場合、検索がLLMによって制御されない場合にアプローチします。\n",
        "1. ツール統合を超えて、追加の機能が必要です。\n",
        "  * ループ救済。\n",
        "  * ツールエラーの管理。\n",
        "  * 思考のチェーンフォールバック。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6gBOX-yj6Ar"
      },
      "source": [
        "##ベストプラクティスを反応します\n",
        "\n",
        "[ツール使用](https://github.com/GoogleCloudPlatform/specialized-training-content/blob/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/#react-tools)上記のベストプラクティスを超えて。\n",
        "\n",
        "1. Don't temperature=0を盲目的に使用しないでください。\n",
        "  * タスクやツールの記述方法によってパフォーマンスが大きく変わる可能性があります。\n",
        "  * \"Thought\"、\"Action\"、\"Observation\"以外のラベルとステップのスキップを含む事例を試してみましょう。\n",
        "  * 様々な思考/推論と行動スタイルの事例を試してみましょう。例えば：\n",
        "    * 次のアクションを特定する思考が最適なタスクもあれば、最初の思考で完全な計画を立てるのが最適なタスクもあります。\n",
        "    * 無関係な観察やツールエラーの後に計画を調整したり、前の思考を再考したりする思考/行動を示しましょう。\n",
        "  * 直前の観察の最も重要な部分を言い換える思考を試してみましょう。\n",
        "1. Do ReActチェーンがループに陥るのをキャッチしましょう。\n",
        "  * ループのキャッチを示す事例で実験しましょう。\n",
        "  * 繰り返される行動をキャッチし、繰り返される行動を指摘する観察をLLMに返すことを検討しましょう。LLMは回復できるかもしれません。\n",
        "  * temperature > 0でループしているチェーンを再実行してみましょう。\n",
        "  * ReActが研究ベンチマークデータセットで最先端である場合、それは多くの場合、連鎖思考の自己整合性フォールバックを伴います。\n",
        "1. Do ファインチューニングを活用しましょう。\n",
        "  * ReActチェーン全体にわたるチューニング事例を含め、最初または最後のLLM呼び出しの事例だけでなく、全体を含めましょう。\n",
        "  * エラー/失敗処理をチューニングデータに含めましょう。\n",
        "  * 最終的な答えが正しい場合でも、不正確なReAct推論を含むチューニング事例を使用しないでください。\n",
        "1. Don't より単純な代替案を評価せずにReActを実装しないでください。\n",
        "  * 管理された拡張機能/プラグインを検討しましょう。\n",
        "    * 拡張機能サービスは、セキュリティ、可観測性、監視、評価などを提供し、実装作業を削減する可能性があります。\n",
        "    * 技術的な評価なしに、管理された拡張機能/プラグインサービスがニーズを満たすと想定しないでください。\n",
        "  * 外部知識をLLM呼び出しに統合するより簡単な方法を検討しましょう。（例：上記のRAGパターン1）。\n",
        "1. Do 大規模なReActのデバッグにLLMを使用しましょう。\n",
        "  * LLMに、タイプ別に失敗を分類する（例：推論ミス、ツール検索失敗、ループに陥った）および/またはReActチェーンの各ステップを正しいか間違っているかを識別するように促します。\n",
        "1. Do テスト、パフォーマンス測定（ドリフトを含む）、システム監視、CI/CDなどにツール機能を含めましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TeXWM0yxb8J"
      },
      "source": [
        "# パート4：LangchainとReact\n",
        "<img src = \"https://raw.githubusercontent.com/GoogleCloudPlatform/specialized-training-content/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/images/5-chained.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IAVv4HGtafY"
      },
      "source": [
        "Langchain は、LLM をすぐに始めるための優れたライブラリです。多くの[ツールの統合](https://python.langchain.com/docs/integrations/tools/)や組み込みの [ReAct エージェント](https://python.langchain.com/docs/modules/agents/agent_types/react)など、さまざまな便利な機能が備わっています。\n",
        "\n",
        "ただし、Langchainとの反応は、すべてのユースケースに最適ではない場合があります。Langchainを使用して使用する場合は、ニーズを満たしているかどうかを評価することが重要です。\n",
        "\n",
        "Langchainが現在ユースケースのニーズを満たしていないことがわかった場合でも、Langchainが1.0リリースに近づくと機能が追加されることに注意してください。\n",
        "\n",
        "Langchainには、[Langsmith](https://docs.smith.langchain.com/)という名前で利用可能な独自の評価と生産ツールもあります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrL7MQSR89ND"
      },
      "source": [
        "##基本的なLangchain React Agent\n",
        "\n",
        "LangchainでのReactの主な利点は、開始するのが非常に少ない作業であることです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XU67FY8-fMN"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.tools import WikipediaQueryRun\n",
        "from langchain.utilities import WikipediaAPIWrapper\n",
        "import wikipedia\n",
        "import vertexai\n",
        "\n",
        "# This is the langchain connection to Vertex AI.\n",
        "# Note this depends on vertexai.init (which was run in Part 0).\n",
        "llm = VertexAI(model_name=MODEL_NAME, temperature=0)\n",
        "\n",
        "# Initialize the Wikipedia tool.\n",
        "_ = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
        "# This next line invisibly maps to the previous line. The WikipediaQueryRun\n",
        "#   call is what matters here for Langchain to use its \"wikipedia\", not\n",
        "#   the variable that call is output to.\n",
        "tools = load_tools([\"wikipedia\"], llm=llm)\n",
        "\n",
        "# Create the ReAct agent.\n",
        "agent = initialize_agent(tools,\n",
        "                         llm,\n",
        "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n",
        "\n",
        "# You can change this question to see how the agent performs.\n",
        "# You may get a GuessedAtParserWarning from the wikipedia API, ignore it.\n",
        "agent.run(\"ボンゾのベッドタイム』でチンパンジーと共演したアメリカ大統領とは？\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gva8WBKgByU4"
      },
      "source": [
        "Langchain のもう 1 つの優れた機能は、組み込みの[ツール統合](https://python.langchain.com/docs/integrations/tools/)です。特に便利なツールの 1 つは数学用です。LLM は数学が苦手ですが、外部計算機があれば数学のパフォーマンスが向上します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00N7WCwxC9y9"
      },
      "outputs": [],
      "source": [
        "# The answer is 4489.\n",
        "# This may timeout or error, that's ok.\n",
        "agent.run(\"What's 67^2?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJu3sQ65CuvV"
      },
      "outputs": [],
      "source": [
        "# Make the llm-math tool available to the agent.\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
        "agent = initialize_agent(tools,\n",
        "                         llm,\n",
        "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n",
        "agent.run(\"What's 67^2?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttzg4RDhKXw7"
      },
      "source": [
        "##観測可能性の課題\n",
        "\n",
        "デフォルトでは、LangchainはReactチェーンの最終出力のみを返します。しかし、特にデバッグするときは、すべてのLLMコールを見ることが必要な場合があります。\n",
        "\n",
        "Langchainには、基礎となるLLMコールにある程度の観測性を提供する冗長モードが含まれています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwCwxRFHKetP"
      },
      "outputs": [],
      "source": [
        "# Note verbose is part of the agent declaration, not the run.\n",
        "agent = initialize_agent(tools,\n",
        "                         llm,\n",
        "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "                         verbose=True)\n",
        "\n",
        "agent.run(\"ボンゾのベッドタイム』でチンパンジーと共演したアメリカ大統領とは？\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1twwHcTLgqp"
      },
      "source": [
        "ここで、冗長モードは、最初の考えでは、LLMがその内部知識を使用したことを示しています。\n",
        "\n",
        "しかし、エージェントがどのように回答に到達したか、またはエージェントが失敗した理由を理解するのに冗長モードは常に十分ではありません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE2thulTLmOJ"
      },
      "outputs": [],
      "source": [
        "agent.run(\"2010年9月1日は何曜日でしたか？\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVMx7GCiL_C9"
      },
      "source": [
        "完全にデバッグするには、Langchain 内部の可視性を向上させる必要があります。 このカスタム可観測性コードのスニペット (この[ノートブック](https://github.com/GoogleCloudPlatform/specialized-training-content/blob/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/langchain_observability_snippet/langchain-observability-snippet.ipynb)からのもの) は、Langchain の[コールバック ハンドラー](https://python.langchain.com/docs/modules/callbacks/)を使用して、エージェントの実行時に何が起こるかを正確に示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjSuNufEMrwK"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Import dependencies.\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from langchain.schema import AgentAction, AgentFinish, Document, LLMResult\n",
        "import pdb\n",
        "from prettyprinter import cpprint\n",
        "from typing import Any, Dict, List, Optional, Sequence, Type, Union\n",
        "from uuid import UUID\n",
        "\n",
        "# Two helper classes.\n",
        "class Color():\n",
        "  \"\"\"印刷された色をより簡単に理解し、より速く操作する。\"\"\"\n",
        "  PURPLE = \"\\033[95m\"\n",
        "  CYAN = \"\\033[96m\"\n",
        "  DARKCYAN = \"\\033[36m\"\n",
        "  BLUE = \"\\033[94m\"\n",
        "  GREEN = \"\\033[92m\"\n",
        "  YELLOW = \"\\033[93m\"\n",
        "  RED = \"\\033[91m\"\n",
        "  BOLD = \"\\033[1m\"\n",
        "  UNDERLINE = \"\\033[4m\"\n",
        "  ITALICS = \"\\x1B[3m\"\n",
        "  END = \"\\033[0m\\x1B[0m\"\n",
        "\n",
        "\n",
        "class OutputFormatter:\n",
        "  \"\"\" コールバックから出力される出力の形式を制御するヘルパー クラス。\n",
        "\n",
        "prod で使用する場合は、出力が書き込まれる場所のハードコーディングを削除する方法で再実装することを検討してください。\n",
        "Python ログを使用してから、カスタム構成を渡すこともできます。\n",
        "  \"\"\"\n",
        "  # TODO: Add str casting here to reduce f\"{}\" in callback class to this class.\n",
        "  def heading(text: str) -> None:\n",
        "    print(f\"{Color.BOLD}{text}{Color.END}\")\n",
        "\n",
        "  def key_info(text: str) -> None:\n",
        "    print(f\"{Color.BOLD}{Color.DARKCYAN}{text}{Color.END}\")\n",
        "\n",
        "  def key_info_labeled(label: str,\n",
        "                       contents: str,\n",
        "                       contents_newlined: Optional[bool] = False\n",
        "                       ) -> None:\n",
        "    print(f\"{Color.BOLD}{Color.DARKCYAN}{label}: {Color.END}{Color.DARKCYAN}\",\n",
        "          end=\"\")\n",
        "    if contents_newlined:\n",
        "      contents = contents.splitlines()\n",
        "    cpprint(f\"{contents}\")\n",
        "    print(f\"{Color.END}\", end=\"\")\n",
        "\n",
        "  def debug_info(text: str) -> None:\n",
        "    print(f\"{Color.BLUE}{text}{Color.END}\")\n",
        "\n",
        "  def debug_info_labeled(label: str,\n",
        "                         contents: str,\n",
        "                         contents_newlined: Optional[bool] = False\n",
        "                         ) -> None:\n",
        "    print(f\"{Color.BOLD}{Color.BLUE}{label}: {Color.END}{Color.BLUE}\",\n",
        "          end=\"\")\n",
        "    if contents_newlined:\n",
        "      contents = contents.splitlines()\n",
        "    cpprint(f\"{contents}\")\n",
        "    print(f\"{Color.END}\", end=\"\")\n",
        "\n",
        "  def llm_call(text: str) -> None:\n",
        "    print(f\"{Color.ITALICS}{text}{Color.END}\")\n",
        "\n",
        "  def llm_output(text: str) -> None:\n",
        "    print(f\"{Color.UNDERLINE}{text}{Color.END}\")\n",
        "\n",
        "  def tool_call(text: str) -> None:\n",
        "    print(f\"{Color.ITALICS}{Color.PURPLE}{text}{Color.END}\")\n",
        "\n",
        "  def tool_output(text: str) -> None:\n",
        "    print(f\"{Color.UNDERLINE}{Color.PURPLE}{text}{Color.END}\")\n",
        "\n",
        "  def debug_error(text: str) -> None:\n",
        "    print(f\"{Color.BOLD}{Color.RED}{text}{Color.END}\")\n",
        "\n",
        "# Actual langchain callback handler, this produces status updates during a\n",
        "#   langchain execution.\n",
        "class AllChainDetails(BaseCallbackHandler):\n",
        "  \"\"\"チェーンの進行状況と状態の詳細を出力する。\n",
        "\n",
        "  チェーンの各実行ステップのコールバック時に利用可能な詳細を公開する。\n",
        "\n",
        "  このクラスのメソッドの引数は、コールバック・メソッドで利用可能な引数（の大部分？\n",
        "    に基づいています。\n",
        "    クラスのすべての実装がすべての引数を使用するわけではありません。\n",
        "\n",
        "  使い方\n",
        "    使用法：コールバック・ハンドラを受け付けるlangchainメソッドまたはクラスの引数として渡す。\n",
        "      ハンドラを受け付けるメソッドやクラスに引数として渡します。全てのlangchainクラスが、初期化時にコールバックハンドラが提供された場合、全てのコールバックを呼び出すわけではないことに注意してください。\n",
        "      初期化時にコールバックハンドラが提供された場合、全てのlangchainクラスが全てのコールバックを起動するわけではないことに注意してください。\n",
        "      推奨される使い方は、チェーンを実行するときにコールバックハンドラを提供することです。\n",
        "      チェーンを実行するときにコールバックハンドラを提供することです。\n",
        "\n",
        "  Example:\n",
        "    from langchain import LLMChain, PromptTemplate\n",
        "    from langchain.llms import VertexAI\n",
        "    import vertexai  # Comes from google-cloud-aiplatform package.\n",
        "    vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "    llm = VertexAI(temperature=0)  # Use any LLM.\n",
        "    prompt_template = \"What food pairs well with {food}?\"\n",
        "    handler = AllChainDetails()\n",
        "    llm_chain = LLMChain(\n",
        "      llm=llm,\n",
        "      prompt=PromptTemplate.from_template(prompt_template))\n",
        "    llm_chain(\"chocolate\", callbacks=[handler])\n",
        "\n",
        "  Args:\n",
        "    debug_mode: True の場合、各チェーン ステップの詳細を出力し、予期しない動作が検出されたときにブレークポイント (pdb を使用) をアクティブにします。\n",
        "    ブレークポイントはコールバック内にあるため、検査可能な langchain 状態の量は、langchain がコールバックに表示するものに制限されることに注意してください。\n",
        "  out: 出力を管理するためのクラス。このクラスに付随する OutputFormatter でのみテストされます。\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               debug_mode: Optional[bool] = False,\n",
        "               out: Type[OutputFormatter] = OutputFormatter,\n",
        "               ) -> None:\n",
        "    self.debug_mode = debug_mode\n",
        "    self.out = out\n",
        "\n",
        "  def on_llm_start(self,\n",
        "                   serialized: Dict[str, Any],\n",
        "                   prompts: List[str],\n",
        "                   **kwargs: Any) -> None:\n",
        "    \"\"\"Run when langchain calls an LLM.\"\"\"\n",
        "    self.out.heading(f\"\\n\\n> Sending text to the LLM.\")\n",
        "\n",
        "    if len(prompts) > 1:\n",
        "      self.out.debug_error(\"prompts has multiple items.\")\n",
        "      self.out.debug_error(\"Only outputting first item in prompts.\")\n",
        "      if self.debug_mode:\n",
        "        self.out.debug_info_labeled(\"Prompts\", f\"{prompts}\")\n",
        "        pdb.set_trace()\n",
        "\n",
        "    self.out.key_info(f\"Text sent to LLM:\")\n",
        "    self.out.llm_call(prompts[0])\n",
        "\n",
        "    if self.debug_mode:\n",
        "      self.out.debug_info_labeled(\"Arguments\", f\"{kwargs}\")\n",
        "      self.out.debug_info_labeled(\"serialized\", f\"{serialized}\")\n",
        "\n",
        "  def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
        "    \"\"\"Run after LLM response is received by langchain.\"\"\"\n",
        "    self.out.heading(f\"\\n\\n> Received response from LLM.\")\n",
        "\n",
        "    if len(response.generations) > 1:\n",
        "      self.out.debug_error(\"response object has multiple generations.\")\n",
        "      self.out.debug_error(\"Only outputting first generation in response.\")\n",
        "      if self.debug_mode:\n",
        "        self.out.debug_info_labeled(\"response\", f\"{response}\")\n",
        "        pdb.set_trace()\n",
        "\n",
        "    self.out.key_info(f\"Text received from LLM:\")\n",
        "    self.out.llm_output(response.generations[0][0].text)\n",
        "\n",
        "    if self.debug_mode:\n",
        "      self.out.debug_info_labeled(\"Arguments\", f\"{kwargs}\")\n",
        "      self.out.debug_info_labeled(\"response\", f\"{response}\")\n",
        "\n",
        "  def on_tool_start(self,\n",
        "                    serialized: Dict[str, Any],\n",
        "                    input_str: str,\n",
        "                    **kwargs: Any,) -> None:\n",
        "    \"\"\"Run when making a call to a tool.\"\"\"\n",
        "    self.out.heading(f\"\\n\\n> Using tool.\")\n",
        "    self.out.key_info_labeled(f\"Tool name\", f\"{serialized['name']}\")\n",
        "    self.out.key_info(f\"Query sent to tool:\")\n",
        "    self.out.tool_call(input_str)\n",
        "\n",
        "    if self.debug_mode:\n",
        "      self.out.debug_info_labeled(\"Arguments\", f\"{kwargs}\")\n",
        "      self.out.debug_info_labeled(\"serialized\", f\"{serialized}\")\n",
        "\n",
        "  def on_tool_end(\n",
        "      self,\n",
        "      output: str,\n",
        "      color: Optional[str] = None,\n",
        "      observation_prefix: Optional[str] = None,\n",
        "      llm_prefix: Optional[str] = None,\n",
        "      **kwargs: Any,) -> None:\n",
        "    \"\"\"Run on response from a tool.\"\"\"\n",
        "    self.out.heading(f\"\\n\\n> Received tool output.\")\n",
        "    self.out.key_info_labeled(f\"Tool name\", f\"{kwargs['name']}\")\n",
        "\n",
        "    if \"output\" not in locals():\n",
        "      self.out.debug_error(\"No tool output.\")\n",
        "      if self.debug_mode:\n",
        "        pdb.set_trace()\n",
        "    else:\n",
        "      self.out.key_info(\"Response from tool:\")\n",
        "      self.out.tool_output(f\"{output}\")\n",
        "\n",
        "    if self.debug_mode:\n",
        "      self.out.debug_info_labeled(\"Arguments\", f\"{kwargs}\")\n",
        "      self.out.debug_info_labeled(\"observation_prefix\",\n",
        "                                  f\"{observation_prefix}\")\n",
        "      self.out.debug_info_labeled(\"llm_prefix\",\n",
        "                                  f\"{llm_prefix}\")\n",
        "\n",
        "  def on_agent_action(self,\n",
        "                      action: AgentAction,\n",
        "                      color: Optional[str] = None,\n",
        "                      **kwargs: Any) -> Any:\n",
        "    \"\"\"Run when agent performs an action.\"\"\"\n",
        "    self.out.heading(f\"\\n\\n> Agent taking an action.\")\n",
        "\n",
        "    if self.debug_mode:\n",
        "      self.out.debug_info_labeled(\"Arguments\", f\"{kwargs}\")\n",
        "      self.out.debug_info_labeled(\"action\", f\"{action}\")\n",
        "\n",
        "  def on_agent_finish(self,\n",
        "                      finish: AgentFinish,\n",
        "                      color: Optional[str] = None,\n",
        "                      **kwargs: Any) -> None:\n",
        "    \"\"\"Run after agent completes.\"\"\"\n",
        "    self.out.heading(f\"\\n\\n> Agent has finished.\")\n",
        "\n",
        "    if self.debug_mode:\n",
        "      self.out.debug_info_labeled(\"Arguments\", f\"{kwargs}\")\n",
        "      self.out.debug_info_labeled(\"finish\",\n",
        "                                  f\"{finish}\")\n",
        "\n",
        "  def on_llm_error(self,\n",
        "                   error: Union[Exception, KeyboardInterrupt],\n",
        "                   **kwargs: Any) -> None:\n",
        "    self.out.debug_error(\"LLM Error\")\n",
        "    self.out.debug_info_labeled(\"Error object\", f\"{error}\")\n",
        "    if self.debug_mode:\n",
        "      pdb.set_trace()\n",
        "\n",
        "  def on_chain_error(self,\n",
        "                     error: Union[Exception, KeyboardInterrupt],\n",
        "                     **kwargs: Any) -> None:\n",
        "    self.out.debug_error(\"Chain Error\")\n",
        "    self.out.debug_info_labeled(\"Error object\", f\"{error}\")\n",
        "    if self.debug_mode:\n",
        "      pdb.set_trace()\n",
        "\n",
        "  def on_tool_error(self,\n",
        "                    error: Union[Exception, KeyboardInterrupt],\n",
        "                    **kwargs: Any) -> None:\n",
        "    self.out.debug_error(\"Chain Error\")\n",
        "    self.out.debug_info_labeled(\"Error object\", f\"{error}\")\n",
        "    if self.debug_mode:\n",
        "      pdb.set_trace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c_t51sDNV_a"
      },
      "source": [
        "カスタム観測可能性コードを含むエージェントを使用して、失敗したクエリを繰り返します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6MBeR4HNgf4"
      },
      "outputs": [],
      "source": [
        "handler = AllChainDetails()\n",
        "agent = initialize_agent(tools,\n",
        "                         llm,\n",
        "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n",
        "agent.run(\"2010年9月1日は何曜日でしたか？\",\n",
        "          callbacks=[handler])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt8bZW6qX-1e"
      },
      "source": [
        "LLMに送信された正確な呼び出しが表示され、LLMがツール（「ツールを使用する」）を選択したとき、LLMのツールへの入力（「ツールに送信されたクエリ：」）、および次のLLMアクティビティが表示されます。\n",
        "\n",
        "エラーの性質は明確になりました。数学ツールは、LLMに「numexpr」ライブラリで実行する式を作成するように指示しますが、LLMには誤って式に「DateTime」ライブラリが含まれています。\n",
        "\n",
        "さらに、LLMは、Langchainを呼び出して、ツールの説明や正確なReact実装（標準の思考 - >アクション - >観測とは異なる）を含む反応を実行するために使用します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MezAzXFAZpwz"
      },
      "source": [
        "### Langchainでの生産観測可能性\n",
        "\n",
        "安定した生産LLMシステムを実行するには、おそらく集中型の外部ロギング/監視プラットフォームで、強力な観察可能性とロギングが必要です。これがなければ、システムが正しく実行されていることを確認することはできず、デバッグできない場合があります。\n",
        "\n",
        "Langchainのコールバックの実装はここで役立ち、一部のMLプラットフォームベンダーはLangchainコールバックハンドラーを提供しています。\n",
        "\n",
        "ただし、一部のユースケースでは、カスタムラングチェーンコールバックハンドラーを作成する必要があります。また、システムに依存しているラングチェーンモジュールの他の部分に応じて、必要な情報をコールバックに表現するためにLangchain内部を変更する必要があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JskUeUagcR2V"
      },
      "source": [
        "##ツールのカスタマイズ摩擦\n",
        "\n",
        "Langchainエージェントに「DateTime」サポートを追加する方法は次のとおりです。\n",
        "\n",
        "1. 数学ツールがReactプロンプトで説明されている方法を変更するため、LLMは「DateTime」を使用しないことを知っています。\n",
        "1. DateTime Operations専用の新しいツールを作成し、LLMが利用できるようにします。\n",
        "1. Langchain Mathツールを変更して、「DateTime」サポートを追加します。\n",
        "1. langchain数学ツールを変更して、「numexpr」から例外をキャッチし、次のコールでLLMにエラーメッセージを提供して、LLMが別のアクションを実行できるようにします。\n",
        "\n",
        "これらには、Langchain内部の知識や、まだ文書化されていないLangchain機能を使用する必要があります。\n",
        "\n",
        "さらに、最良の反応性能のために、指示、模範、およびツールの説明を調整する必要があります。これは、「DateTime」ツールの問題を管理する以外に、[カスタムLangchainエージェント](https://python.langchain.com/docs/modules/agents/)を作成する必要があることを意味します。\n",
        "\n",
        "多くのユースケースでは、この摩擦は克服する価値があります。しかし、フレームワークを採用するという決定と同様に、ソフトウェア開発のベストプラクティスに従い、利用可能なフレームワークと構築の長所と短所をゼロから完全に調査します。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}