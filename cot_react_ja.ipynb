{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ5caKL2Ff2B"
      },
      "source": [
        "# 高度なプロンプト：Chain of Thought and ReAct (Reasoning + Acting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMkREhcA-Rtw"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pecYSnz2i2fk"
      },
      "source": [
        "このノートブックは、**Chain of Thought and ReAct (Reasoning + Acting)に基づいています** [Applied-Ai-Engieering-examples](https://github.com/googlecloudplatform/applied-ai-engineering-サンプル)GitHubリポジトリ。このリポジトリには、Google Cloud Applied AI Engineeringチームが開発したリファレンスガイド、青写真、コードサンプル、および実践的なラボが含まれています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H106E0clf7t"
      },
      "source": [
        "# パート0：はじめに\n",
        "\n",
        "このノートブックのターゲットオーディエンスは、タスク、ワークフロー、プロセス、機能などを繰り返し実行するためのエンジニアリングプロンプトです。安定性とパフォーマンスは、1回限りのニーズを求めるよりも重要です。\n",
        "\n",
        "このノートブックは、2つの強力なLLMプロンプト戦略をカバーしています。\n",
        "\n",
        "React（およびそのバリアント）は、幻覚を最小限に抑えながらLLMの推論を改善するための現在の最先端のプロンプト技術です。\n",
        "\n",
        "このノートブックの4つの部分は次のとおりです。\n",
        "\n",
        "1. 思考のチェーンプロンプト：LLM出力を改善するために推論の言語説明を使用します。\n",
        "\n",
        "1. アクション、検索、ツールの使用：LLMSが外部システムとどのように相互作用するか。\n",
        "\n",
        "1. 反応（推論 +演技）プロンプト：外部システムの相互作用と考えられたチェーンプロンプトの書面による推論の説明を組み合わせます。\n",
        "\n",
        "1. Langchain and React：Langchain React Agentを使用するときに何を期待するか。\n",
        "\n",
        "このノートブックはColabでテストされました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5FUT4VoDhsz"
      },
      "source": [
        "## このノートブックの使用方法\n",
        "\n",
        "* 最初にパート0を実行します。\n",
        "* パート1〜4それぞれパート0のコードに依存しますが、他の以前のパートのコードに依存しません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbz5Q4flkDgo"
      },
      "source": [
        "## 前提条件\n",
        "\n",
        "-  LLMS（大規模な言語モデル）の理解：\n",
        "-  LLMとは何か、そしてそれらがどのように機能するか。\n",
        "-LLMSは、次のトークンの繰り返し予測因子として。\n",
        "-LLM予測は、トレーニングデータとの類似性を最大化します。\n",
        "-  LLMプロンプトの経験：\n",
        "- 言語モデルを「プロンプト」することの意味。[推奨リソース](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/introduction-plompt-design)。\n",
        "-  [ゼロショット、ワンショット、少数のショット](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/introduction-plompt-design#include-xamplesの違い)プロンプト、およびパフォーマンスと堅牢性を最大化するために、少数のショットプロンプトが不可欠である理由を理解すること。\n",
        "-Google Cloud Vertex LLMSに基本的な知識。[推奨リソース](https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/api-quickstart)\n",
        "-  Langchainとは何か、それが解決することを目指している問題を知ってください。\n",
        "-  [推奨リソース](https://python.langchain.com/docs/get_started/introduction)および[Tutorials](https://github.com/googlecloudplatform/generative-ai/tree/main/language/orchestration/langchain)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmWgaCsdu6k1"
      },
      "source": [
        "## キー用語\n",
        "\n",
        "一貫性のために、このノートブックは特定の方法で次の用語を使用します。\n",
        "\n",
        "**プロンプト**：テンプレートに挿入される値に関係なく、コールのパフォーマンスと堅牢性を最大化する特定の手法を使用して作成されたテンプレートLLMコール。\n",
        "\n",
        "**LLMコール**：LLMにテキストを送信します。\n",
        "\n",
        "**LLM応答**：LLMによって予測されたテキスト、LLMコールを行うときにLLMから戻ってくるもの。\n",
        "\n",
        "**チェーン/チェーン**コンテキストに応じて：\n",
        "* 紹介されたチェーンのプロンプト、論理的に連続的な推論ステップ。\n",
        "* LLMシステムでは、LLMへの連続呼び出し。各コールは前のコールの応答に依存します。\n",
        "\n",
        "**exemplar**：1つまたは少数のプロンプトの「例」。\n",
        "* 従来のMLの意味での「例」との混乱、つまり「データの一部」（「トレーニングの例」など）を避けるために使用されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-glBTWPl1WD"
      },
      "source": [
        "## 参照\n",
        "\n",
        "* 小島、タケシ、他「大規模な言語モデルはゼロショットの推論者です。」神経情報処理システムの進歩35（2022）：22199-22213。[link](https://arxiv.org/abs/2205.11916)（アクセス2023 09 22）\n",
        "* Wang、Xuezhi、et al。「自己整合性は、言語モデルの一連の思考推論を改善します。」arxiv preprint arxiv：2203.11171（2022）。[link](https://arxiv.org/abs/2203.11171)（アクセス2023 09 03）。\n",
        "*ウェイ、ジェイソン他「考えられたチェーンプロンプトは、大規模な言語モデルで推論を引き出します。」神経情報処理システムの進歩35（2022）：24824-24837。[link](https://arxiv.org/abs/2201.11903)（アクセス2023 09 03）。\n",
        "* Yao、Shunyu、et al。「反応：言語モデルでの推論と行動の相乗効果。」Arxiv Preprint arxiv：2210.03629（2022）。[link](https://arxiv.org/abs/2210.03629)（アクセス2023 09 03）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC1b7po9xWM6"
      },
      "source": [
        "## セットアップ - このコードを最初に実行してください！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ_4h24m-B8u"
      },
      "outputs": [],
      "source": [
        "# Tested with these package versions.\n",
        "# Note this notebook uses matplotlib.pyplot. This is in the default Colab\n",
        "#   runtime, but you may need to install it in other notebook environments.\n",
        "!pip install --user langchain==0.0.316 google-cloud-aiplatform==1.35.0 prettyprinter==0.18.0 wikipedia==1.4.0 numexpr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCngWdptsN_Q"
      },
      "source": [
        "**さらに進む前にランタイムを再起動してください**\n",
        "\n",
        "ランタイムが削除されていない限り（再起動しても）、この以前のセルを再実行する必要はありません。\n",
        "\n",
        "ランタイムが再起動した場合、パート0の残りのセルを再実行します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-MjBceCQvcq"
      },
      "source": [
        "Colabを使用している場合は、次のセルでコードを実行します。[Vertex aiisifigine]を使用するには、Googleクラウド[プロジェクト](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects)にアクセスできるアカウントでポップアップをフォローし、認証します。llms](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)。\n",
        "\n",
        "Colab以外の場所でこのノートブックを実行している場合は、環境に適切なGoogleクラウドアクセスがあることを確認してください。それがあなたにとって新しい概念である場合は、[あなたのローカル環境のアプリケーションデフォルトの資格情報](https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev)を調べることを検討してください。より多くの認証オプションについて説明します[こちら](https://cloud.google.com/docs/authentication)。\n",
        "\n",
        "Google Cloudをまったく初めて使用する場合は、[開始](https://cloud.google.com/docs/get-started)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhnxRspMGGiz"
      },
      "outputs": [],
      "source": [
        "# Colab authentication.\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    print('Authenticated')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSeWZt3ZpxeY"
      },
      "source": [
        "Google CloudプロジェクトIDを次のセルに設定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLDEjCVzp7eh"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"YOUR_PROJECT_ID_HERE\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "# Code examples may misbehave if the model is changed.\n",
        "MODEL_NAME = \"text-bison@001\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fTAg64qFY2B"
      },
      "outputs": [],
      "source": [
        "# Set up Vertex PaLM API.\n",
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=PROJECT_ID,\n",
        "              location=LOCATION)\n",
        "parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_output_tokens\": 1024,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "\n",
        "model = TextGenerationModel.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSpDXdhBvhtu"
      },
      "source": [
        "この関数は、ノートブック全体で使用され、完全なLLMコールと応答を表示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esxRVsLAvvr6"
      },
      "outputs": [],
      "source": [
        "def call_llm(model, parameters, llm_call, show_activity = True):\n",
        "  response = model.predict(llm_call, **parameters).text\n",
        "\n",
        "  if show_activity:\n",
        "    BOLD = \"\\033[1m\"\n",
        "    UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
        "    print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
        "    print(f\"{BOLD}The response:{UNFORMAT}\")\n",
        "    print(response)\n",
        "\n",
        "  return response  # Return to `_` if not needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoiMSEJoY9gt"
      },
      "outputs": [],
      "source": [
        "# Wrap code cell output to improve notebook readability.\n",
        "# Source: https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results/61401455#61401455\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css(arg):\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US-jQm1MuGBa"
      },
      "source": [
        "# パート1：考え方のチェーンプロンプト\n",
        "\n",
        "LLMSにとって、チェーンはファッショナブルなアクセサリー以上のものです。\n",
        "\n",
        "<img src = \"https://raw.githubusercontent.com/GoogleCloudPlatform/specialized-training-content/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/images/1-chains.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82YfCjFJVX60"
      },
      "source": [
        "## 概要\n",
        "\n",
        "考え方のプロンプトでは、目的の出力に到達するための推論ステップを示す1つまたは少数のショットの模範を提供します。これは、標準の1または少数のショットプロンプトとは異なり、模範が入力と正しい出力のみを示します。\n",
        "\n",
        "思考の連鎖模範で提供する推論の内訳は、人が問題や仕事を通して考えている自然言語の内部モノローグに似ています。\n",
        "\n",
        "「内部モノローグ」が奇妙な概念である場合、問題を解決したり、タスクを達成したりするために自分の考えを言語化する方法を考えてください。たとえば、あなたは夕食を作っています：\n",
        "\n",
        "「OK OK私はセロリを切り刻みました。今、私は鶏肉を始める必要があります。オーブンはオンですか？オーブンの予熱を始めましょう。待って、どの温度？もう一度レシピをチェックする必要があります... `` `\n",
        "\n",
        "この「内部モノローグ」または「内部スピーチ」は、タスクの次に何が起こるべきかを特定することにより、これまで見たことのない新しい問題に問題解決パターンを適用することを容易にします。\n",
        "\n",
        "テキスト推論の「内部独白」を含む模範を使用してLLMを呼び出すことにより、LLMは同様のテキスト推論を含む応答を生成します。LLMに応答の一部として推論テキストが生成されると、応答が目的の出力で終了する可能性が高くなります。\n",
        "\n",
        "応答の推論ステップ\n",
        "また、LLMが最終出力にどのように到着したかの解釈可能性を提供します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydRfjsuBI5Ip"
      },
      "source": [
        "## 思考の基本の連鎖\n",
        "\n",
        "数学の単語の問題は、数学的および論理的に単純なものであるが、推論の複数のステップが必要なため、良いチェーンのデモンストレーションです。\n",
        "\n",
        "この例（思考の連鎖から[紙](https://arxiv.org/pdf/2201.11903.pdf)から）誤った答えに注意してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VJcAD7lYXE0"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n",
        "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
        "A: The answer is 11.\n",
        "Q: The cafeteria had 23 apples.\n",
        "If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
        "A:\"\"\"\n",
        "\n",
        "_ = call_llm(model, parameters, question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vmzEro2Z707"
      },
      "source": [
        "一連の思考を含めるために模範を書き直すことは、LLMに、質問を複数の単純な推論のステップに分解する方法を示しています。\n",
        "\n",
        "モデル応答は、同様の思考の連鎖に従い、正解の可能性を高めます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_QojLuvZzLV"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n",
        "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
        "A: Roger started with 5 balls. 2 cans of 3 tennis balls\n",
        "each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
        "Q: The cafeteria had 23 apples.\n",
        "If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
        "A:\"\"\"\n",
        "\n",
        "_ = call_llm(model, parameters, question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjwgFMOLaem9"
      },
      "source": [
        "思考の連鎖には、各推論ステップからのフォローするステップと中間出力/結論を説明するテキストの両方が含まれています。\n",
        "\n",
        "以下のコードの「質問」変数を変更して、さまざまな質問を試してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd4e62T7aWoG"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"Nomfundo writes legal briefs.\n",
        "Each brief has 3 sections, each section takes 4 hours.\n",
        "She wrote 3 briefs this week. How long did it take?\"\"\"\n",
        "\n",
        "one_shot_exemplar = \"\"\"Q: Roger has 5 tennis balls.\n",
        "He buys 2 more cans of tennis balls.\n",
        "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
        "A: Roger started with 5 balls. 2 cans of 3 tennis balls\n",
        "each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
        "Q: \"\"\"\n",
        "\n",
        "# Prepending the one shot exemplar before the question we want answered.\n",
        "llm_call = f\"{one_shot_exemplar}{question}\\nA:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XUp7beLcQsS"
      },
      "source": [
        "LLM応答は通常、模範の推論スタイルを模倣します。これは、模範の推論のチェーンがタスクに適している場合、最高のパフォーマンスを得ることができます。\n",
        "\n",
        "以下のセルを比較してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPQVYIPucnkF"
      },
      "outputs": [],
      "source": [
        "# Correct answer: 360, 375.\n",
        "question = \"\"\"A high efficiency factory produces 100 units per day.\n",
        "A medium efficiency factory produces 60 units per day.\n",
        "A low efficiency factory produces 30 units per day.\n",
        "Megacorp owns 5 factories. 3 are high efficiency, 2 are low efficiency.\n",
        "Tomorrow they reconfigure a low efficiency factory up to medium efficiency.\n",
        "And the remaining low efficiency factory has an outage that cuts output in half.\n",
        "How many units can they produce today? How many tomorrow?\"\"\"\n",
        "\n",
        "one_shot_exemplar = \"\"\"Q: Roger has 5 tennis balls.\n",
        "He buys 2 more cans of tennis balls.\n",
        "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
        "A: Roger started with 5 balls. 2 cans of 3 tennis balls\n",
        "each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
        "Q: \"\"\"\n",
        "\n",
        "llm_call = f\"{one_shot_exemplar}{question}\\nA:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ6Xo0gwpi35"
      },
      "source": [
        "出力の間違いに注意してください。LLM応答は、明日まだ実行されている3つの高効率工場を考慮していません。\n",
        "\n",
        "このタスクでは、さまざまな測定単位（テニスボール缶対工場出力）への接続と、数日間の数の持ち運びを含む推論ステップを使用して、一連の思考を使用することをお勧めします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThikEZV1cNYM"
      },
      "outputs": [],
      "source": [
        "better_one_shot_exemplar = \"\"\"Q: A large tennis ball can has 5 balls.\n",
        "A small tennis ball can has 3 balls.\n",
        "Roger has 3 large cans and 2 small cans today.\n",
        "Tomorrow he wins a bet and turns one small can into a large can.\n",
        "How many balls does he have today? How many tomorrow?\n",
        "A: 3 large cans is 3 * 5 = 15 tennis balls.\n",
        "2 small cans is 2 * 3 = 6 tennis balls.\n",
        "Today Roger has 15 + 6 = 21 tennis balls.\n",
        "Tomorrow's trade means losing one small tennis ball can and gaining a large can.\n",
        "Roger still has the cans he had yesterday.\n",
        "2 small cans from yesterday - 1 = 1 small can\n",
        "3 large cans from yesterday + 1 = 4 large cans\n",
        "4 large cans is 4 * 5 = 20 tennis balls.\n",
        "1 small can is 1 * 3 tennis balls.\n",
        "Tomorrow Roger has 20 + 3 = 23 tennis balls.\n",
        "Q: \"\"\"\n",
        "\n",
        "llm_call = f\"{better_one_shot_exemplar}{question}\\nA:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXNKuX_BttIk"
      },
      "source": [
        "## 思考ユースケースの連鎖\n",
        "\n",
        "数学の単語の問題はあまり役に立たないかもしれませんが、一連の思考は他のタイプの問題でうまく機能します。\n",
        "\n",
        "思考の連鎖からのいくつかの例[紙](https://arxiv.org/pdf/2201.11903.pdf)は情報を操作し、妥当性を評価し、指示を与え、テキストを変更/理解し、状態を追跡しています。\n",
        "\n",
        "<img src = \"https://raw.githubusercontent.com/GoogleCloudPlatform/specialized-training-content/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/images/2-cot.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX-kn_08m6VW"
      },
      "source": [
        "思考の連鎖によく反応する他のタイプのタスクは次のとおりです。\n",
        "* データの変換と濃縮。\n",
        "* データの解釈。\n",
        "* コード生成。\n",
        "* テキストの品質の評価（LLM応答の品質の評価を含む）。\n",
        "* 合成データの作成。\n",
        "\n",
        "一般的に、いくつかの簡単なステップを「話す」ことによって解決されるあらゆる種類の問題は、思考候補の良いチェーンです。\n",
        "\n",
        "より複雑な思考の使用の使用のために、模範全体であなたの考え方の推論スタイルがより一貫しているほど、LLMはその応答において同じスタイルの推論に従う可能性が高くなります。これは次の2つの例に注意してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRwGi1BUX8IE"
      },
      "source": [
        "#### 例：テーブルの理解"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFFmFWgIw_Lt"
      },
      "outputs": [],
      "source": [
        "# The correct answer is Post-War British Literature.\n",
        "question = \"\"\"\n",
        "| Book Name | Edition | ISBN | Publisher | Aug 1 Amazon Avg New Price | Aug 1 Amazon Avg Used Price | Aug 1 Abebooks Avg New Price | Aug 1 Abebooks Avg Used Price | Sep 1 Amazon Avg New Price | Sep 1 Amazon Avg Used Price | Sep 1 Abebooks Avg New Price | Sep 1 Abebooks Avg Used Price |\n",
        "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
        "| Physics for Computer Scientists | 10th | 978-1-118-56906-1 | Pearson Education | $149.99 | $79.99 | $142.94 | $66.94 | $129.99 | $59.99 | $139.94 | $56.94 |\n",
        "| Fundamentals of Calculus | 8th | 978-0-470-45831-0 | John Wiley & Sons | $139.99 | $99.99 | $137.94 | $87.94 | $129.99 | $79.99 | $129.94 | $76.94 |\n",
        "| Post-War British Literature | 2nd | 978-0-300-08897-2 | Oxford University Press | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
        "| Modern Religions: An Overview | 3rd | 978-0-19-992545-3 | Oxford University Press | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
        "| The Norton Introduction to Literature | 11th | 978-0-393-45078-1 | W. W. Norton & Company | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
        "| The Norton Anthology of American Literature | 9th | 978-0-393-93750-8 | W. W. Norton & Company | $179.99 | $139.99 | $174.94 | $127.94 | $169.99 | $124.99 | $174.94 | $121.94 |\n",
        "| The Norton Anthology of World Literature | 8th | 978-0-393-92855-6 | W. W. Norton & Company | $179.99 | $139.99 | $174.94 | $127.94 | $169.99 | $124.99 | $174.94 | $121.94 |\n",
        "| The Elements of Style | 5th | 978-0-205-11265-3 | Longman | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
        "\n",
        "What Oxford book dropped the most in used book price on Amazon between Aug and Sep?\n",
        "\"\"\"\n",
        "\n",
        "context = \"\"\"Answer questions about a table.\n",
        "All questions must be supported by facts in the table.\n",
        "All reasoning must be done step by step.\n",
        "Explain the reasoning.\n",
        "When looking at multiple rows, explain the reasoning for each row one by one.\n",
        "\"\"\"\n",
        "\n",
        "llm_call = f\"{context}\\n{question}\\nAnswer:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_bpOTJcXviZ"
      },
      "source": [
        "次に、いくつかの模範を追加します。\n",
        "\n",
        "模範は質問とは異なるソーステーブルを使用しているが、考え方のチェーンの推論はまだ機能していることに注意してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGUOqCKO_SIW"
      },
      "outputs": [],
      "source": [
        "few_shot_exemplar = \"\"\"\n",
        "Table:\n",
        "| Item Name | SKU | Vendor | Aug 1 Inventory | Sep 1 Inventory | Sale Count |\n",
        "|---|---|---|---|---|---|\n",
        "| iPhone 13 Pro Max | MGL83LL/A | Apple | 100 | 80 | 17 |\n",
        "| iPhone 13 Pro | MLL03LL/A | Apple | 50 | 40 | 9 |\n",
        "| iPhone 13 | MLKG3LL/A | Apple | 25 | 20 | 4 |\n",
        "| Samsung Galaxy S22 Ultra | SM-S908U | Samsung | 100 | 80 | 19 |\n",
        "| Samsung Galaxy S22 Plus | SM-S906U | Samsung | 50 | 40 | 10 |\n",
        "| Samsung Galaxy S22 | SM-S901U | Samsung | 25 | 20 | 5 |\n",
        "| Google Pixel 6 Pro | GA01314-US | Google | 100 | 80 | 20 |\n",
        "\n",
        "Question:\n",
        "What iPhone sold the most in August?\n",
        "Answer: I need to look at each item one by one and determine if it is an iPhone.\n",
        "Only iPhone items are considered.\n",
        "The iPhone items are the iPhone 13 Pro Max, the iPhone 13 Pro, and the iPhone 13.\n",
        "I need to look at how much each iPhone sold one by one, and then see which sold count is the highest.\n",
        "iPhone 13 Pro Max sale count is 17.\n",
        "iPhone 13 Pro sale count is 9.\n",
        "iPhone 13 sale count is 4.\n",
        "The biggest number of 17, 9, and 4 is 17.\n",
        "The answer is iPhone 13 Pro Max.\n",
        "\n",
        "Table:\n",
        "| Item Name | SKU | Vendor | Aug 1 Inventory | Sep 1 Inventory | Sale Count |\n",
        "|---|---|---|---|---|---|\n",
        "| iPhone 13 Pro Max | MGL83LL/A | Apple | 100 | 80 | 17 |\n",
        "| iPhone 13 Pro | MLL03LL/A | Apple | 50 | 40 | 9 |\n",
        "| iPhone 13 | MLKG3LL/A | Apple | 25 | 20 | 4 |\n",
        "| Samsung Galaxy S22 Ultra | SM-S908U | Samsung | 100 | 80 | 19 |\n",
        "| Samsung Galaxy S22 Plus | SM-S906U | Samsung | 50 | 40 | 10 |\n",
        "| Samsung Galaxy S22 | SM-S901U | Samsung | 25 | 20 | 5 |\n",
        "| Google Pixel 6 Pro | GA01314-US | Google | 100 | 80 | 20 |\n",
        "\n",
        "Question:\n",
        "What Samsung phone has the most units unaccounted for on Sep 1?\n",
        "Answer: I need to look at each item one by one and determine if it is a Samsung item.\n",
        "I have to look at the Item Name for Samsung items.\n",
        "Only Samsung items are considered.\n",
        "The Samsung items are the S22 Ultra, the S22 Plus, and the S22.\n",
        "One by one, I need to look at the Sep 1 and Aug 1 inventory difference for each Samsung item to see how many units should have been sold.\n",
        "Then I need to compare that number to the actual sale count value for that item.\n",
        "The phone with the biggest difference between the sale count field and the inventory differences is the most unaccounted for.\n",
        "Samsung Galaxy S22 Ultra had 100 in stock Aug 1 and 80 in stock Sep 1. 100 minus 80 is 20 (100 - 80 = 20). Sale count is 19. 20 minus 19 is 1 (20 - 19 = 1). 1 unit is unaccounted for.\n",
        "Samsung Galaxy S22 Plus had 50 in stock Aug 1 and 40 in stock Sep 1. 50 minus 40 is 10 (50 - 40 = 10). Sale count is 10. The sale count matches the inventory difference, no units are unaccounted for.\n",
        "Samsung Galaxy S22 had 25 in stock Aug 1 and 20 in stock Sep 1. 25 minus 20 is 5 (25 - 20 = 5). Sale count is 5. 20 minus 19 is 1. The sale count matches the inventory difference, no units are unaccounted for.\n",
        "Only the S22 Ultra had anything unaccounted for.\n",
        "The answer is Samsung Galaxy S22 Ultra.\n",
        "\n",
        "Table:\n",
        "| Item Name | SKU | Vendor | Aug 1 Inventory | Sep 1 Inventory | Sale Count |\n",
        "|---|---|---|---|---|---|\n",
        "| iPhone 13 Pro Max | MGL83LL/A | Apple | 100 | 80 | 17 |\n",
        "| iPhone 13 Pro | MLL03LL/A | Apple | 50 | 40 | 9 |\n",
        "| iPhone 13 | MLKG3LL/A | Apple | 25 | 20 | 4 |\n",
        "| Samsung Galaxy S22 Ultra | SM-S908U | Samsung | 100 | 80 | 19 |\n",
        "| Samsung Galaxy S22 Plus | SM-S906U | Samsung | 50 | 40 | 10 |\n",
        "| Samsung Galaxy S22 | SM-S901U | Samsung | 25 | 20 | 5 |\n",
        "| Google Pixel 6 Pro | GA01314-US | Google | 100 | 80 | 20 |\n",
        "\n",
        "Question:\n",
        "What vendor had the most total sales?\n",
        "Answer: I need to look at the vendors one by one.\n",
        "I have to deduce the vendors from the Item Name field.\n",
        "There are three unique vendors in the table: Apple, Samsung, and Google.\n",
        "For each vendor, I need to find the sale count for each item one by one, then add up the sales counts.\n",
        "The Apple items are the iPhone 13 Pro Max with 17 sales, the iPhone 13 Pro with 9 sales, and the iPhone 13 with 4 sales.\n",
        "17 + 9 + 4 = 30. 30 Apple phones were sold.\n",
        "The Samsung items are the Samsung Galaxy S22 Ultra with 19 sales, the Samsung Galaxy S22 Plus with 10 sales, and the Samsung Galaxy S22 with 5 sales.\n",
        "19 + 10 + 5 = 34. 34 Samsung phones were sold.\n",
        "The Google item is the Google Pixel 6 Pro with 20 sales. 20 Google phones were sold.\n",
        "30 Apple, 34 Samsung, 20 Google. 34 is the biggest number, it is for Samsung sales.\n",
        "The answer is Samsung.\n",
        "\n",
        "Table:\n",
        "| Item Name | SKU | Vendor | Aug 1 Inventory | Sep 1 Inventory | Sale Count |\n",
        "|---|---|---|---|---|---|\n",
        "| iPhone 13 Pro Max | MGL83LL/A | Apple | 100 | 80 | 17 |\n",
        "| iPhone 13 Pro | MLL03LL/A | Apple | 50 | 40 | 9 |\n",
        "| iPhone 13 | MLKG3LL/A | Apple | 25 | 20 | 4 |\n",
        "| Samsung Galaxy S22 Ultra | SM-S908U | Samsung | 100 | 80 | 19 |\n",
        "| Samsung Galaxy S22 Plus | SM-S906U | Samsung | 50 | 40 | 10 |\n",
        "| Samsung Galaxy S22 | SM-S901U | Samsung | 25 | 20 | 5 |\n",
        "| Google Pixel 6 Pro | GA01314-US | Google | 100 | 80 | 20 |\n",
        "\n",
        "Question:\n",
        "What item had the most sales?\n",
        "Answer: I need to look at each item one by one.\n",
        "The iPhone 13 Pro Max had 17 sales.\n",
        "The iPhone 13 Pro had 9 sales.\n",
        "The iPhone 13 had 4 sales.\n",
        "The Samsung Galaxy S22 Ultra had 19 sales.\n",
        "The Samsung Galaxy S22 Plus had 10 sales.\n",
        "The Samsung Galaxy S22 had 5 sales.\n",
        "The Google Pixel 6 Pro had 20 sales.\n",
        "The sales numbers are 17, 9, 3, 19, 10, 5, and 20.\n",
        "20 is the biggest sales number, that is for the Google Pixel 6 Pro.\n",
        "The answer is the Google Pixel 6 Pro.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Prepending the few shot exemplars before the question we want answered.\n",
        "llm_call = f\"{context}\\n{few_shot_exemplar}{question}\\nAnswer:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf0vyGCAZndK"
      },
      "source": [
        "さらに2つの質問（読みやすさのモデルの呼び出しを抑制します）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm_GnH8yZb9-"
      },
      "outputs": [],
      "source": [
        "# The correct answer is $6.15.\n",
        "question = \"\"\"\n",
        "Table:\n",
        "| Book Name | Edition | ISBN | Publisher | Aug 1 Amazon Avg New Price | Aug 1 Amazon Avg Used Price | Aug 1 Abebooks Avg New Price | Aug 1 Abebooks Avg Used Price | Sep 1 Amazon Avg New Price | Sep 1 Amazon Avg Used Price | Sep 1 Abebooks Avg New Price | Sep 1 Abebooks Avg Used Price |\n",
        "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
        "| Physics for Computer Scientists | 10th | 978-1-118-56906-1 | Pearson Education | $149.99 | $79.99 | $142.94 | $66.94 | $129.99 | $59.99 | $139.94 | $56.94 |\n",
        "| Fundamentals of Calculus | 8th | 978-0-470-45831-0 | John Wiley & Sons | $139.99 | $99.99 | $137.94 | $87.94 | $129.99 | $79.99 | $129.94 | $76.94 |\n",
        "| Post-War British Literature | 2nd | 978-0-300-08897-2 | Oxford University Press | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
        "| Modern Religions: An Overview | 3rd | 978-0-19-992545-3 | Oxford University Press | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
        "| The Norton Introduction to Literature | 11th | 978-0-393-45078-1 | W. W. Norton & Company | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
        "| The Norton Anthology of World Literature | 8th | 978-0-393-92855-6 | W. W. Norton & Company | $179.99 | $139.99 | $174.94 | $127.94 | $169.99 | $124.99 | $174.94 | $121.94 |\n",
        "| The Elements of Style | 5th | 978-0-205-11265-3 | Longman | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
        "\n",
        "Question:\n",
        "How much money would be saved if I purchased 3 new copies of the Elements of Style from Abe books instead of Amazon in August?\n",
        "\"\"\"\n",
        "\n",
        "llm_call = f\"{context}\\n{few_shot_exemplar}{question}\\nAnswer:\"\n",
        "print(call_llm(model, parameters, llm_call, show_activity=False))\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# The correct answer is Physics for Computer Scientists.\n",
        "question = \"\"\"\n",
        "Table:\n",
        "| Book Name | Edition | ISBN | Publisher | Aug 1 Amazon Avg New Price | Aug 1 Amazon Avg Used Price | Aug 1 Abebooks Avg New Price | Aug 1 Abebooks Avg Used Price | Sep 1 Amazon Avg New Price | Sep 1 Amazon Avg Used Price | Sep 1 Abebooks Avg New Price | Sep 1 Abebooks Avg Used Price |\n",
        "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
        "| Physics for Computer Scientists | 10th | 978-1-118-56906-1 | Pearson Education | $149.99 | $79.99 | $142.94 | $66.94 | $129.99 | $59.99 | $139.94 | $56.94 |\n",
        "| Fundamentals of Calculus | 8th | 978-0-470-45831-0 | John Wiley & Sons | $139.99 | $99.99 | $137.94 | $87.94 | $129.99 | $79.99 | $129.94 | $76.94 |\n",
        "| Post-War British Literature | 2nd | 978-0-300-08897-2 | Oxford University Press | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
        "| Modern Religions: An Overview | 3rd | 978-0-19-992545-3 | Oxford University Press | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
        "| The Norton Introduction to Literature | 11th | 978-0-393-45078-1 | W. W. Norton & Company | $129.99 | $89.99 | $122.94 | $74.94 | $119.99 | $74.99 | $124.94 | $71.94 |\n",
        "| The Norton Anthology of World Literature | 8th | 978-0-393-92855-6 | W. W. Norton & Company | $179.99 | $139.99 | $174.94 | $127.94 | $169.99 | $124.99 | $174.94 | $121.94 |\n",
        "| The Elements of Style | 5th | 978-0-205-11265-3 | Longman | $119.99 | $79.99 | $117.94 | $72.94 | $114.99 | $69.99 | $114.94 | $66.94 |\n",
        "\n",
        "Question: What book has the largest difference between new and used Aug Amazon prices?\n",
        "\"\"\"\n",
        "\n",
        "llm_call = f\"{context}\\n{few_shot_exemplar}{question}\\nAnswer:\"\n",
        "print(call_llm(model, parameters, llm_call, show_activity=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jk98xwBpSnl"
      },
      "source": [
        "データ理解のユースケースの場合、データスキーマが事前にデータスキーマを知っている場合は、そのスキーマと一致する必要があります。\n",
        "\n",
        "一般に、模範的なデータ構造がデータ構造の構造であるほど、LLMが正しく応答する可能性が高くなります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWB4WcfdaLNi"
      },
      "source": [
        "#### 例：タグ付けデータと構造化されたデータ出力\n",
        "\n",
        "LLMワークフローの2つの一般的なニーズは、説明からタグまたはカテゴリを生成し、構造化されたデータを出力することです。\n",
        "\n",
        "この例は両方を行います。タグ付けのパフォーマンスは、特定のタグが最適な理由を通じて、チェーンオブテアの模範とともに改善されます（タグが選択された理由の解釈可能性を提供します）。\n",
        "\n",
        "さらに、JSONのような一般的なデータ形式であっても、構造化されたデータ出力がどのように見えるかを示すと、パフォーマンスが向上します。\n",
        "\n",
        "[データソース](https://data.amerigeoss.org/dataset/gsa-json-adc1d)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xOLcvQdXWfd"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"Given a JSON entry of a data source, output a JSON with the following fields and explain the reasoning:\n",
        "pii: True/False, the dataset contains Personally Identifiable Information.\n",
        "age: How many years since the dataset was last modified.\n",
        "keywords: New keywords to index this dataset under, beyond the current set of keywords.\n",
        "The last text output should be the JSON.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "question = \"\"\"\n",
        "{\n",
        "    \"@type\" : \"dcat:Dataset\",\n",
        "    \"description\" : \"<p>The MDS 3.0 Frequency Report summarizes information for active residents currently in nursing homes. The source of these counts is the residents MDS assessment record. The MDS assessment information for each active nursing home resident is consolidated to create a profile of the most recent standard information for the resident.</p>\\n\",\n",
        "    \"title\" : \"MDS 3.0 Frequency Report\",\n",
        "    \"accessLevel\" : \"public\",\n",
        "    \"identifier\" : \"465\",\n",
        "    \"license\" : \"http://opendefinition.org/licenses/odc-odbl/\",\n",
        "    \"modified\" : \"2016-04-05\",\n",
        "    \"temporal\" : \"2012-01-01T00:00:00-05:00/2015-12-31T00:00:00-05:00\",\n",
        "    \"contactPoint\" : {\n",
        "      \"@type\" : \"vcard:Contact\",\n",
        "      \"fn\" : \"Health Data Initiative\",\n",
        "      \"hasEmail\" : \"mailto:HealthData@hhs.gov\"\n",
        "    },\n",
        "    \"bureauCode\" : [ \"009:38\" ],\n",
        "    \"keyword\" : [ \"Activities of Daily Living (ADL)\" ],\n",
        "    \"language\" : [ \"en\" ],\n",
        "    \"programCode\" : [ \"009:000\" ],\n",
        "    \"publisher\" : {\n",
        "      \"@type\" : \"org:Organization\",\n",
        "      \"name\" : \"Centers for Medicare & Medicaid Services\",\n",
        "      \"subOrganizationOf\" : {\n",
        "        \"@type\" : \"org:Organization\",\n",
        "        \"name\" : \"Department of Health & Human Services\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "llm_call = f\"{context}\\nJSON:{question}\\nAnswer:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0W-zY4uewRs"
      },
      "source": [
        "JSON形式は正しいですが、年齢は間違っており、キーワードは予測されていません。1つの模範を追加すると、正しい応答が得られます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUn2EeXQe6pu"
      },
      "outputs": [],
      "source": [
        "one_shot_exemplar = \"\"\"\n",
        "JSON:\n",
        "{\n",
        "\n",
        "    \"@type\" : \"dcat:Dataset\",\n",
        "    \"description\" : \"The primary purpose of this system of records is to properly pay medical insurance benefits to or on behalf of entitled beneficiaries.\",\n",
        "    \"title\" : \"Medicare Multi-Carrier Claims System\",\n",
        "    \"accessLevel\" : \"restricted public\",\n",
        "    \"dataQuality\" : true,\n",
        "    \"identifier\" : \"b6ffafab-1cfd-42dd-b8cb-7a554efaefa7\",\n",
        "    \"landingPage\" : \"http://www.cms.gov/Research-Statistics-Data-and-Systems/Computer-Data-and-Systems/Privacy/Systems-of-Records-Items/09-70-0501-MCS.html\",\n",
        "    \"license\" : \"http://www.usa.gov/publicdomain/label/1.0/\",\n",
        "    \"modified\" : \"2014-09-30\",\n",
        "    \"rights\" : \"Contains personally identifiable information and is subject to the Privacy Act of 1974, as amended at 5 United States Code (U.S.C.) 552a.  Requests should be directed to the appropriate System Manager, identified in the System of Records notice.\",\n",
        "    \"primaryITInvestmentUII\" : \"009-000004256, 009-000004254\",\n",
        "    \"systemOfRecords\" : \"09-70-0501\",\n",
        "\n",
        "    \"contactPoint\" : {\n",
        "      \"@type\" : \"vcard:Contact\",\n",
        "      \"fn\" : \"Health Data Initiative\",\n",
        "      \"hasEmail\" : \"mailto:Healthdata@hhs.gov\"\n",
        "    },\n",
        "    \"bureauCode\" : [ \"009:38\" ],\n",
        "    \"keyword\" : [ \"medicare\", \"part b\", \"claims\" ],\n",
        "    \"programCode\" : [ \"009:078\" ],\n",
        "    \"theme\" : [ \"Medicare\" ],\n",
        "    \"publisher\" : {\n",
        "      \"@type\" : \"org:Organization\",\n",
        "      \"name\" : \"Centers for Medicare & Medicaid Services\",\n",
        "      \"subOrganizationOf\" : {\n",
        "        \"@type\" : \"org:Organization\",\n",
        "        \"name\" : \"Department of Health & Human Services\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "Answer: The 'rights' tag says 'Contains personally identifiable information' so pii is True.\n",
        "The 'modified' tag is '2014-09-30'. The current year is 2023, 2023 minus 2014 is 9, so the age is 9.\n",
        "To determine keywords I will look at all the fields that describe the dataset.\n",
        "Then I will take the most salient and distinctive aspects of the fields and make those keywords.\n",
        "Looking at all the fields, the ones that describe the dataset are  \"description\" and \"title\".\n",
        "The \"title\" field is \"Medicare Multi-Carrier Claims System\".\n",
        "Good keywords from the \"title\" field are \"medicare\" and \"claims\".\n",
        "The \"description\" field is \"\"The primary purpose of this system of records is to properly pay medical insurance benefits to or on behalf of entitled beneficiaries.\"\n",
        "Good keywords from the \"description\" field are \"medical insurance benefits\".\n",
        "Good proposed keywords from both fields are \"medicare\", \"claims\", and \"medical insurance benefits\".\n",
        "Next inspect the \"keyword\" field to make sure the proposed keywords are not already included.\n",
        "The \"keyword\" field contains the keywords \"medicare\", \"part b\", and \"claims\".\n",
        "From our proposed keywords, \"medicare\" should not be output since it is already in the \"keyword\" field.\n",
        "That leaves \"claims\" and \"medical insurance benefits\" as proposed keywords.\n",
        "\n",
        "Output JSON:\n",
        "{\n",
        "  \"pii\" : true,\n",
        "  \"age\" : 9,\n",
        "  \"keywords\" : [\"claims\", \"medical insurance benefits\"]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Prepending the one shot exemplar before the question we want answered.\n",
        "llm_call = f\"{context}{one_shot_exemplar}\\nJSON:{question}\\nAnswer:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbtSBsrpjg56"
      },
      "source": [
        "出力は正しいですが、キーワードのオーバーラップの理由がより明確になる可能性があり、これにより、プロンプトがより堅牢になります。これを改善するために考えてから、1つのソリューションの次のセルをご覧ください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIGy06bNkdNf"
      },
      "outputs": [],
      "source": [
        "few_shot_exemplar = \"\"\"\n",
        "JSON:\n",
        "{\n",
        "\n",
        "    \"@type\" : \"dcat:Dataset\",\n",
        "    \"description\" : \"The primary purpose of this system of records is to properly pay medical insurance benefits to or on behalf of entitled beneficiaries.\",\n",
        "    \"title\" : \"Medicare Multi-Carrier Claims System\",\n",
        "    \"accessLevel\" : \"restricted public\",\n",
        "    \"dataQuality\" : true,\n",
        "    \"identifier\" : \"b6ffafab-1cfd-42dd-b8cb-7a554efaefa7\",\n",
        "    \"landingPage\" : \"http://www.cms.gov/Research-Statistics-Data-and-Systems/Computer-Data-and-Systems/Privacy/Systems-of-Records-Items/09-70-0501-MCS.html\",\n",
        "    \"license\" : \"http://www.usa.gov/publicdomain/label/1.0/\",\n",
        "    \"modified\" : \"2014-09-30\",\n",
        "    \"rights\" : \"Contains personally identifiable information and is subject to the Privacy Act of 1974, as amended at 5 United States Code (U.S.C.) 552a.  Requests should be directed to the appropriate System Manager, identified in the System of Records notice.\",\n",
        "    \"primaryITInvestmentUII\" : \"009-000004256, 009-000004254\",\n",
        "    \"systemOfRecords\" : \"09-70-0501\",\n",
        "\n",
        "    \"contactPoint\" : {\n",
        "      \"@type\" : \"vcard:Contact\",\n",
        "      \"fn\" : \"Health Data Initiative\",\n",
        "      \"hasEmail\" : \"mailto:Healthdata@hhs.gov\"\n",
        "    },\n",
        "    \"bureauCode\" : [ \"009:38\" ],\n",
        "    \"keyword\" : [ \"medicare\", \"part b\", \"claims\" ],\n",
        "    \"programCode\" : [ \"009:078\" ],\n",
        "    \"theme\" : [ \"Medicare\" ],\n",
        "    \"publisher\" : {\n",
        "      \"@type\" : \"org:Organization\",\n",
        "      \"name\" : \"Centers for Medicare & Medicaid Services\",\n",
        "      \"subOrganizationOf\" : {\n",
        "        \"@type\" : \"org:Organization\",\n",
        "        \"name\" : \"Department of Health & Human Services\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "Answer: The \"rights\" field says 'Contains personally identifiable information' so pii is true.\n",
        "The \"modified\" field is \"2014-09-30\". The current year is 2023, 2023 minus 2014 is 9, so the age is 9.\n",
        "To determine keywords I will look at all the fields that describe the dataset.\n",
        "Then I will take the most salient and distinctive aspects of the fields and make those keywords.\n",
        "Looking at all the fields, the ones that describe the dataset are \"description\" and \"title\".\n",
        "The \"title\" field is \"Medicare Multi-Carrier Claims System\".\n",
        "Good keywords from the \"title\" field are \"medicare\" and \"claims\".\n",
        "The \"description\" field is \"The primary purpose of this system of records is to properly pay medical insurance benefits to or on behalf of entitled beneficiaries.\"\n",
        "Good keywords from the \"description\" field are \"medical insurance benefits\".\n",
        "Good proposed keywords from both fields are \"medicare\", \"claims\", and \"medical insurance benefits\".\n",
        "Next inspect the \"keyword\" field to make sure the proposed keywords are not already included.\n",
        "The \"keyword\" field contains the keywords \"medicare\", \"part b\", and \"claims\".\n",
        "From our proposed keywords, \"medicare\" should not be output since it is already in the \"keyword\" field.\n",
        "That leaves \"claims\" and \"medical insurance benefits\" as acceptable new keywords.\n",
        "\n",
        "Output JSON:\n",
        "{\n",
        "  \"pii\" : true,\n",
        "  \"age\" : 9,\n",
        "  \"keywords\" : [\"claims\", \"medical insurance benefits\"]\n",
        "}\n",
        "\n",
        "\n",
        "JSON:\n",
        "{\n",
        "  \"@type\": \"dcat:Dataset\",\n",
        "  \"title\": \"Data.gov Top 10 Visiting Countries - Archival\",\n",
        "  \"description\": \"This dataset provides top 10 visiting countries by month in Data.gov up to July 2013.\",\n",
        "  \"modified\": \"2016-01-20\",\n",
        "  \"accessLevel\": \"public\",\n",
        "  \"identifier\": \"GSA-32491\",\n",
        "  \"dataQuality\": true,\n",
        "  \"describedBy\": \"http://www.data.gov/metric\",\n",
        "  \"describedByType\": \"text/csv\",\n",
        "  \"issued\": \"2013-05-13\",\n",
        "  \"license\": \"https://creativecommons.org/publicdomain/zero/1.0/\",\n",
        "  \"spatial\": \"United States\",\n",
        "  \"publisher\": {\n",
        "      \"@type\": \"org:Organization\",\n",
        "      \"name\": \"General Services Administration\"\n",
        "  },\n",
        "  \"accrualPeriodicity\": \"R/P1M\",\n",
        "  \"isPartOf\": \"GSA-2015-09-14-01\",\n",
        "  \"contactPoint\": {\n",
        "      \"@type\": \"vcard:Contact\",\n",
        "      \"fn\": \"Hyon Joo Kim\",\n",
        "      \"hasEmail\": \"mailto:hyon.kim@gsa.gov\"\n",
        "  },\n",
        "  \"distribution\": [{\n",
        "          \"@type\": \"dcat:Distribution\",\n",
        "          \"mediaType\": \"text/csv\",\n",
        "          \"format\": \"text/csv\",\n",
        "          \"title\": \"Data.gov_Top_10_Visiting_Countries.csv\",\n",
        "          \"downloadURL\": \"https://inventory.data.gov/dataset/b0d40da1-a505-476a-a49b-cfc50ea6d9da/resource/0a1a3fb8-a813-4470-b50c-51b7856203be/download/userssharedsdfdata.govtop10visitingcountries.csv\"\n",
        "      }\n",
        "  ],\n",
        "  \"keyword\": [\"Countries\", \"Interactive\"],\n",
        "  \"bureauCode\": [\"023:00\"],\n",
        "  \"programCode\": [\"023:019\"],\n",
        "  \"language\": [\"us-EN\"],\n",
        "  \"theme\": [\"Countries\", \"Top 10\"]\n",
        "  }\n",
        "\n",
        "Answer: The \"accessLevel\" field says \"public\" so pii is False.\n",
        "The \"modified\" field is \"2016-01-20\". The current year is 2023, 2023 minus 16 is 7, so the age is 8.\n",
        "To determine keywords I will look at all the fields that describe the dataset.\n",
        "Then I will take the most salient and distinctive aspects of the fields and make those keywords.\n",
        "Looking at all the fields, the ones that describe the dataset are  \"description\" and \"title\".\n",
        "The \"title\" field is \"Data.gov Top 10 Visiting Countries - Archival\".\n",
        "Good keywords from the \"title\" field are \"data.gov\", \"top 10\".\n",
        "The \"description\" field is \"This dataset provides top 10 visiting countries by month in Data.gov up to July 2013.\"\n",
        "Good keywords from the \"description\" field are \"top 10\" and \"visiting countries\".\n",
        "Good proposed keywords from both fields are \"data.gov\", \"top 10\", and \"visiting countries\".\n",
        "Next inspect the \"keyword\" field to make sure the proposed keywords are not already included.\n",
        "The \"keyword\" field contains the keywords \"Countries\" and \"Interactive\"\n",
        "None of the proposed keywords are in the \"keyword\" field.\n",
        "\"data.gov\", \"top 10\", and \"visiting countries\" are all acceptable new keywords.\n",
        "\n",
        "Output JSON:\n",
        "{\n",
        "  \"pii\" : false,\n",
        "  \"age\" : 9,\n",
        "  \"keywords\" : [\"data.gov\", \"top 10\", \"visiting countries\"]\n",
        "}\n",
        "\"\"\"\n",
        "llm_call = f\"{context}{few_shot_exemplar}\\nJSON:{question}\\nAnswer:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKPOkKfax7wB"
      },
      "source": [
        "## zero-shotの思考チェーン（「段階的に考えましょう」）\n",
        "\n",
        "ゼロショットチェーンの考え方は、LLMコールの最後に「トリガー文」を追加するときです。たとえば、「段階的に考えましょう」、「深呼吸をすることから始めます」、または「解決策：」。これは、迅速かつ簡単なパフォーマンスを向上させる方法であり、さまざまなタスクに柔軟に対応できます（一方、少数の思考の連鎖には、質問に似ている必要があります）。\n",
        "\n",
        "ただし、ゼロショットの思考チェーンは、ほぼすべての状況で数ショットのパフォーマンスを低下させます。さらに、ゼロショットの思考チェーンでは、LLMを2回呼び出す必要があります - 応答を生成するために、そして再び応答から答えを抽出する必要があります（応答構造を示す模範がないため）。最後に、ゼロショットのチェーンオブテアは、質問に答えるのではなく、質問を再定義する傾向があります。\n",
        "\n",
        "一般的に、少数のショットチェーンの模範を書くときのインスピレーションを除いて、堅牢なプロンプトをエンジニアリングする場合は、ゼロショットチェーンの考え方は推奨されません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXosOkcbuaTf"
      },
      "source": [
        "## 思考の連鎖の利点\n",
        "\n",
        "1. 最小限の努力のための簡単なLLM品質の向上。\n",
        "1. 問題を解決するための手順を口頭で「話す」ことによって解決できるタスクに適用できます。\n",
        "1. 解釈可能性。これにより、デバッグが支援され、エンドユーザーの解釈が必要なユースケースが可能になります。\n",
        "1. 既製のLLMSで動作し、追加のLLMトレーニングやチューニングは必要ありません。\n",
        "1. 異なるLLM間の堅牢性。考えられたチェーンプロンプトからの最終的な出力は、ドリフトを減らします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqPu2gaXexr3"
      },
      "source": [
        "##思考の短所\n",
        "\n",
        "1. 長いLLMコールと応答によるコストの増加。\n",
        "1. 推論時間が遅い。\n",
        "1. 幻覚はまだ可能です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYrjss2N2qnf"
      },
      "source": [
        "## 思考のチェーンベストプラクティス\n",
        "\n",
        "これらの推奨事項は、現在の理解を反映しており、LLMはすべて急速に変化しています。これのいくつかは、特定のコーナーケースとLLMアーキテクチャでは間違っている可能性があります。\n",
        "\n",
        "これらのベストプラクティスの例外を見つけた場合は、GitHubの問題を提出することを検討してください。\n",
        "\n",
        "### 重要なベストプラクティス\n",
        "\n",
        "思考の連鎖から良いパフォーマンスを得るには、これらのベストプラクティスに従う必要があります。\n",
        "\n",
        "1. 小さなLLMを**使用しない**でください。\n",
        "  * 理想的には、少なくとも15BパラメーターのLLMを使用します。\n",
        "  * 蒸留や改良されたLLMアーキテクチャのような技術が、最終的にこのアドバイスを変えることを期待したい。\n",
        "1. 思考の連鎖の前に答えを書くのではなく、思考の連鎖の後に答えを書く。\n",
        "1. [温度](https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/api-quickstart#try_text_text_prompts)を0に設定する。\n",
        "1. ワンショットやゼロショットだけでなく、数ショットの思考連鎖を使うこと。\n",
        "1. ステップ・バイ・ステップで推論を話すときに言うことをすべて盛り込んだ模範解答を書くこと。\n",
        "  * 思考の連鎖には自然言語による推論が必要だ。\n",
        "  * 自然言語による推論の代わりに数式を使わない。自然言語を補足するために方程式を加えるのは構わない。\n",
        "1. 思考の連鎖が幻覚を止めると決めつけてはいけない。\n",
        "  * 思考の連鎖はLLMの推論能力を向上させるが、LLMが事実をでっち上げることを止めるわけではない。\n",
        "\n",
        "### 追加のベストプラクティス\n",
        "\n",
        "思考の連鎖から最大限に活用するためのより多くのヒント。\n",
        "\n",
        "1. Few-Shotプロンプト作成のDo's and Don'ts\n",
        "Don't 過度にFew-Shot事例の順序にこだわる必要はありません。パフォーマンスに変化はないでしょう。\n",
        "\n",
        "  * 分類タスクは例外です。同じクラスの事例を連続して複数提示しないようにしましょう。\n",
        "\n",
        "1. Do Chain-of-Thoughtプロンプトが失敗する箇所を分析し、よくある失敗に対処するためのFew-Shot事例を作成しましょう。\n",
        "\n",
        "1. Don't 最初から6つ以上のFew-Shot事例を作成する必要はありません。タスクによっては、それ以上の事例が役立つ場合もありますが、多くの場合は不要です。\n",
        "\n",
        "1. Do 複数のプロンプトエンジニアにそれぞれ最適なプロンプトを作成してもらいましょう。\n",
        "\n",
        "  * 例えば、3つのタスクがあり、3人のプロンプトエンジニアがいる場合、各エンジニアが1つのタスクに集中するよりも、全員が3つのタスクのプロンプトを作成する方が良い結果が得られるでしょう。\n",
        "\n",
        "1. Don't タスクに必要な推論ステップが1つか2つの場合、Chain-of-Thoughtで結果が改善するとは期待しないでください。\n",
        "\n",
        "1. Don't 事例とタスクの推論ステップ数を厳密に一致させることに気を使いすぎる必要はありません。\n",
        "\n",
        "  * 推論のスタイルや構造を一致させる方が重要です。\n",
        "  * ステップ数を一致させることができればパフォーマンス上の利点がありますが、できなくてもChain-of-Thoughtは依然としてパフォーマンス向上に貢献します。\n",
        "\n",
        "1. Do LLMをチューニングする際にChain-of-Thoughtを追加しましょう。\n",
        "\n",
        "  * LLMに質問と回答からChain-of-Thoughtによる推論を生成させ、その推論をチューニングデータの回答に追加することができます。\n",
        "  * プロンプトとチューニングは二者択一ではありません。チューニングデータの入力が適切に設計されたプロンプトを含む場合、最適なチューニングモデルパフォーマンスが得られます。\n",
        "\n",
        "1. Do データ分布に一致する事例を含めましょう。\n",
        "\n",
        "  * 例えば、データがクラスA 80%、クラスB 20%で、5つのFew-Shot事例を作成する場合、4つの事例はクラスA、1つの事例はクラスBにするべきです。\n",
        "  * 分類タスクでは事例の順序が重要になることもありますが、クラス分布を一致させることで順序に対する頑健性が高まります。\n",
        "  * 連続して同じクラスの事例を複数提示しないように注意しましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY8sKdk9fN3Z"
      },
      "source": [
        "## 自己整合性\n",
        "\n",
        "自己整合性は、一連の思考プロンプトのパフォーマンスを改善するための手法です。同じLLMコールを複数回呼び出して、最も一般的な答えを出します。\n",
        "\n",
        "これは、温度= 0で思考の連鎖を使用するためのルールを「破る」ことを意味します。\n",
        "\n",
        "自己整合性の背後にある直観は次のとおりです。\n",
        "1.同一のLLM呼び出しに対する複数の応答は、応答のさまざまな推論パスを意味します。\n",
        "1.誤った推論パスは、異なる誤った回答につながります。\n",
        "1.正しい推論パスは同じ正解につながります。\n",
        "1.いくつかの正解と多くの誤った答えしか得られないかもしれませんが、正解は一意の誤った答えよりも一般的です。\n",
        "\n",
        "自己整合性を試してみましょう。まず、温度0でこの次のLLM呼び出しを実行して、誤った応答を生成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYKVZ8iHhf1d"
      },
      "outputs": [],
      "source": [
        "# The answer is 1300 + 100 (maintenance) + 75 (upgrade) = 1475.\n",
        "question = \"\"\"Factories have a baseline productivity of 100 units per day.\n",
        "Not all factories have the baseline productivity.\n",
        "When a factory is being upgraded, it has 25% of the baseline productivity.\n",
        "When a factory is undergoing maintenance, it has 50% of the baseline.\n",
        "When a factory is under labor action, it produces nothing.\n",
        "Megacorp has 19 factories in total.\n",
        "3 factories are being upgraded.\n",
        "2 factories are under maintenance.\n",
        "1 is under labor action.\n",
        "How many units does megacorp produce in a day?\"\"\"\n",
        "\n",
        "context = \"\"\"Answer questions showing the full math and reasoning.\n",
        "Follow the pattern in the example.\n",
        "\"\"\"\n",
        "\n",
        "one_shot_exemplar = \"\"\"Q: A regular tennis ball can holds 5 balls.\n",
        "A large tennis ball can holds 200% of a regular tennis ball can.\n",
        "A small tennis ball can holds 40% of a regular tennis ball can.\n",
        "A collectable tennis ball can holds no tennis balls.\n",
        "Roger has 10 tennis ball cans.\n",
        "3 cans are large cans.\n",
        "4 cans are small cans.\n",
        "1 can is collectable.\n",
        "How many tennis balls does Roger have?\n",
        "A: We need to find the number of regular tennis ball cans.\n",
        "Roger has 10 (total) - 3 (large) - 4 (small) - 1 (collectable) = 2 regular cans.\n",
        "A large tennis ball can holds 200% of 5 = 10 tennis balls.\n",
        "A small tennis ball can holds 40% of 5 = 2 tennis balls.\n",
        "Next count how many balls come from each can type.\n",
        "3 large cans is 3 * 10 = 30 tennis balls.\n",
        "4 small cans is 2 * 4 = 8 tennis balls.\n",
        "2 regular cans is 2 * 5 = 10 tennis balls\n",
        "1 collectable can is 0 tennis balls.\n",
        "To get the answer, add the number of balls from each can type.\n",
        "Roger has 30 (large) + 8 (small) + 10 (regular) + 0 (collectable) = 48 balls.\n",
        "The answer is 48.\n",
        "\n",
        "Q: \"\"\"\n",
        "\n",
        "llm_call = f\"{context}\\n{one_shot_exemplar}{question}\\nA:\"\n",
        "_ = call_llm(model, parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfyjnV8Clxia"
      },
      "source": [
        "次に、「温度」を.7に上げ、高い「TOP_P」と「TOP_K」値を使用して異なる応答を生成します。\n",
        "\n",
        "次のセルを数回実行し、答えがどのように変化するかに注意してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqr8DxNylcC1"
      },
      "outputs": [],
      "source": [
        "sc_parameters = {\n",
        "    \"temperature\": .7,\n",
        "    \"max_output_tokens\": 512,\n",
        "    \"top_p\": 1,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "\n",
        "_ = call_llm(model, sc_parameters, llm_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrbTQUGymnUr"
      },
      "source": [
        "上記のコードを再実行すると、さまざまな推論と回答が表示されます。\n",
        "\n",
        "次に、多くの応答をループして生成し、回答を抽出し、回答を最も一般的で最も一般的ではないように出力します。\n",
        "\n",
        "これには数分かかります。実行中は、さまざまな推論と回答に注意してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L1KRC6Hm5Ir"
      },
      "outputs": [],
      "source": [
        "from collections import Counter  # Easy counting of most common responses.\n",
        "sc_runs = 40\n",
        "responses = [None] * sc_runs\n",
        "answers = [None] * sc_runs\n",
        "\n",
        "for i in range(0, sc_runs):\n",
        "  print(f\"Response {i}...\")\n",
        "  responses[i] = call_llm(model,\n",
        "                          sc_parameters,\n",
        "                          llm_call,\n",
        "                          # Turn off printing LLM calls/responses.\n",
        "                          show_activity=False)\n",
        "  # If the response doesn't contain 'The answer is', the split fails.\n",
        "  # The split also fails if the answer contains a decimal or comma.\n",
        "  try:\n",
        "    answers[i] = responses[i].split(\"The answer is\")[1].split(\".\")[0].strip()\n",
        "  except Exception as e:\n",
        "    answers[i] = \"NA\"\n",
        "  print(responses[i])\n",
        "print(\"Answers and counts from most common to least common:\")\n",
        "print(Counter(answers).most_common())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxZ2S8hd9f33"
      },
      "source": [
        "上記のセルからの最後の出力は、異なる回答のカウントです。正解（1475）は、最も一般的な答えとして戻ってくるはずです。\n",
        "\n",
        "LLMの呼び出しが多いほど、最も一般的な答えは正しい答えです。\n",
        "\n",
        "また、結果をプロットして、回答の分布を視覚化することもできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfJiXg_qWB0A"
      },
      "outputs": [],
      "source": [
        "# Thanks to Hans-Christian Fuchs for this.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(Counter(answers).keys(), Counter(answers).values())\n",
        "ax.tick_params(axis='x', rotation=55)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyMEmx1J_osN"
      },
      "source": [
        "### 自己整合性の利点\n",
        "\n",
        "1. 低エフォルトのパフォーマンスブースト。\n",
        "1. 思考の模範を助けます。\n",
        "1. 異なるLLMにわたる迅速な堅牢性の増加。\n",
        "1. 回答分布に基づいて、擬似「信頼」推定値を提供します。\n",
        "1. 単一の正解なしで問題に対して「平均」回答を使用する機会。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsaFThs-_pyG"
      },
      "source": [
        "###自己整合性の欠点\n",
        "\n",
        "1. コストの増加。\n",
        "1. 推論時間の遅いおよび/またはスループットの削減。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov281oL--eRh"
      },
      "source": [
        "### 自己整合のベストプラクティス\n",
        "\n",
        "1. Do temperature=.7、top_k=40、top_p=1、そして10個のレスポンスを初期設定として使用しましょう。\n",
        "\n",
        "  * ユースケースによって異なる値が必要になる場合があるため、そこから実験を行いましょう。\n",
        "  * 本番環境で使用する最適な値を見つけるために、ハイパーパラメータ探索を実施しましょう。\n",
        "  * LLMパラメータよりもレスポンス数を探索する方がはるかに価値がある可能性が高いことに注意してください。また、LLMパラメータを試す場合でも、それらを大幅に減らすことは通常価値がありません。\n",
        "\n",
        "1. Do 初期のプロンプトエンジニアリングの試みが失敗した場合、早めに自己整合性を試しましょう。\n",
        "\n",
        "  * 自己整合性は、連鎖思考プロンプトのエンジニアリングを続けるよりも、パフォーマンスを向上させる可能性が高いです。\n",
        "\n",
        "1. Don't コストとレイテンシの影響を無視しないでください。\n",
        "\n",
        "1. Do 実行時間を短縮するためにLLM呼び出しを並列化しましょう。\n",
        "\n",
        "  * 自己整合性ユースケースに必要なLLMスループットとレイテンシの評価を後回しにしないでください。\n",
        "\n",
        "1. Do レスポンス分布を創造的な方法で使用しましょう。例えば：\n",
        "\n",
        "  * Xパーセント未満の回答しか一致しない場合、その質問にフラグを立てて人間のレビューに回しましょう。\n",
        "  * 複数の要約を生成し、テキスト類似性指標を使用して、どの生成された要約が最も「平均的」かを特定しましょう。\n",
        "\n",
        "1. Do 自己整合性をFew-Shot事例の作成やプロンプトのデバッグに活用しましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhA8gbnohLo7"
      },
      "source": [
        "# パート2：アクション、検索、ツールの使用\n",
        "\n",
        "LLMは、カラスのように、ツールを使用するのに熟達しています。\n",
        "\n",
        "<img src = \"https://raw.githubusercontent.com/GoogleCloudPlatform/specialized-training-content/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/images/3-crow.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD2iMx2wwBmN"
      },
      "source": [
        "##幻覚、接地、ツール/アクション/検索/ぼろきれ\n",
        "<a name=\"rag\"> </a>\n",
        "\n",
        "LLMは信頼できる事実源ではありません。LLM応答に正しい事実が含まれている場合、LLMのパラメーターが実際にエンコードするものの緊急効果です。単語間の確率的関係です。\n",
        "\n",
        "事実の正確性が重要な場合、これらの確率的関係に依存することは危険です。\n",
        "\n",
        "また、LLMSは、最新情報について迅速または安価に再訓練することもできません。また、再訓練が可能性がある場合でも、壊滅的な忘却は、トレーニングデータセットが増加するにつれて、古い情報の新しいエラーにつながる可能性があります。\n",
        "\n",
        "LLM応答が事実上正しくない場合、それはしばしば「幻覚」と呼ばれますが、より正確には[妄想](https://en.wikipedia.org/wiki/delusion)です。\n",
        "\n",
        "幻覚は非専門家によって見逃される可能性があります。LLM応答は、生成されたテキストが文法的に正確で、よく形成され、トーンに自信がある場合でも、事実上正しくありません。\n",
        "\n",
        "このLLM呼び出しが出力するものを参照してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD5WUUvDuHuD"
      },
      "outputs": [],
      "source": [
        "question = \"Who is Chancellor of Germany?\"\n",
        "_ = call_llm(model, parameters, question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM5MjaAjm77V"
      },
      "source": [
        "現在のモデルは正しく反応する可能性がありますが、2023年8月、メルケル首相が辞任してからほぼ2年後、これが応答でした。\n",
        "<img src = \"https://raw.githubusercontent.com/GoogleCloudPlatform/specialized-training-content/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/images/6-hallucinate.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAENyzS3o2YO"
      },
      "source": [
        "幻覚を管理する最良の方法は、LLMを正確で最新の外部データソースに接続することです。\n",
        "\n",
        "「接地」とは、外部情報を使用して幻覚を管理することです。「グラウンド」する1つの方法は、挿入された情報に基づいて応答を基にするための手順とともに、外部情報をLLMコールに挿入することです。\n",
        "\n",
        "「検索拡張生成」または「ラグ」は、LLMが外部知識を使用していると言う一般的な方法です。それは異なることを意味する可能性があります：\n",
        "1. 外部検索システムは、ユーザークエリを入力として取得し、情報を出力し、LLMコールのユーザークエリと組み合わせます。（たとえば、クエリの埋め込みをドキュメントの埋め込みと比較し、LLMコールに最も近いドキュメントを挿入します）。[コードサンプル](https://github.com/googlecloudplatform/generative-ai/blob/main/language/use-cases/document-qa/question_answering_documents_langchain_matching_engine.ipynb)。\n",
        "\n",
        "1. ユーザーのクエリに基づいて外部情報システムへの検索コールを策定する手順でLLMを呼び出し、ユーザーのクエリと取得情報を組み合わせてLLMに別の呼び出しを行います。\n",
        "\n",
        "3. 結合したオーダーメイドレトリバーとジェネレーターディープラーニングモデルを一緒に訓練/調整しました（元の[RAG Paper](https://arxiv.org/pdf/2005.11401.pdf)の焦点）。\n",
        "\n",
        "このノートブックは# 2に焦点を当てており、言語「ツール」/「ツール使用」を使用して、LLMを指示するように外部システムを使用するように説明し、あいまいな用語のぼろを避けます。パート3の後半では、「アクション」と「演技」を使用して、Reactが議論される方法と一致します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVj0W1lihch4"
      },
      "source": [
        "## LLMツールの使用方法\n",
        "\n",
        "LLMツールの使用の基本パターンは次のとおりです。\n",
        "1. 以下を説明する最初のLLMコールを作成します。\n",
        "* I：完了したいタスク。\n",
        "* II：外部システム。\n",
        "* III：外部システムへの呼び出しを策定する方法。\n",
        "2. LLMによって生成された応答を使用して外部システムを呼び出します。\n",
        "3.外部システムからの応答を含む2番目のLLMコールを作成し、LLMが外部システムからの応答を使用して元のタスクを完了するように指示します。\n",
        "\n",
        "私たちのLLMシステムが、上記の首相の例のような事実ベースの質問に答えることになっている場合：\n",
        "1. 最初のLLMコールは、LLMを指示して、知識ベースの検索クエリを生成します。\n",
        "2. LLMの応答は、知識ベースを照会するために使用され、クエリの結果がキャプチャされます。\n",
        "3. 2番目のLLMコールには、ナレッジベースクエリ、元の質問、およびLLMがナレッジベースクエリの結果を使用して質問に答えるための指示の結果が含まれます。\n",
        "\n",
        "LLMのツールは、データベース、Web検索、ドキュメント検索システムなど、多くのものになる可能性があります。LLMシステムの一部は、LLMを外部情報ソースと統合するコードです。\n",
        "\n",
        "このノートブックでは、ウィキペディアを外部情報ソースとして使用し、基本的なLLMシステムを構築して、事実ベースの質問に答えます。私たちのLLMシステムは次のとおりです。\n",
        "1. LLMを呼び出して、ウィキペディア検索クエリを生成します。\n",
        "1. ウィキペディアAPIを呼び出して、クエリの結果を取得します。\n",
        "1. Wikipedia API応答と元の質問を使用して、LLMをもう一度呼び出します。\n",
        "\n",
        "このノートブックの範囲を超えて、LLMSは、複数のツールを説明するInstandinosで呼び出すことができます。LLMはどちらもツールを選択し、ツールへの呼び出しを策定します。また、LLMツールは読み取り専用である必要はありません。ツールを使用して外部システムと対話できます（ただし、倫理と公平性への影響を考慮してください。Tは、 *幻覚は概要を尋ねたいと思っていますが、自動化された紙のグレーディングに影響を与えるように、誰かの人生に影響を与える可能性があるという決定を下すと、壊滅的です。誰かの人生）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91FoLDpruqF4"
      },
      "source": [
        "##サンプルツール\n",
        "\n",
        "以下の関数はクエリを取り、クエリのトップウィキペディア記事マッチを返し、記事の最初の `return_chars`文字を取得します。\n",
        "\n",
        "このツールは教育目的であり、やや制限されています。リストやサイドバーにアクセスすることはできず、提案をうまく処理せず、ウィキペディアの記事内での検索をサポートせず、常に結果を返すとは限りません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cLj2TiCt0cn"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "def wiki_tool(query, return_chars = 1000):\n",
        "  try:\n",
        "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
        "  # If no exact match, take Wikipedia's auto-suggestion.\n",
        "  except wikipedia.exceptions.PageError as e:\n",
        "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
        "  snippet = page[0:return_chars]\n",
        "  return snippet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRZ6v1z0uWAd"
      },
      "source": [
        "ツールを試してください："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4o-3Td9uZ-U"
      },
      "outputs": [],
      "source": [
        "wiki_tool(\"chancellor of germany\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7gGYXbxs8b7"
      },
      "source": [
        "##チェーンLLMはツールの使用を求めています\n",
        "\n",
        "基本的な2段階のツール使用LLMチェーンには、ここで段階的に分類されているいくつかのピースが含まれています。\n",
        "\n",
        "この例でモデルを（2023年10月の時点で）、あいまいなミュージシャンについての質問に電話すると、誤った答えが幻覚を起こします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHK1aJ_oXtJZ"
      },
      "outputs": [],
      "source": [
        "question = \"What musician released the album 'Somebody in the Snow'?\"\n",
        "_ = call_llm(model, parameters, question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nqc0vhU5H1X"
      },
      "source": [
        "### ステップ1：ツールを使用するためのLLMの指示を提供する\n",
        "\n",
        "LLMに、タスクとツールの使用方法の両方の指示を提供する必要があります。\n",
        "\n",
        "LLM呼び出しのこの「命令」部分は、「コンテキスト」または「条件」（「コンディショニング」、「コンディショニングプロンプト」）のバリエーションと呼ばれることがあります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhWpoRFGA21n"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"Answer questions using a lookup of Wikipedia.\n",
        "After each question, write a Wikipedia search followed by '<STOP>'.\n",
        "The Wikipedia search will be used to retrieve the most relevant content.\n",
        "A section of the Wikipedia article will then be sent to the next LLM call.\n",
        "Use the text of the Wikipedia article to answer the question.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqWY6f3EBDyO"
      },
      "source": [
        "### ステップ2：模範を提供します\n",
        "\n",
        "LLMには、ツールを使用してタスクを完了する方法を示す模範が必要です。\n",
        "\n",
        "この例には、1ショットの模範的なものしかありません。\n",
        "\n",
        "この模範のウィキペディアの記事のテキストは、2023年8月に「wiki_tool（ \"chancellor ofドイツ\"）を実行しています。\n",
        "\n",
        "注：将来の再試行の後、LLMは外部ツールなしでこの質問に正しく答えます。しかし、このワンショットの模範は、ウィキペディア検索のパターン、応答、および応答に基づいた回答を示すため、まだ機能します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Haoj8nWSA_fy"
      },
      "outputs": [],
      "source": [
        "exemplar = \"\"\"Question: Who is Chancellor of Germany?\n",
        "Wikipedia Search: chancellor of Germany<STOP>\n",
        "Wikipedia Article: The chancellor of Germany, officially the federal chancellor of the Federal Republic of Germany, is the head of the federal government of Germany, and the commander in chief of the German Armed Forces during wartime. The chancellor is the chief executive of the Federal Cabinet and heads the executive branch. The chancellor is elected by the Bundestag on the proposal of the federal president and without debate (Article 63 of the German Constitution).The current officeholder is Olaf Scholz of the SPD, who was elected in December 2021, succeeding Angela Merkel. He was elected after the SPD entered into a coalition agreement with Alliance 90/The Greens and the FDP.\\n\\n\\n== History of the office ==\\nThe office of Chancellor has a long history, stemming back to the Holy Roman Empire, when the office of German archchancellor was usually held by archbishops of Mainz. The title was, at times, used in several states of German-speaking Europe. The modern office of chancellor was established with the\n",
        "Answer: Olaf Scholz\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deMaQ9ddDQhc"
      },
      "source": [
        "### ステップ3：LLMチェーンで最初の呼び出しを行う\n",
        "\n",
        "私たちのコンテキストと模範を質問と組み合わせて、Wikipediaの検索クエリを回答として要求するLLMに電話をかけます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC4l5oHtD9OO"
      },
      "outputs": [],
      "source": [
        "step_one_call = f\"\"\"{context}\n",
        "\n",
        "{exemplar}\n",
        "\n",
        "Question: {question}\n",
        "Wikipedia Search:\"\"\"\n",
        "step_one_response = call_llm(model, parameters, step_one_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDUi5JB8GCL9"
      },
      "source": [
        "### ステップ4：LLMの応答を使用してツールをクエリします\n",
        "\n",
        "注LLM応答には、Wikipedia検索クエリ以上のものが含まれています。\n",
        "\n",
        "LLMSは、LLMコールのトークンと以前に予測されたトークンのトークンに基づいて、次のトークンを何度も繰り返し予測することで機能します。これは、LLMが過剰なテキストを生成することを意味します。ウィキペディア検索クエリの後に停止することはわかりません。\n",
        "\n",
        "ウィキペディアの検索クエリを超えたすべてのものはごみです。余分なテキストは「<stop> `signifierを使用して破棄されますが、これはラインブレークでも実行できます。\n",
        "\n",
        "生産システムでは、このようなLLMコールを行うときに応答サイズを制限することにより、コストを制御することが重要です。\n",
        "\n",
        "次の関数は、最初のチェーンステップからLLM応答を取得し、ウィキペディアクエリを返します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2cqh5R4HTHV"
      },
      "outputs": [],
      "source": [
        "def get_wiki_query (llm_response, stop_text = \"<STOP>\"):\n",
        "  # Assumes the query is in the first line.\n",
        "  first_line = llm_response.splitlines()[0]\n",
        "  query = first_line.split(stop_text)[0]\n",
        "  return query.strip() # Remove leading and trailing whitespace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sv6ox89JYPe"
      },
      "source": [
        "以前のLLMコールからの応答でこの関数を使用してクエリを抽出し、「wiki_tool」を使用してウィキペディアを検索します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d5CKJRyJW5C"
      },
      "outputs": [],
      "source": [
        "wiki_query = get_wiki_query(step_one_response)\n",
        "print(f\"Tool Query: {wiki_query}\")\n",
        "wiki_text = wiki_tool(wiki_query)\n",
        "print(f\"Wikipedia Snippet: {wiki_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAmH5sQddF9Q"
      },
      "source": [
        "###ステップ5：ツール応答を使用して、LLMチェーンで2回目の呼び出しを行う\n",
        "\n",
        "次に、ツールから出力を取得し、2番目のLLMコールを作成して質問に答えます。\n",
        "\n",
        "LLMツールの使用は、一般に、以前の呼び出しと応答の履歴を維持しています。チェーン内の2番目の呼び出しを作成するには：\n",
        "1. チェーン内の最初のLLMコールから始めます。\n",
        "1. 以前に生成されたウィキペディアクエリを追加します。\n",
        "1. ウィキペディアの検索結果を追加します。\n",
        "\n",
        "これが私たちの最初の呼び出しがどのように見えるかを思い出させます："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJKx_TKAdmRz"
      },
      "outputs": [],
      "source": [
        "print(step_one_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCCGiIZuChoA"
      },
      "source": [
        "この最初のLLMコールは、最初のLLM応答からのクエリと、ウィキペディアツールからの出力と、模範と一致する構造と組み合わされます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRsLkHfRd3hY"
      },
      "outputs": [],
      "source": [
        "step_two_call = f\"\"\"{step_one_call} {wiki_query}\n",
        "Wikipedia Article: {wiki_text}\n",
        "Answer: \"\"\"\n",
        "step_two_response = call_llm(model, parameters, step_two_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU-UI3mnKPLq"
      },
      "source": [
        "## すべてのステップをまとめる\n",
        "\n",
        "下のこのコードスニペットは、上記のすべての手順、従属パッケージ、および従属関数を2段階のツール使用LLMチェーンを管理する単一の関数に収集します。\n",
        "\n",
        "適切なパッケージをインストールして認証されたと仮定して、このコードを独自のプロジェクトにコピーして貼り付けることができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o__JbR9LKiNX"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "\n",
        "def call_llm(model, parameters, llm_call, show_activity = True):\n",
        "  # Wraps an LLM call to Vertex, optionally displaying the call and response.\n",
        "  response = model.predict(llm_call, **parameters).text\n",
        "\n",
        "  if show_activity:\n",
        "    BOLD = \"\\033[1m\"\n",
        "    UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
        "    print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
        "    print(f\"{BOLD}The response:{UNFORMAT}\")\n",
        "    print(response)\n",
        "\n",
        "  return response  # Return to `_` if not needed.\n",
        "\n",
        "\n",
        "def wiki_tool(query, return_chars = 1000):\n",
        "  try:\n",
        "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
        "  # If no exact match, take Wikipedia's suggestion.\n",
        "  except wikipedia.exceptions.PageError as e:\n",
        "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
        "  snippet = page[0:return_chars]\n",
        "  return snippet\n",
        "\n",
        "\n",
        "def get_wiki_query (llm_response, stop_text = \"<STOP>\"):\n",
        "  # Extract the wikipedia query from the LLM response.\n",
        "  # Assumes the query is in the first line.\n",
        "  first_line = llm_response.splitlines()[0]\n",
        "  query = first_line.split(stop_text)[0]\n",
        "  return query.strip() # Remove leading and trailing whitespace\n",
        "\n",
        "\n",
        "def wiki_tool_chain(model,\n",
        "                    parameters,\n",
        "                    context,\n",
        "                    exemplar,\n",
        "                    question,\n",
        "                    show_activity=False):\n",
        "  # Answer a query using wikipedia by calling an LLM.\n",
        "  step_one_call = (\n",
        "      f\"{context}\\n\\n{exemplar}\\n\\nQuestion: {question}\\nWikipedia Search:\"\n",
        "  )\n",
        "  if show_activity:\n",
        "    print(\"\\033[1mMaking the first LLM call...\\033[0m\\x1B[0m\")\n",
        "  step_one_response = call_llm(model, parameters, step_one_call, show_activity)\n",
        "  wiki_query = get_wiki_query(step_one_response)\n",
        "  wiki_text = wiki_tool(wiki_query)\n",
        "\n",
        "  step_two_call = (\n",
        "      f\"{step_one_call} {wiki_query}\\nWikipedia Article: {wiki_text}\\nAnswer: \"\n",
        "  )\n",
        "  if show_activity:\n",
        "    print(\"\\033[1mMaking the second LLM call...\\033[0m\\x1B[0m\")\n",
        "  step_two_response = call_llm(model, parameters, step_two_call, show_activity)\n",
        "\n",
        "  return step_two_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l9ChpYxWlS3"
      },
      "source": [
        "上記のコードを使用する例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChHBEqg7MQCZ"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "# Outside this notebook, set PROJECT_ID, LOCATION, and MODEL_NAME.\n",
        "# When running in the notebook, these are set in part 0.\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "# These settings control how deterministic the LLM response is.\n",
        "parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "context = \"\"\"Answer questions using a lookup of wikipedia.\n",
        "After each question, write a wikipedia search followed by '<STOP>'.\n",
        "The wikipedia search will be used to retrieve the most relevant content.\n",
        "A section of the wikipedia article will then be sent to the next LLM call.\n",
        "Use the text of the wikipedia article to answer the question.\"\"\"\n",
        "\n",
        "exemplar = \"\"\"Question: Who is Chancellor of Germany?\n",
        "Wikipedia Search: chancellor of Germany<STOP>\n",
        "Wikipedia Article: The chancellor of Germany, officially the federal chancellor of the Federal Republic of Germany, is the head of the federal government of Germany, and the commander in chief of the German Armed Forces during wartime. The chancellor is the chief executive of the Federal Cabinet and heads the executive branch. The chancellor is elected by the Bundestag on the proposal of the federal president and without debate (Article 63 of the German Constitution).The current officeholder is Olaf Scholz of the SPD, who was elected in December 2021, succeeding Angela Merkel. He was elected after the SPD entered into a coalition agreement with Alliance 90/The Greens and the FDP.\\n\\n\\n== History of the office ==\\nThe office of Chancellor has a long history, stemming back to the Holy Roman Empire, when the office of German archchancellor was usually held by archbishops of Mainz. The title was, at times, used in several states of German-speaking Europe. The modern office of chancellor was established with the\n",
        "Answer: Olaf Scholz\"\"\"\n",
        "\n",
        "answer = wiki_tool_chain(model,\n",
        "                         parameters,\n",
        "                         context,\n",
        "                         exemplar,\n",
        "                         \"What musician released the album 'Somebody in the Snow'?\",\n",
        "                         show_activity = False)\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oukbFAyxoNPR"
      },
      "source": [
        "`show_activity = true`を使用して、LLMコールの内訳を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU3h3GkcbgUn"
      },
      "outputs": [],
      "source": [
        "wiki_tool_chain(model,\n",
        "                parameters,\n",
        "                context,\n",
        "                exemplar,\n",
        "                \"What musician released the album 'Somebody in the Snow'?\",\n",
        "                show_activity = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjKjBgbMXWyZ"
      },
      "source": [
        "「質問」を変更して実験してみてください。`show_activity = true`を保持して、LLMチェーンの2つのステップを確認します。\n",
        "\n",
        "これは多くの質問ではうまくいきません。上記のように、私たちのツールはあまり良くなく、いくつかの質問で完全に失敗します。\n",
        "\n",
        "ツールの使用ベストプラクティスは[パート3で詳細](https://github.com/GoogleCloudPlatform/specialized-training-content/blob/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/#react-tools)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH4Z5cqnmkpM"
      },
      "source": [
        "# パート3：反応（推論 +演技）プロンプト\n",
        "\n",
        "React（推論 +アクション）は、外部システムと対話することにより、複雑なタスクを介して、思考とツールの使用のチェーンとツールの使用を組み合わせて合計します。\n",
        "\n",
        "Reactスタイルのプロンプトは、現在（2023年秋）ほとんどのプロンプト駆動型LLMタスクの最先端です。LLMまたはLLMベースのチャットボットまたはシステムが外部システムと対話するプラグインまたは拡張機能を使用すると、Reactスタイルのシステムを使用しています。一般に、最新の知識を反映するLLMシステムは、Hood-The-Hoodの下で反応スタイルの機能を使用して目に見えません。\n",
        "\n",
        "外部システムと対話しようとするLLM：\n",
        "\n",
        "<img src = \"https://raw.githubusercontent.com/GoogleCloudPlatform/specialized-training-content/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/images/4-robot.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mepNl0JxmsTF"
      },
      "source": [
        "## 反応の基本\n",
        "\n",
        "反応チェーンには通常、3つのインターリーブパーツがあります。\n",
        "-  **思考**：思考の連鎖のように、これらは最終出力に向けて進歩するため、LLMによって生成されるウェイポイント、計画、推論などです。\n",
        "-  **アクション**：LLMが生成したコマンド、呼び出し、または外部システムにアクセスする手順。外部システムは、情報を提供するツールである可能性がありますが、より一般的なものである可能性があります（つまり、アクションは外部システムの状態を観察または変更します）。\n",
        "-  **観測**：外部システムからの応答、フィードバック、結果など。LLMコールに挿入されて次の思考を生成します。\n",
        "\n",
        "これらの3つのステップは、LLMがタスクを完了するまで繰り返されます。\n",
        "\n",
        "考え方のプロンプトと同様に、この繰り返されるサイクルは「内部の独白」または「内部スピーチ」を形成しますが、行動する決定を重要な追加と、単なる推論を超えて行動からフィードバックします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cZ-EbgBm5Zz"
      },
      "source": [
        "### 反応チェーンはどのように見えますか\n",
        "\n",
        "ReactチェーンでLLMコールを分解する前に、完全なReactチェーンがどのように見えるかを確認するのに役立ちます。\n",
        "\n",
        "このチェーンでのアクションはウィキペディアの検索であり、観察はウィキペディアの記事からのスニペットです。\n",
        "\n",
        "LLMへの元の呼び出しは次のとおりです。\n",
        "「質問：誰が最初に生まれたのか、ロナルド・リーガンまたはジェラルド・フォード？」（今のところ、指示、模範などを無視して）。\n",
        "\n",
        "完成したReactチェーンはこのように見えます。完全な観察結果を読むために右にスクロールします。\n",
        "\n",
        "```\n",
        "質問：最初に生まれたのは誰ですか、ロナルドレーガンまたはジェラルドフォード？\n",
        "考え1：ロナルド・レーガンを調べて、彼がいつ生まれたのかを見る必要があります。\n",
        "アクション1：ロナルドレーガン<Stop>\n",
        "観察1：ロナルド・ウィルソン・レーガン（1911年2月6日 -  2004年6月5日）は、1981年から1989年まで米国の第40代大統領を務めたアメリカの政治家および俳優でした。そして、最初の離婚した大統領。レーガンはイリノイ州タンピコで生まれ、イリノイ州ディクソンで育ちました。彼はユーレカ大学で教育を受け、そこで経済学と社会学を学びました。卒業後、レーガンはカリフォルニアに移り、そこでラジオスポーツアナウンサーになりました。彼は後に演技に引っ越し、50以上の映画に出演しました。レーガンは、1947年から1952年までスクリーン俳優ギルドの社長を務めました。\n",
        "考え2：ロナルド・レーガンは1911年に生まれました。ジェラルド・フォードを調べて、彼がいつ生まれたのかを見る必要があります。\n",
        "アクション2：ジェラルドフォード<Stop>\n",
        "観察2：ジェラルド・ルドルフ・フォード・ジュニア（jerr-əld;生まれたレスリー・リンチ・キング・ジュニア、1913年7月14日 -  2006年12月26日）は、1974年から1977年に米国第38代大統領を務めたアメリカの政治家でした。以前は、1965年から1973年まで米国下院で共和党の指導者を務め、スピロアグニューの辞任の後、リチャードニクソン大統領によって40番目の副大統領に任命されました。フォードは、ニクソンが1974年に辞任したときに大統領職に成功しましたが、1976年に完全な任期に選挙で敗北しました。フォードは、大統領または副大統領の選挙を勝ち取らずに米国大統領になった唯一の人物です。フォードはネブラスカ州オマハで生まれ、ミシガン州グランドラピッズで育ちました。彼はミシガン大学に通い、そこで学校のフットボールチームでプレーしてから最終的にイェールロースクールに通いました。その後、彼は1942年から1946年まで米国海軍保護区に勤務しました。フォードは1949年にミシガン州5の米国代表として政治的キャリアを始めました。\n",
        "考え3：ジェラルドフォードは1913年に生まれました。1911年は1913年以前です。回答[ロナルドレーガン]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlDmYHuInLVb"
      },
      "source": [
        "### 反応チェーンを分解します\n",
        "\n",
        "上記の例の反応チェーンは、3つのLLM呼び出しから構築されています。\n",
        "\n",
        "注このセクションの回答は、パート2ツールでのディスカッションで追加のテキストがどのように削除されたかと同様に、余分な予測されたテキストを削除されています。\n",
        "\n",
        "**電話1：**\n",
        "```\n",
        "質問：最初に生まれたのは誰ですか、ロナルドレーガンまたはジェラルドフォード？\n",
        "考え1：\n",
        "```\n",
        "**応答1：**\n",
        "```\n",
        "ロナルド・レーガンを調べて、彼がいつ生まれたのかを見る必要があります。\n",
        "アクション1：ロナルドレーガン<Stop>\n",
        "```\n",
        "\n",
        "最初のLLMコールは次のとおりです。\n",
        "1.以前のLLMコールプラス\n",
        "1.前のコールプラスへのLLM応答\n",
        "1.ウィキペディアルックアップ結果プラス\n",
        "1.「思考#：」\n",
        "\n",
        "**コール2：**\n",
        "\n",
        "コール2は、連結コール1 +応答1 +ウィキペディアルックアップの結果（観察中） +「思考2：」によって作成されます。\n",
        "```\n",
        "質問：最初に生まれたのは誰ですか、ロナルドレーガンまたはジェラルドフォード？\n",
        "考え1：ロナルド・レーガンを調べて、彼がいつ生まれたのかを見る必要があります。\n",
        "アクション1：ロナルドレーガン<Stop>\n",
        "観察1：ロナルド・ウィルソン・レーガン（1911年2月6日 -  2004年6月5日）は、1981年から1989年まで米国の第40代大統領を務めたアメリカの政治家および俳優でした。そして、最初の離婚した大統領。レーガンはイリノイ州タンピコで生まれ、イリノイ州ディクソンで育ちました。彼はユーレカ大学で教育を受け、そこで経済学と社会学を学びました。卒業後、レーガンはカリフォルニアに移り、そこでラジオスポーツアナウンサーになりました。彼は後に演技に引っ越し、50以上の映画に出演しました。レーガンは、1947年から1952年までスクリーン俳優ギルドの社長を務めました。\n",
        "考え2：\n",
        "```\n",
        "\n",
        "**応答2：**\n",
        "```\n",
        "ロナルド・レーガンは1911年に生まれました。ジェラルド・フォードを調べて、彼がいつ生まれたのかを見る必要があります。\n",
        "アクション2：ジェラルドフォード<Stop>\n",
        "```\n",
        "\n",
        "**電話3：**\n",
        "\n",
        "コール2と同じように、Wikipedia Lookup + \"Thought 3：\"の結果、コール2 +応答2 +を連結してコール3を作成します。\n",
        "\n",
        "```\n",
        "質問：最初に生まれたのは誰ですか、ロナルドレーガンまたはジェラルドフォード？\n",
        "考え1：ロナルド・レーガンを調べて、彼がいつ生まれたのかを見る必要があります。\n",
        "アクション1：ロナルドレーガン<Stop>\n",
        "観察1：ロナルド・ウィルソン・レーガン（1911年2月6日 -  2004年6月5日）は、1981年から1989年まで米国の第40代大統領を務めたアメリカの政治家および俳優でした。そして、最初の離婚した大統領。レーガンはイリノイ州タンピコで生まれ、イリノイ州ディクソンで育ちました。彼はユーレカ大学で教育を受け、そこで経済学と社会学を学びました。卒業後、レーガンはカリフォルニアに移り、そこでラジオスポーツアナウンサーになりました。彼は後に演技に引っ越し、50以上の映画に出演しました。レーガンは、1947年から1952年までスクリーン俳優ギルドの社長を務めました。\n",
        "考え2：ロナルド・レーガンは1911年に生まれました。ジェラルド・フォードを調べて、彼がいつ生まれたのかを見る必要があります。\n",
        "アクション2：ジェラルドフォード<Stop>\n",
        "観察2：ジェラルド・ルドルフ・フォード・ジュニア（jerr-əld;生まれたレスリー・リンチ・キング・ジュニア、1913年7月14日 -  2006年12月26日）は、1974年から1977年に米国第38代大統領を務めたアメリカの政治家でした。以前は、1965年から1973年まで米国下院で共和党の指導者を務め、スピロアグニューの辞任の後、リチャードニクソン大統領によって40番目の副大統領に任命されました。フォードは、ニクソンが1974年に辞任したときに大統領職に成功しましたが、1976年に完全な任期に選挙で敗北しました。フォードは、大統領または副大統領の選挙を勝ち取らずに米国大統領になった唯一の人物です。\n",
        "フォードはネブラスカ州オマハで生まれ、ミシガン州グランドラピッズで育ちました。彼はミシガン大学に通い、そこで学校のサッカーチームでプレーしてから最終的にイェールロースクールに通いました。その後、彼は1942年から1946年まで米国海軍保護区に勤務しました。フォードは1949年にミシガン州5の米国代表として政治的キャリアを始めました。\n",
        "考え3：\n",
        "```\n",
        "\n",
        "最後に、LLMは答えを返します。\n",
        "\n",
        "**応答3：**\n",
        "```\n",
        "ジェラルドフォードは1913年に生まれました。1911年は1913年以前です。回答[ロナルドレーガン]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX060zm2p4A5"
      },
      "source": [
        "## 反応チェーンを手動で実行します\n",
        "\n",
        "このセクションでは、Reactチェーンを段階的に実行します。\n",
        "\n",
        "次のコードセルには、いくつかのことが必要です。\n",
        "1. LLMが反応する方法を理解するための指示（コンテキスト）。\n",
        "2.少なくとも1つの模範。\n",
        "3. LLMのアクションを実行するツール。\n",
        "4. LLM呼び出しを行うパームAPIモデルオブジェクト。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk1oTh8HuXoB"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"Answer questions with thoughts, actions, and observations.\n",
        "\n",
        "Think about the next action to take. Then take an action.\n",
        "All actions are a lookup of Wikipedia.\n",
        "The Wikipedia action returns the beginning of the best-matching article.\n",
        "When making a Wikipedia lookup action, end the lookup with <STOP>.\n",
        "After the Wikipedia action, you will have an observation.\n",
        "The observation is based on what you learn from the Wikipedia lookup action.\n",
        "After the observation, begin the loop again with a thought.\n",
        "\n",
        "Repeat as necessary a thought, taking an action, and having an observation.\n",
        "Keep repeating as necessary until you know the answer to the question.\n",
        "When you think you have an answer, return the answer in the format:\n",
        "\"Answer[answer goes here between square brackets]\" as part of a thought.\n",
        "Make sure to capitalize \"Answer\".\n",
        "\n",
        "Only use information in the observations to answer the question.\"\"\"\n",
        "\n",
        "exemplar = \"\"\"Example:\n",
        "Question: Who was born first, Ronald Reagan or Gerald Ford?\n",
        "Thought 1: I need to look up Ronald Reagan and see when he was born.\n",
        "Action 1: Ronald Reagan<STOP>\n",
        "Observation 1: Ronald Wilson Reagan (February 6, 1911 – June 5, 2004) was an American politician and actor who served as the 40th president of the United States from 1981 to 1989. A conservative, he was the first president from the West Coast and the first divorced president. Reagan was born in Tampico, Illinois, and raised in Dixon, Illinois. He was educated at Eureka College, where he studied economics and sociology. After graduating, Reagan moved to California, where he became a radio sports announcer. He later moved into acting, appearing in over 50 films. Reagan served as president of the Screen Actors Guild from 1947 to 1952.\n",
        "Thought 2: Ronald Reagan was born in 1911. I need to look up Gerald Ford and see when he was born.\n",
        "Action 2: Gerald Ford<STOP>\n",
        "Observation 2: Gerald Rudolph Ford Jr. ( JERR-əld; born Leslie Lynch King Jr.; July 14, 1913 – December 26, 2006) was an American politician who served as the 38th president of the United States from 1974 to 1977. He previously served as the leader of the Republican Party in the U.S. House of Representatives from 1965 to 1973, when he was appointed the 40th vice president by President Richard Nixon, after Spiro Agnew's resignation. Ford succeeded to the presidency when Nixon resigned in 1974, but was defeated for election to a full term in 1976. Ford is the only person to become U.S. president without winning an election for president or vice president.\n",
        "Ford was born in Omaha, Nebraska and raised in Grand Rapids, Michigan. He attended the University of Michigan, where he played for the school's football team before eventually attending Yale Law School. Afterward, he served in the U.S. Naval Reserve from 1942 to 1946. Ford began his political career in 1949 as the U.S. representative from Michigan's 5\n",
        "Thought 3: Gerald Ford was born in 1913. 1911 is before 1913. Answer[Ronald Reagan]\"\"\"\n",
        "\n",
        "# Code for calling Wikipedia.\n",
        "import wikipedia\n",
        "def wiki_tool(query, return_chars = 1000):\n",
        "  try:\n",
        "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
        "  # If no exact match, take Wikipedia's suggestion.\n",
        "  except wikipedia.exceptions.PageError as e:\n",
        "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
        "  snippet = page[0:return_chars]\n",
        "  return snippet\n",
        "\n",
        "# Initialized PaLM API model.\n",
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "# These settings control how deterministic the LLM response is.\n",
        "parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(MODEL_NAME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5wOlmfAv53K"
      },
      "source": [
        "最初のLLMコールは、コンテキスト、模範、質問、および最初の思考のラベルです。\n",
        "\n",
        "各ラインの開始時のアクション/思考/観測ラベルは、チェーンを反応するために重要であり、LLM応答がインターリーブ反応ステップの「スクリプト」に固執する可能性を高めます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afRXzBhlwBw6"
      },
      "outputs": [],
      "source": [
        "question = \"When was the opening year of the theater that debuted Ibsen's 'A Doll's House'?\"\n",
        "llm_call_1 = f\"{context}\\n\\n{exemplar}\\n\\nQuestion: {question}\\nThought 1:\"\n",
        "print(llm_call_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFm05Ymwwjwc"
      },
      "outputs": [],
      "source": [
        "response_1 = call_llm(model, parameters, llm_call_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVPLsqolw16U"
      },
      "source": [
        "応答の1行目と2行目は良いです。このモデルは、合理的な思考と適切なアクションを生成しました。\n",
        "\n",
        "しかし、上記のツールを使用するのと同じように、LLMはごみのテキストを生成し続けます。LLMSは次のトークンを繰り返し予測し、ReactスタイルのLLMコールでは、次のトークンがRLMの残りのReactチェーンの予測であることを忘れないでください。\n",
        "\n",
        "ツール使用セクションと同じように、追加のテキストが破棄されます。最初の2つの応答線のみが保持されます：「Thought 1」と「アクション1」。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbgFW4Ehy6gh"
      },
      "outputs": [],
      "source": [
        "# Only take the first two lines of the response.\n",
        "# Splitlines returns a list with an item for each line.\n",
        "response_1 = response_1.splitlines()[0:2]\n",
        "# Turn response 1 into text from the list so we can concatenate to llm call 1.\n",
        "response_1 = (\"\\n\").join(response_1)\n",
        "print(response_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZxIR6nmuCLl"
      },
      "source": [
        "次に、LLMの「アクション1」応答でウィキペディアツールをクエリします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU7ExxFq0odj"
      },
      "outputs": [],
      "source": [
        "# Look up the LLM's action in Wikipedia.\n",
        "wiki_text_1 = wiki_tool(\"A Doll's House\")\n",
        "print(wiki_text_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAWEAjWw3MyU"
      },
      "source": [
        "次に、wikipediaツール出力を「観測1」として追加して、次のLLMコールを作成し、「観測2」の考えを追加します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn4C9X7H0vT4"
      },
      "outputs": [],
      "source": [
        "# Construct the next LLM call.\n",
        "llm_call_2 = f\"{llm_call_1} {response_1}\\nObservation 1: {wiki_text_1}\\nThought 2:\"\n",
        "print(llm_call_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNS0yZre1Obb"
      },
      "outputs": [],
      "source": [
        "response_2 = call_llm(model, parameters, llm_call_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dJZdWI91Xut"
      },
      "source": [
        "Reactチェーンの3回目のLLMコールの場合、2回目の呼び出しと同じ手順に従います。\n",
        "1. 応答の最初の2行を取ります。\n",
        "2. ウィキペディアのアクションを調べます。\n",
        "3. 応答、ウィキペディア出力、および以前のLLMコールからLLMコールを組み立てます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yioUsUmI1mdf"
      },
      "outputs": [],
      "source": [
        "# Only take the first two lines of the response.\n",
        "# Splitlines returns a list with an item for each line.\n",
        "response_2 = response_2.splitlines()[0:2]\n",
        "# Turn response 1 into text from the list so we can concatenate to llm call 1.\n",
        "response_2 = (\"\\n\").join(response_2)\n",
        "print(response_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plRMm1DS1mdf"
      },
      "outputs": [],
      "source": [
        "# Look up the LLM's action in Wikipedia.\n",
        "wiki_text_2 = wiki_tool(\"Royal Theatre in Copenhagen, Denmark\")\n",
        "print(wiki_text_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEqsooGh1mdf"
      },
      "outputs": [],
      "source": [
        "# Construct the next LLM call.\n",
        "llm_call_3 = f\"{llm_call_2} {response_2}\\nObservation 2: {wiki_text_2}\\nThought 3:\"\n",
        "print(llm_call_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gWWK89A1mdf"
      },
      "outputs": [],
      "source": [
        "response_3 = call_llm(model, parameters, llm_call_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsVNvZ5F2HjV"
      },
      "source": [
        "そして、私たちには答えがあります！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfmXEYdg2aMb"
      },
      "source": [
        "## Running Reactチェーンのための完全なPythonコードスニペット\n",
        "\n",
        "アプリケーションでReactを使用するには、以前に手動で実行された手順を自動化する必要があります。\n",
        "\n",
        "以下の指導的コードスニペットは、Reactチェーンを実行します。LLMへのフォーマットされたReactコールを作成し、アクションを抽出し、アクションを実行し、LLMが回答で応答したかどうかを検出します。\n",
        "\n",
        "それは**非常に**あなたが以下のコードを歩いて、コメントを読んで、Reactチェーンがどのように自動化されているかをよりよく理解することをお勧めします。\n",
        "\n",
        "これは生産対応のコードではありません：\n",
        "1. スニペットは、この特定の最小限のReactの例にハードコードされています。Reactチェーンは異なるように見えます（これについては後で詳しく説明します）、Reactチェーンで構築された有用なアプリケーションにはカスタマイズされたツールが必要です。\n",
        "2. スニペットは脆い、特にベアボーンウィキペディアツール。\n",
        "3. LLMは、以前のアクションを再評価し、無限にループを反応させる可能性があります。このスニペットは、「MAX_STEPS」LLMコールの後に停止し、生産Reactコードはループをキャッチして回復しようとするはずです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVc3xRoWw1HM"
      },
      "outputs": [],
      "source": [
        "import wikipedia\n",
        "\n",
        "def call_llm(model, parameters, llm_call, show_activity = True):\n",
        "  # Wraps an LLM call to Vertex, optionally displaying the call and response.\n",
        "  response = model.predict(llm_call, **parameters).text\n",
        "\n",
        "  if show_activity:\n",
        "    BOLD = \"\\033[1m\"\n",
        "    UNFORMAT = \"\\033[0m\\x1B[0m\"\n",
        "    print(f\"{BOLD}The call to the LLM:{UNFORMAT}\\n{llm_call}\\n\")\n",
        "    print(f\"{BOLD}The response:{UNFORMAT}\")\n",
        "    print(response)\n",
        "  return response  # Return to `_` if not needed.\n",
        "\n",
        "\n",
        "def wiki_tool(query, return_chars = 1000):\n",
        "  try:\n",
        "    page = wikipedia.page(query, auto_suggest=False, redirect=True).content\n",
        "  # If no exact match, take Wikipedia's suggestion.\n",
        "  except wikipedia.exceptions.PageError as e:\n",
        "    page = wikipedia.page(query, auto_suggest=True, redirect=True).content\n",
        "  snippet = page[0:return_chars]\n",
        "  return snippet\n",
        "\n",
        "\n",
        "def wiki_react_chain(model,\n",
        "                     parameters,\n",
        "                     context,\n",
        "                     exemplar,\n",
        "                     question,\n",
        "                     max_steps=7,\n",
        "                     show_activity=False):\n",
        "  # Call an LLM in a ReACT-style Thought -> Action -> Observation loop.\n",
        "  # Call the LLM max_steps times or to an answer in the pattern Answer[ans].\n",
        "\n",
        "  # Construct the first LLM call, teeing up the first thought.\n",
        "  next_llm_call = f\"{context}\\n\\n{exemplar}\\n\\nQuestion: {question}\\nThought 1:\"\n",
        "\n",
        "  step = 1\n",
        "  while step <= max_steps:\n",
        "\n",
        "    if show_activity:\n",
        "      print(f\"\\033[1mReAct chain step {step}:\\033[0m\\x1B[0m\")\n",
        "    llm_response = call_llm(model, parameters, next_llm_call, show_activity)\n",
        "\n",
        "    # Check for an answer. Look only at the first line of the response, since\n",
        "    #   the LLM will continue predicting beyond the next thought.\n",
        "    # This is brittle, it assumes no line breaks in the thought.\n",
        "    response_first_line = llm_response.splitlines()[0]\n",
        "    first_line_answer_split = response_first_line.split(\"Answer[\")\n",
        "    if len(first_line_answer_split) > 1:  # If there's a split on \"Answer[\".\n",
        "      # Return the answer, removing the \"]\" that comes after the answer.\n",
        "      return first_line_answer_split[1].split(\"]\")[0]\n",
        "\n",
        "    # If no answer, assume following response line is action.\n",
        "    response_second_line = llm_response.splitlines()[1]\n",
        "    \"\"\"\n",
        "      Note the hard coded \"<STOP>\" characters marking the end of the action.\n",
        "      This isn't strictly necessary if we assume the first line in the LLM\n",
        "      response is the thought and the second is the action, and that any\n",
        "      subsequent lines are garbage. But instructing the LLM to explicitly signal\n",
        "      structure it the response often gives more structurally consistent\n",
        "      responses, and also makes it easier to detect one way ReAct can fail.\n",
        "    \"\"\"\n",
        "    # Extract the wiki query from the action line of the response.\n",
        "    wiki_query = response_second_line.split(\":\")[1].split(\"<STOP>\")[0]\n",
        "    # Remove leading/trailing whitespace.\n",
        "    wiki_query = wiki_query.strip()\n",
        "    if show_activity:\n",
        "      print(f\"\\033[1mQuerying wikipedia for: {wiki_query}.\\033[0m\\x1B[0m\")\n",
        "    wiki_text = wiki_tool(wiki_query)\n",
        "\n",
        "    # Assemble the next LLM call.\n",
        "    # Only use the lines of the LLM response with the first thought and action.\n",
        "    usable_response = f\"{response_first_line}\\n{response_second_line}\"\n",
        "    # Assemble the wiki response into the observation line.\n",
        "    obs = f\"Observation {step}: {wiki_text}\"\n",
        "    step += 1\n",
        "    # Previous llm call + the first action and thought in the response +\n",
        "    # the result of the wikipedia lookup = llm call for next ReAct step.\n",
        "    # Note that next_llm_call was the last call we made, but we reassign it to\n",
        "    #   the same variable name so the loop works.\n",
        "    next_llm_call = f\"{next_llm_call} {usable_response}\\n{obs}\\nThought {step}:\"\n",
        "\n",
        "  # If max_steps exceeded and the loop exits.\n",
        "  # Would be better to raise an exception.\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoGC1n_K7r3t"
      },
      "source": [
        "上記の反応チェーンコードスニペットを使用した例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-6qK3n7-6uh"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "# Outside this notebook, set PROJECT_ID, LOCATION, and MODEL_NAME.\n",
        "# When running in the notebook, these are set in part 0.\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "# These settings control how deterministic the LLM response is.\n",
        "parameters = {\n",
        "    \"temperature\": 0,\n",
        "    \"max_output_tokens\": 256,\n",
        "    \"top_p\": 0.8,\n",
        "    \"top_k\": 40\n",
        "}\n",
        "model = TextGenerationModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "context = \"\"\"Answer questions with thoughts, actions, and observations.\n",
        "\n",
        "Think about the next action to take. Then take an action.\n",
        "All actions are a lookup of wikipedia.\n",
        "The wikipedia action returns the beginning of the best-matching article.\n",
        "When making a wikipedia lookup action, end the lookup with <STOP>.\n",
        "After the wikipedia action, you will make an observation.\n",
        "The observation is based on what you learn from the wikipedia lookup action.\n",
        "After the observation, begin the loop again with a thought.\n",
        "\n",
        "Repeat as necessary a thought, taking an action, and making an observation.\n",
        "Keep repeating as necessary until you know the answer to the question.\n",
        "When you think you have an answer, return the answer in the format:\n",
        "\"Answer[answer goes here between square brackets]\"\n",
        "as part of a thought. Make sure to capitalize \"Answer\".\n",
        "\n",
        "Only use information in the observations to answer the question.\"\"\"\n",
        "\n",
        "exemplar = \"\"\"Example:\n",
        "Question: Who was born first, Ronald Reagan or Gerald Ford?\n",
        "Thought 1: I need to look up Ronald Reagan and see when he was born.\n",
        "Action 1: Ronald Reagan<STOP>\n",
        "Observation 1: Ronald Wilson Reagan (February 6, 1911 – June 5, 2004) was an American politician and actor who served as the 40th president of the United States from 1981 to 1989. A conservative, he was the first president from the West Coast and the first divorced president. Reagan was born in Tampico, Illinois, and raised in Dixon, Illinois. He was educated at Eureka College, where he studied economics and sociology. After graduating, Reagan moved to California, where he became a radio sports announcer. He later moved into acting, appearing in over 50 films. Reagan served as president of the Screen Actors Guild from 1947 to 1952.\n",
        "Thought 2: Ronald Reagan was born in 1911. I need to look up Gerald Ford and see when he was born.\n",
        "Action 2: Gerald Ford<STOP>\n",
        "Observation 2: Gerald Rudolph Ford Jr. ( JERR-əld; born Leslie Lynch King Jr.; July 14, 1913 – December 26, 2006) was an American politician who served as the 38th president of the United States from 1974 to 1977. He previously served as the leader of the Republican Party in the U.S. House of Representatives from 1965 to 1973, when he was appointed the 40th vice president by President Richard Nixon, after Spiro Agnew's resignation. Ford succeeded to the presidency when Nixon resigned in 1974, but was defeated for election to a full term in 1976. Ford is the only person to become U.S. president without winning an election for president or vice president.\n",
        "Ford was born in Omaha, Nebraska and raised in Grand Rapids, Michigan. He attended the University of Michigan, where he played for the school's football team before eventually attending Yale Law School. Afterward, he served in the U.S. Naval Reserve from 1942 to 1946. Ford began his political career in 1949 as the U.S. representative from Michigan's 5\n",
        "Thought 3: Gerald Ford was born in 1913. 1911 is before 1913. Answer[Ronald Reagan]\"\"\"\n",
        "\n",
        "question = \"What city was the youngest of the three engineers who designed the Ford T Model born in?\"\n",
        "\n",
        "answer = wiki_react_chain(model,\n",
        "                          parameters,\n",
        "                          context,\n",
        "                          exemplar,\n",
        "                          question,\n",
        "                          show_activity = True)\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7L113wd8gdy"
      },
      "source": [
        "上記の「質問」を変更して実験します。あなたは素晴らしい結果を得ることができないかもしれません。これは、脆いウィキペディアツールによるものかもしれませんが、反応のエラーも表示される場合があります。\n",
        "\n",
        "コンテキストまたは模範を変更することにより、反応障害のパフォーマンスをどのように改善できるかを考えてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joeGGFHunvFW"
      },
      "source": [
        "## その他の反応のユースケース\n",
        "\n",
        "Reactパターンは、質問に答えるだけではありません。\n",
        "\n",
        "異なるコンテキストと模範で、上記のReactコードスニペットは事実チェックに適合しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qD06UTNDoVIm"
      },
      "outputs": [],
      "source": [
        "question = \"The GDP of Japan is higher than the GDP of BRICS.\"\n",
        "\n",
        "context = \"\"\"You are verifying claims as true or false.\n",
        "Verify the claim with thoughts, actions, and observations.\n",
        "Determine if there is an observation that SUPPORTS or REFUTES the claim.\n",
        "\n",
        "Think about the next action to take to verify the claim. Then take an action.\n",
        "All actions are a lookup of wikipedia.\n",
        "The wikipedia action returns the beginning of the best-matching article.\n",
        "When making a wikipedia lookup action, end the lookup with <STOP>.\n",
        "After the wikipedia action, you will make an observation.\n",
        "The observation is based on what you learn from the wikipedia lookup action.\n",
        "After the observation, begin the loop again with a thought.\n",
        "\n",
        "Repeat as necessary a thought, taking an action, and making an observation.\n",
        "Keep repeating as necessary until you reach a conclusion about the claim.\n",
        "If an observation refutes the claim, return the answer as \"Answer[REFUTES]\".\n",
        "If an observation supports the claim, return the answer as \"Answer[SUPPORTS]\".\n",
        "\n",
        "Only use information in the observations to answer the question.\"\"\"\n",
        "\n",
        "exemplar = \"\"\"Example:\n",
        "Claim: Ronald Reagan was born before Gerald Ford.\n",
        "Thought 1: I need to look up Ronald Reagan and see when he was born.\n",
        "Action 1: Ronald Reagan<STOP>\n",
        "Observation 1: Ronald Wilson Reagan (February 6, 1911 – June 5, 2004) was an American politician and actor who served as the 40th president of the United States from 1981 to 1989. A conservative, he was the first president from the West Coast and the first divorced president. Reagan was born in Tampico, Illinois, and raised in Dixon, Illinois. He was educated at Eureka College, where he studied economics and sociology. After graduating, Reagan moved to California, where he became a radio sports announcer. He later moved into acting, appearing in over 50 films. Reagan served as president of the Screen Actors Guild from 1947 to 1952.\n",
        "Thought 2: Ronald Reagan was born in 1911. I need to look up Gerald Ford and see when he was born.\n",
        "Action 2: Gerald Ford<STOP>\n",
        "Observation 2: Gerald Rudolph Ford Jr. ( JERR-əld; born Leslie Lynch King Jr.; July 14, 1913 – December 26, 2006) was an American politician who served as the 38th president of the United States from 1974 to 1977. He previously served as the leader of the Republican Party in the U.S. House of Representatives from 1965 to 1973, when he was appointed the 40th vice president by President Richard Nixon, after Spiro Agnew's resignation. Ford succeeded to the presidency when Nixon resigned in 1974, but was defeated for election to a full term in 1976. Ford is the only person to become U.S. president without winning an election for president or vice president.\n",
        "Ford was born in Omaha, Nebraska and raised in Grand Rapids, Michigan. He attended the University of Michigan, where he played for the school's football team before eventually attending Yale Law School. Afterward, he served in the U.S. Naval Reserve from 1942 to 1946. Ford began his political career in 1949 as the U.S. representative from Michigan's 5\n",
        "Thought 3: Gerald Ford was born in 1913. Ronald Reagan was born in 1911. 1911 is before 1913. Ronald Reagan was born before Gerald Ford. Answer[SUPPORTS]\"\"\"\n",
        "\n",
        "answer = wiki_react_chain(model,\n",
        "                          parameters,\n",
        "                          context,\n",
        "                          exemplar,\n",
        "                          question,\n",
        "                          show_activity = True)\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6d_bgAVJgLa"
      },
      "source": [
        "ウィキペディアツールの制限は、このプロンプトのユーティリティを制限し、ニュートラルな「十分な情報」の回答に対するサポートの欠如も同様です。\n",
        "\n",
        "ただし、このユースケースにどのように簡単に適応したかを考えてください。反応パターンは、次のことでも良い結果を示しています。\n",
        "* テキストベースの仮想世界とのナビゲートと対話。\n",
        "* Webをサーフィンします。\n",
        "* 購入手順を使用して、eコマーストランザクションを作成します。\n",
        "* ジャーナル記事の文献検索の実施。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHexpQOYLb9E"
      },
      "source": [
        "<a name=\"react-tools\"> </a>\n",
        "## ツールの使用ベストプラクティス\n",
        "\n",
        "上記のプロンプトを実験した場合、おそらく失敗が発生した可能性があります。多くの場合、これはウィキペディアツールが限られているためです。\n",
        "\n",
        "いくつかのベストプラクティスに従うことで、この教育の例よりも堅牢で効果的なツールを構築するのに役立ちます。\n",
        "\n",
        "1. Do プロンプト内でツールとその使用方法を明確に記述しましょう。\n",
        "  * 理想的なツール使用を示すfew-shot事例を含めましょう。\n",
        "  * 例えば、「doc search」とだけ記述されたツールは、「自然言語クエリを使用して社内文書を検索します。レスポンスは、クエリとの関連性が高い順に並べられた文書名のリストです。」のように記述された同じツールよりもパフォーマンスが低くなります。\n",
        "2. Do ツールの範囲と複雑さを慎重に検討しましょう。\n",
        "  * ツールのAPIがLLMにとって十分にシンプルかどうかを検討しましょう。\n",
        "  * 多くの場合、1つの複雑なツールよりも複数のシンプルなツールの方が効果的です。開発者にとっては単一のAPIであっても、LLMツールとしては複数のツールに分けた方が良い場合があります。\n",
        "  * 例えば、ユースケースでデータベースにアクセスするためにSQLを実行する必要がある場合、LLMを使用してSQLクエリを最初から生成するのではなく、いくつかの個別のSQLテンプレートを個々のツールとして検討しましょう。\n",
        "3. Do ツール出力を構造的および文体的に一貫させましょう。\n",
        "  * ツール出力のバリエーションが少ないほど、LLMがその出力を効果的に使用する可能性が高くなります。\n",
        "4. Do ツール出力を短く、関連性の高いものにしましょう。\n",
        "  * 冗長なツール出力は、LLMの入力長制限に負担をかける可能性があります。\n",
        "  * ReAct論文のWikipediaエージェント実装は素晴らしい例です。Wikipedia記事内を検索し、記事全体ではなく、見つかった用語の周辺のテキストスニペットのみを返します。\n",
        "5. Do エラーを適切に処理しましょう。\n",
        "  * 例外をキャッチし、有用なエラーメッセージを提供しましょう。\n",
        "  * タイムアウトやレート制限などのツール側の問題を管理しましょう。\n",
        "  * few-shot事例でエラー処理を示しましょう。\n",
        "  * ツールが失敗し、次のLLM呼び出しで有用なエラーを提供した場合、LLMは自己修正する可能性があります。\n",
        "6. Do ツール使用プロンプトをチューニングしましょう。\n",
        "  * 様々なツール使用を含むパラメータ効率の良いチューニングセット（わずか10個の例でも）は、パフォーマンスを大幅に向上させることができます。\n",
        "7. Do LLMを呼び出してツールアクションを生成する際の出力長を制限しましょう。\n",
        "  * LLMはツールアクションを超えてテキストを生成し続けます。\n",
        "8. Don't セキュリティを忘れないでください。多くのツール使用パターンはセキュリティリスクを生み出します。\n",
        "  * LLMのツールを介してアクセス可能なものはすべて、敵対的な入力で実験するエンドユーザーに見られる可能性があると想定しましょう。\n",
        "  * LLMのツール呼び出しが悪意のあるものであることはないと想定しないでください。例えば、SQLインジェクションはLLMツールを介して可能です。\n",
        "\n",
        "このノートブックのツールは、これらのベストプラクティスの多くに従っていません。\n",
        "\n",
        "1. Wikipediaの記事は構造が予測できません。\n",
        "1. Wikipediaの記事は数千語になることもありますが、このツールは記事の関連部分に焦点を当てることをサポートしていません。\n",
        "1. プロンプトではWikipediaとは何か、またはその使用方法が説明されていません（ただし、LLMはトレーニングデータからWikipediaが何かを「知って」います）。\n",
        "1. エラーメッセージがなく、エラー処理は最小限です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZyRtBuT_eSQ"
      },
      "source": [
        "## 利点を反応します\n",
        "\n",
        "1. 幻覚が少ない。\n",
        "  * 信頼できる情報源とLLMの「メモリ」に依存する接地。\n",
        "1. 再訓練なしでLLMの知識を更新/拡張します。\n",
        "1. 既製のLLMSで動作し、追加のLLMトレーニングやチューニングは必要ありません。\n",
        "1. さまざまなユースケースをサポートします。\n",
        "1. 複数のツールで動作します。\n",
        "1. ツールを改善することでシステム全体のパフォーマンスを改善することは、プロンプトまたはLLM自体を改善するよりも簡単です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mgJ8MSQKIkt"
      },
      "source": [
        "## React Disdvantages\n",
        "\n",
        "1. 複数のLLM呼び出しにより、遅い（高レイテンシー）および高価です。\n",
        "1. 外部ツールは、維持およびセキュリティの懸念を維持し、より多くのシステムコンポーネントを意味します。\n",
        "1. 反応ループとその他の非回答シナリオが一般的です。\n",
        "  * Vs.幻覚がより一般的であると思考の連鎖。\n",
        "  * 専門的または最新の情報を必要としないユースケースの場合、思考の連鎖は反応する可能性があります。\n",
        "1. 反応推論（Think-> Act）は柔軟性が低く、純粋な一連の思考の連鎖のより柔軟な推論とパフォーマンスが低下する可能性があります。\n",
        "1. 外部情報が必要な場合、RAGよりも複雑な場合、検索がLLMによって制御されない場合にアプローチします。\n",
        "1. ツール統合を超えて、追加の機能が必要です。\n",
        "  * ループ救済。\n",
        "  * ツールエラーの管理。\n",
        "  * 思考のチェーンフォールバック。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6gBOX-yj6Ar"
      },
      "source": [
        "## ベストプラクティスを反応します\n",
        "\n",
        "[ツール使用](https://github.com/GoogleCloudPlatform/specialized-training-content/blob/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/#react-tools)上記のベストプラクティスを超えて。\n",
        "\n",
        "1. Don't temperature=0を盲目的に使用しないでください。\n",
        "  * タスクやツールの記述方法によってパフォーマンスが大きく変わる可能性があります。\n",
        "  * \"Thought\"、\"Action\"、\"Observation\"以外のラベルとステップのスキップを含む事例を試してみましょう。\n",
        "  * 様々な思考/推論と行動スタイルの事例を試してみましょう。例えば：\n",
        "    * 次のアクションを特定する思考が最適なタスクもあれば、最初の思考で完全な計画を立てるのが最適なタスクもあります。\n",
        "    * 無関係な観察やツールエラーの後に計画を調整したり、前の思考を再考したりする思考/行動を示しましょう。\n",
        "  * 直前の観察の最も重要な部分を言い換える思考を試してみましょう。\n",
        "1. Do ReActチェーンがループに陥るのをキャッチしましょう。\n",
        "  * ループのキャッチを示す事例で実験しましょう。\n",
        "  * 繰り返される行動をキャッチし、繰り返される行動を指摘する観察をLLMに返すことを検討しましょう。LLMは回復できるかもしれません。\n",
        "  * temperature > 0でループしているチェーンを再実行してみましょう。\n",
        "  * ReActが研究ベンチマークデータセットで最先端である場合、それは多くの場合、連鎖思考の自己整合性フォールバックを伴います。\n",
        "1. Do ファインチューニングを活用しましょう。\n",
        "  * ReActチェーン全体にわたるチューニング事例を含め、最初または最後のLLM呼び出しの事例だけでなく、全体を含めましょう。\n",
        "  * エラー/失敗処理をチューニングデータに含めましょう。\n",
        "  * 最終的な答えが正しい場合でも、不正確なReAct推論を含むチューニング事例を使用しないでください。\n",
        "1. Don't より単純な代替案を評価せずにReActを実装しないでください。\n",
        "  * 管理された拡張機能/プラグインを検討しましょう。\n",
        "    * 拡張機能サービスは、セキュリティ、可観測性、監視、評価などを提供し、実装作業を削減する可能性があります。\n",
        "    * 技術的な評価なしに、管理された拡張機能/プラグインサービスがニーズを満たすと想定しないでください。\n",
        "  * 外部知識をLLM呼び出しに統合するより簡単な方法を検討しましょう。（例：上記のRAGパターン1）。\n",
        "1. Do 大規模なReActのデバッグにLLMを使用しましょう。\n",
        "  * LLMに、タイプ別に失敗を分類する（例：推論ミス、ツール検索失敗、ループに陥った）および/またはReActチェーンの各ステップを正しいか間違っているかを識別するように促します。\n",
        "1. Do テスト、パフォーマンス測定（ドリフトを含む）、システム監視、CI/CDなどにツール機能を含めましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TeXWM0yxb8J"
      },
      "source": [
        "# パート4：LangchainとReact\n",
        "<img src = \"https://raw.githubusercontent.com/GoogleCloudPlatform/specialized-training-content/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/app_dev_llm/images/5-chained.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IAVv4HGtafY"
      },
      "source": [
        "Langchain は、LLM をすぐに始めるための優れたライブラリです。多くの[ツールの統合](https://python.langchain.com/docs/integrations/tools/)や組み込みの [ReAct エージェント](https://python.langchain.com/docs/modules/agents/agent_types/react)など、さまざまな便利な機能が備わっています。\n",
        "\n",
        "ただし、Langchainとの反応は、すべてのユースケースに最適ではない場合があります。Langchainを使用して使用する場合は、ニーズを満たしているかどうかを評価することが重要です。\n",
        "\n",
        "Langchainが現在ユースケースのニーズを満たしていないことがわかった場合でも、Langchainが1.0リリースに近づくと機能が追加されることに注意してください。\n",
        "\n",
        "Langchainには、[Langsmith](https://docs.smith.langchain.com/)という名前で利用可能な独自の評価と生産ツールもあります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrL7MQSR89ND"
      },
      "source": [
        "## 基本的なLangchain React Agent\n",
        "\n",
        "LangchainでのReactの主な利点は、開始するのが非常に少ない作業であることです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XU67FY8-fMN"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.tools import WikipediaQueryRun\n",
        "from langchain.utilities import WikipediaAPIWrapper\n",
        "import wikipedia\n",
        "import vertexai\n",
        "\n",
        "# This is the langchain connection to Vertex AI.\n",
        "# Note this depends on vertexai.init (which was run in Part 0).\n",
        "llm = VertexAI(model_name=MODEL_NAME, temperature=0)\n",
        "\n",
        "# Initialize the Wikipedia tool.\n",
        "_ = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
        "# This next line invisibly maps to the previous line. The WikipediaQueryRun\n",
        "#   call is what matters here for Langchain to use its \"wikipedia\", not\n",
        "#   the variable that call is output to.\n",
        "tools = load_tools([\"wikipedia\"], llm=llm)\n",
        "\n",
        "# Create the ReAct agent.\n",
        "agent = initialize_agent(tools,\n",
        "                         llm,\n",
        "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n",
        "\n",
        "# You can change this question to see how the agent performs.\n",
        "# You may get a GuessedAtParserWarning from the wikipedia API, ignore it.\n",
        "agent.run(\"What US President costarred with a chimp in 'Bedtime for Bonzo'?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gva8WBKgByU4"
      },
      "source": [
        "Langchain のもう 1 つの優れた機能は、組み込みの[ツール統合](https://python.langchain.com/docs/integrations/tools/)です。特に便利なツールの 1 つは数学用です。LLM は数学が苦手ですが、外部計算機があれば数学のパフォーマンスが向上します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00N7WCwxC9y9"
      },
      "outputs": [],
      "source": [
        "# The answer is 4489.\n",
        "# This may timeout or error, that's ok.\n",
        "agent.run(\"What's 67^2?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJu3sQ65CuvV"
      },
      "outputs": [],
      "source": [
        "# Make the llm-math tool available to the agent.\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
        "agent = initialize_agent(tools,\n",
        "                         llm,\n",
        "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n",
        "agent.run(\"What's 67^2?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttzg4RDhKXw7"
      },
      "source": [
        "## 観測可能性の課題\n",
        "\n",
        "デフォルトでは、LangchainはReactチェーンの最終出力のみを返します。しかし、特にデバッグするときは、すべてのLLMコールを見ることが必要な場合があります。\n",
        "\n",
        "Langchainには、基礎となるLLMコールにある程度の観測性を提供する冗長モードが含まれています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwCwxRFHKetP"
      },
      "outputs": [],
      "source": [
        "# Note verbose is part of the agent declaration, not the run.\n",
        "agent = initialize_agent(tools,\n",
        "                         llm,\n",
        "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "                         verbose=True)\n",
        "\n",
        "agent.run(\"What US President costarred with a chimp in 'Bedtime for Bonzo'?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1twwHcTLgqp"
      },
      "source": [
        "ここで、冗長モードは、最初の考えでは、LLMがその内部知識を使用したことを示しています。\n",
        "\n",
        "しかし、エージェントがどのように回答に到達したか、またはエージェントが失敗した理由を理解するのに冗長モードは常に十分ではありません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE2thulTLmOJ"
      },
      "outputs": [],
      "source": [
        "agent.run(\"What day of the week was September 1st, 2010?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVMx7GCiL_C9"
      },
      "source": [
        "完全にデバッグするには、Langchain 内部の可視性を向上させる必要があります。 このカスタム可観測性コードのスニペット (この[ノートブック](https://github.com/GoogleCloudPlatform/specialized-training-content/blob/184be57c9ff4de18e20f3fdfbe1ef7fb35ce023f/courses/generative_ai/langchain_observability_snippet/langchain-observability-snippet.ipynb)からのもの) は、Langchain の[コールバック ハンドラー](https://python.langchain.com/docs/modules/callbacks/)を使用して、エージェントの実行時に何が起こるかを正確に示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjSuNufEMrwK"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Import dependencies.\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from langchain.schema import AgentAction, AgentFinish, Document, LLMResult\n",
        "import pdb\n",
        "from prettyprinter import cpprint\n",
        "from typing import Any, Dict, List, Optional, Sequence, Type, Union\n",
        "from uuid import UUID\n",
        "\n",
        "# Two helper classes.\n",
        "class Color():\n",
        "  \"\"\"For easier understanding and faster manipulation of printed colors.\"\"\"\n",
        "  PURPLE = \"\\033[95m\"\n",
        "  CYAN = \"\\033[96m\"\n",
        "  DARKCYAN = \"\\033[36m\"\n",
        "  BLUE = \"\\033[94m\"\n",
        "  GREEN = \"\\033[92m\"\n",
        "  YELLOW = \"\\033[93m\"\n",
        "  RED = \"\\033[91m\"\n",
        "  BOLD = \"\\033[1m\"\n",
        "  UNDERLINE = \"\\033[4m\"\n",
        "  ITALICS = \"\\x1B[3m\"\n",
        "  END = \"\\033[0m\\x1B[0m\"\n",
        "\n",
        "\n",
        "class OutputFormatter:\n",
        "  \"\"\" Helper class to control the format of printed output from the callbacks.\n",
        "\n",
        "  If used in prod, consider reimplementing in a way that removes hardcoding\n",
        "    of where the output is written. Maybe use Python logging and then pass a\n",
        "    custom configuration?\n",
        "  \"\"\"\n",
        "  # TODO: Add str casting here to reduce f\"{}\" in callback class to this class.\n",
        "  def heading(text: str) -> None:\n",
        "    print(f\"{Color.BOLD}{text}{Color.END}\")\n",
        "\n",
        "  def key_info(text: str) -> None:\n",
        "    print(f\"{Color.BOLD}{Color.DARKCYAN}{text}{Color.END}\")\n",
        "\n",
        "  def key_info_labeled(label: str,\n",
        "                       contents: str,\n",
        "                       contents_newlined: Optional[bool] = False\n",
        "                       ) -> None:\n",
        "    print(f\"{Color.BOLD}{Color.DARKCYAN}{label}: {Color.END}{Color.DARKCYAN}\",\n",
        "          end=\"\")\n",
        "    if contents_newlined:\n",
        "      contents = contents.splitlines()\n",
        "    cpprint(f\"{contents}\")\n",
        "    print(f\"{Color.END}\", end=\"\")\n",
        "\n",
        "  def debug_info(text: str) -> None:\n",
        "    print(f\"{Color.BLUE}{text}{Color.END}\")\n",
        "\n",
        "  def debug_info_labeled(label: str,\n",
        "                         contents: str,\n",
        "                         contents_newlined: Optional[bool] = False\n",
        "                         ) -> None:\n",
        "    print(f\"{Color.BOLD}{Color.BLUE}{label}: {Color.END}{Color.BLUE}\",\n",
        "          end=\"\")\n",
        "    if contents_newlined:\n",
        "      contents = contents.splitlines()\n",
        "    cpprint(f\"{contents}\")\n",
        "    print(f\"{Color.END}\", end=\"\")\n",
        "\n",
        "  def llm_call(text: str) -> None:\n",
        "    print(f\"{Color.ITALICS}{text}{Color.END}\")\n",
        "\n",
        "  def llm_output(text: str) -> None:\n",
        "    print(f\"{Color.UNDERLINE}{text}{Color.END}\")\n",
        "\n",
        "  def tool_call(text: str) -> None:\n",
        "    print(f\"{Color.ITALICS}{Color.PURPLE}{text}{Color.END}\")\n",
        "\n",
        "  def tool_output(text: str) -> None:\n",
        "    print(f\"{Color.UNDERLINE}{Color.PURPLE}{text}{Color.END}\")\n",
        "\n",
        "  def debug_error(text: str) -> None:\n",
        "    print(f\"{Color.BOLD}{Color.RED}{text}{Color.END}\")\n",
        "\n",
        "# Actual langchain callback handler, this produces status updates during a\n",
        "#   langchain execution.\n",
        "class AllChainDetails(BaseCallbackHandler):\n",
        "  \"\"\"Outputs details of chain progress and state.\n",
        "\n",
        "  Exposes details available at callback time to each executed step in a chain.\n",
        "\n",
        "  Method arguments in this class are based on the (most of?) the arguments\n",
        "    available to the callback method, though not all implementations in this\n",
        "    class use all the arguments.\n",
        "\n",
        "  Usage:\n",
        "    Pass as an argument to a langchain method or class that accepts a callback\n",
        "      handler. Note that  not all langchain classes will invoke all callbacks\n",
        "      when the callback handler is provided at initialization time, so the\n",
        "      recommended usage is to provide the callback handler when executing a\n",
        "      chain.\n",
        "\n",
        "  Example:\n",
        "    from langchain import LLMChain, PromptTemplate\n",
        "    from langchain.llms import VertexAI\n",
        "    import vertexai  # Comes from google-cloud-aiplatform package.\n",
        "    vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "    llm = VertexAI(temperature=0)  # Use any LLM.\n",
        "    prompt_template = \"What food pairs well with {food}?\"\n",
        "    handler = AllChainDetails()\n",
        "    llm_chain = LLMChain(\n",
        "      llm=llm,\n",
        "      prompt=PromptTemplate.from_template(prompt_template))\n",
        "    llm_chain(\"chocolate\", callbacks=[handler])\n",
        "\n",
        "  Args:\n",
        "    debug_mode: If True, prints more details of each chain step and activates\n",
        "      breakpoints (using pdb) when unexpected behavior is detected. Note that\n",
        "      the breakpoints are in the callbacks, which limits the amount of\n",
        "      inspectable langchain state to what langchain surfaces to callbacks.\n",
        "    out: Class for managing output, only tested with the OutputFormatter\n",
        "      accompanying this class.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               debug_mode: Optional[bool] = False,\n",
        "               out: Type[OutputFormatter] = OutputFormatter,\n",
        "               ) -> None:\n",
        "    self.debug_mode = debug_mode\n",
        "    self.out = out\n",
        "\n",
        "  def on_llm_start(self,\n",
        "                   serialized: Dict[str, Any],\n",
        "                   prompts: List[str],\n",
        "                   **kwargs: Any) -> None:\n",
        "    \"\"\"Run when langchain calls an LLM.\"\"\"\n",
        "    self.out.heading(f\"\\n\\n> Sending text to the LLM.\")\n",
        "\n",
        "    if len(prompts) > 1:\n",
        "      self.out.debug_error(\"prompts has multiple items.\")\n",
        "      self.out.debug_error(\"Only outputting first item in prompts.\")\n",
        "      if self.debug_mode:\n",
        "        self.out.debug_info_labeled(\"Prompts\", f\"{prompts}\")\n",
        "        pdb.set_trace()\n",
        "\n",
        "    self.out.key_info(f\"Text sent to LLM:\")\n",
        "    self.out.llm_call(prompts[0])\n",
        "\n",
        "    if self.debug_mode:\n",
        "      self.out.debug_info_labeled(\"Arguments\", f\"{kwargs}\")\n",
        "      self.out.debug_info_labeled(\"serialized\", f\"{serialized}\")\n",
        "\n",
        "  def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
        "    \"\"\"Run after LLM response is received by langchain.\"\"\"\n",
        "    self.out.heading(f\"\\n\\n> Received response from LLM.\")\n",
        "\n",
        "    if len(response.generations) > 1:\n",
        "      self.out.debug_error(\"response object has multiple generations.\")\n",
        "      self.out.debug_error(\"Only outputting first generation in response.\")\n",
        "      if self.debug_mode:\n",
        "        self.out.debug_info_labeled(\"response\", f\"{response}\")\n",
        "        pdb.set_trace()\n",
        "\n",
        "    self.out.key_info(f\"Text received from LLM:\")\n",
        "    self.out.llm_output(response.generations[0][0].text)\n",
        "\n",
        "    if self.debug_mode:\n",
        "      self.out.debug_info_labeled(\"Arguments\", f\"{kwargs}\")\n",
        "      self.out.debug_info_labeled(\"response\", f\"{response}\")\n",
        "\n",
        "  def on_tool_start(self,\n",
        "                    serialized: Dict[str, Any],\n",
        "                    input_str: str,\n",
        "                    **kwargs: Any,) -> None:\n",
        "    \"\"\"Run when making a call to a tool.\"\"\"\n",
        "    self.out.heading(f\"\\n\\n> Using tool.\")\n",
        "    self.out.key_info_labeled(f\"Tool name\", f\"{serialized['name']}\")\n",
        "    self.out.key_info(f\"Query sent to tool:\")\n",
        "    self.out.tool_call(input_str)\n",
        "\n",
        "    if self.debug_mode:\n",
        "      self.out.debug_info_labeled(\"Arguments\", f\"{kwargs}\")\n",
        "      self.out.debug_info_labeled(\"serialized\", f\"{serialized}\")\n",
        "\n",
        "  def on_tool_end(\n",
        "      self,\n",
        "      output: str,\n",
        "      color: Optional[str] = None,\n",
        "      observation_prefix: Optional[str] = None,\n",
        "      llm_prefix: Optional[str] = None,\n",
        "      **kwargs: Any,) -> None:\n",
        "    \"\"\"Run on response from a tool.\"\"\"\n",
        "    self.out.heading(f\"\\n\\n> Received tool output.\")\n",
        "    self.out.key_info_labeled(f\"Tool name\", f\"{kwargs['name']}\")\n",
        "\n",
        "    if \"output\" not in locals():\n",
        "      self.out.debug_error(\"No tool output.\")\n",
        "      if self.debug_mode:\n",
        "        pdb.set_trace()\n",
        "    else:\n",
        "      self.out.key_info(\"Response from tool:\")\n",
        "      self.out.tool_output(f\"{output}\")\n",
        "\n",
        "    if self.debug_mode:\n",
        "      self.out.debug_info_labeled(\"Arguments\", f\"{kwargs}\")\n",
        "      self.out.debug_info_labeled(\"observation_prefix\",\n",
        "                                  f\"{observation_prefix}\")\n",
        "      self.out.debug_info_labeled(\"llm_prefix\",\n",
        "                                  f\"{llm_prefix}\")\n",
        "\n",
        "  def on_agent_action(self,\n",
        "                      action: AgentAction,\n",
        "                      color: Optional[str] = None,\n",
        "                      **kwargs: Any) -> Any:\n",
        "    \"\"\"Run when agent performs an action.\"\"\"\n",
        "    self.out.heading(f\"\\n\\n> Agent taking an action.\")\n",
        "\n",
        "    if self.debug_mode:\n",
        "      self.out.debug_info_labeled(\"Arguments\", f\"{kwargs}\")\n",
        "      self.out.debug_info_labeled(\"action\", f\"{action}\")\n",
        "\n",
        "  def on_agent_finish(self,\n",
        "                      finish: AgentFinish,\n",
        "                      color: Optional[str] = None,\n",
        "                      **kwargs: Any) -> None:\n",
        "    \"\"\"Run after agent completes.\"\"\"\n",
        "    self.out.heading(f\"\\n\\n> Agent has finished.\")\n",
        "\n",
        "    if self.debug_mode:\n",
        "      self.out.debug_info_labeled(\"Arguments\", f\"{kwargs}\")\n",
        "      self.out.debug_info_labeled(\"finish\",\n",
        "                                  f\"{finish}\")\n",
        "\n",
        "  def on_llm_error(self,\n",
        "                   error: Union[Exception, KeyboardInterrupt],\n",
        "                   **kwargs: Any) -> None:\n",
        "    self.out.debug_error(\"LLM Error\")\n",
        "    self.out.debug_info_labeled(\"Error object\", f\"{error}\")\n",
        "    if self.debug_mode:\n",
        "      pdb.set_trace()\n",
        "\n",
        "  def on_chain_error(self,\n",
        "                     error: Union[Exception, KeyboardInterrupt],\n",
        "                     **kwargs: Any) -> None:\n",
        "    self.out.debug_error(\"Chain Error\")\n",
        "    self.out.debug_info_labeled(\"Error object\", f\"{error}\")\n",
        "    if self.debug_mode:\n",
        "      pdb.set_trace()\n",
        "\n",
        "  def on_tool_error(self,\n",
        "                    error: Union[Exception, KeyboardInterrupt],\n",
        "                    **kwargs: Any) -> None:\n",
        "    self.out.debug_error(\"Chain Error\")\n",
        "    self.out.debug_info_labeled(\"Error object\", f\"{error}\")\n",
        "    if self.debug_mode:\n",
        "      pdb.set_trace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c_t51sDNV_a"
      },
      "source": [
        "カスタム観測可能性コードを含むエージェントを使用して、失敗したクエリを繰り返します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6MBeR4HNgf4"
      },
      "outputs": [],
      "source": [
        "handler = AllChainDetails()\n",
        "agent = initialize_agent(tools,\n",
        "                         llm,\n",
        "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n",
        "agent.run(\"What day of the week was September 1st, 2010?\",\n",
        "          callbacks=[handler])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt8bZW6qX-1e"
      },
      "source": [
        "LLMに送信された正確な呼び出しが表示され、LLMがツール（「ツールを使用する」）を選択したとき、LLMのツールへの入力（「ツールに送信されたクエリ：」）、および次のLLMアクティビティが表示されます。\n",
        "\n",
        "エラーの性質は明確になりました。数学ツールは、LLMに「numexpr」ライブラリで実行する式を作成するように指示しますが、LLMには誤って式に「DateTime」ライブラリが含まれています。\n",
        "\n",
        "さらに、LLMは、Langchainを呼び出して、ツールの説明や正確なReact実装（標準の思考 - >アクション - >観測とは異なる）を含む反応を実行するために使用します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MezAzXFAZpwz"
      },
      "source": [
        "### Langchainでの生産観測可能性\n",
        "\n",
        "安定した生産LLMシステムを実行するには、おそらく集中型の外部ロギング/監視プラットフォームで、強力な観察可能性とロギングが必要です。これがなければ、システムが正しく実行されていることを確認することはできず、デバッグできない場合があります。\n",
        "\n",
        "Langchainのコールバックの実装はここで役立ち、一部のMLプラットフォームベンダーはLangchainコールバックハンドラーを提供しています。\n",
        "\n",
        "ただし、一部のユースケースでは、カスタムラングチェーンコールバックハンドラーを作成する必要があります。また、システムに依存しているラングチェーンモジュールの他の部分に応じて、必要な情報をコールバックに表現するためにLangchain内部を変更する必要があります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JskUeUagcR2V"
      },
      "source": [
        "## ツールのカスタマイズ摩擦\n",
        "\n",
        "Langchainエージェントに「DateTime」サポートを追加する方法は次のとおりです。\n",
        "\n",
        "1. 数学ツールがReactプロンプトで説明されている方法を変更するため、LLMは「DateTime」を使用しないことを知っています。\n",
        "1. DateTime Operations専用の新しいツールを作成し、LLMが利用できるようにします。\n",
        "1. Langchain Mathツールを変更して、「DateTime」サポートを追加します。\n",
        "1. langchain数学ツールを変更して、「numexpr」から例外をキャッチし、次のコールでLLMにエラーメッセージを提供して、LLMが別のアクションを実行できるようにします。\n",
        "\n",
        "これらには、Langchain内部の知識や、まだ文書化されていないLangchain機能を使用する必要があります。\n",
        "\n",
        "さらに、最良の反応性能のために、指示、模範、およびツールの説明を調整する必要があります。これは、「DateTime」ツールの問題を管理する以外に、[カスタムLangchainエージェント](https://python.langchain.com/docs/modules/agents/)を作成する必要があることを意味します。\n",
        "\n",
        "多くのユースケースでは、この摩擦は克服する価値があります。しかし、フレームワークを採用するという決定と同様に、ソフトウェア開発のベストプラクティスに従い、利用可能なフレームワークと構築の長所と短所をゼロから完全に調査します。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}